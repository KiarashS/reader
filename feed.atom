<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://KiarashS.github.io/reader/index.html</id>
    <title>My RSS Reader::Kiarash Soleimanzadeh</title>
    <updated>2024-02-14T13:04:10.922Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://KiarashS.github.io/reader/index.html"/>
    <link rel="self" href="https://KiarashS.github.io/reader/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[What Is Data Lineage, And Why Does It Matter?]]></title>
        <id>https://www.kdnuggets.com/?p=163970</id>
        <link href="https://www.kdnuggets.com/what-is-data-lineage-and-why-does-it-matter?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=what-is-data-lineage-and-why-does-it-matter"/>
        <updated>2024-02-14T13:00:23.000Z</updated>
        <summary type="html"><![CDATA[If you’ve ever had conversations with data professionals, you’ve probably heard “data lineage” pop up quite a few times. So what is data lineage all about, and why is it important?]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What would be the best approach to build a neural net to identify urban sketches?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqm7j9/what_would_be_the_best_approach_to_build_a_neural/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqm7j9/what_would_be_the_best_approach_to_build_a_neural/"/>
        <updated>2024-02-14T12:53:54.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! I've been thinking about this for a little while now and I'm curious what would be the right approach to build a model that can identify a sketch. I have no clue about neural nets or machine learning, except for linear reg and gradient descent which I played around with when I was 16 but that's about it and it's been many years.
 The reason I want to do that is because sketching is a hobby of mine and I want to be able to click a picture of the sketch and have a model rate it. So I want to know where I should start. 
 - Do I build a neural net from scratch? Or do I use an existing model and train it?
 - What framework should I look into? 
 ​
    submitted by    /u/guyunderthequilt  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review — Gemini: A Family of Highly Capable Multimodal Models]]></title>
        <id>https://medium.com/p/615c2a100592</id>
        <link href="https://sh-tsang.medium.com/review-gemini-a-family-of-highly-capable-multimodal-models-615c2a100592?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:49:08.000Z</updated>
        <summary type="html"><![CDATA[Foundation Model or Large Multimodal Model (LMM) Accepting Textual, Visual, and Audio as Inputs
Continue reading on Medium »]]></summary>
        <author>
            <name>Sik-Ho Tsang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DecoGenius: Unveiling the Complexity of a Neural Visionary]]></title>
        <id>https://medium.com/p/06541ecc624a</id>
        <link href="https://medium.com/@cryptodesignai/decogenius-unveiling-the-complexity-of-a-neural-visionary-06541ecc624a?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:42:12.000Z</updated>
        <summary type="html"><![CDATA[In the realm of interior design, the fusion of creativity and technology has sparked a revolution, and at the forefront of this innovation…
Continue reading on Medium »]]></summary>
        <author>
            <name>Cryptodesign AI</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Sign Language Recognition (SLR): How good is it really and can I make it work for a less popular sign language?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqlp23/p_sign_language_recognition_slr_how_good_is_it/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqlp23/p_sign_language_recognition_slr_how_good_is_it/"/>
        <updated>2024-02-14T12:26:17.000Z</updated>
        <summary type="html"><![CDATA[I've seen a lot of beginner tutorials to implement video stream based sign language recognition but they all seem to have some issues in a real world situation (Let's say a TV recording of a press conference).
 A client asked if we can do this, so I started wondering:
  
How good is the current state of the are for SLR really? Is it being used in practice?
 Are there existing models or even services that can just be used?
 Do these exist or can be adopted for less popular sign language dialects?
  
   submitted by    /u/Enum1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Norms and Special Kinds of Metrices and Vectors.]]></title>
        <id>https://medium.com/p/44cc9e870a32</id>
        <link href="https://medium.com/@danishelahi016/norms-and-special-kinds-of-metrices-and-vectors-44cc9e870a32?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:24:56.000Z</updated>
        <summary type="html"><![CDATA[This post will discuss the importance of norms and different kinds of metrics and vectors.
Continue reading on Medium »]]></summary>
        <author>
            <name>Danish Elahi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning VS Machine Learning: The Ultimate Comparison]]></title>
        <id>https://medium.com/p/d456fd10d727</id>
        <link href="https://medium.com/@careervira.community/deep-learning-vs-machine-learning-the-ultimate-comparison-d456fd10d727?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:19:42.000Z</updated>
        <summary type="html"><![CDATA[In this blog, we will delve into the detailed comparison between deep learning vs machine learning from basic parameters to career points.
Continue reading on Medium »]]></summary>
        <author>
            <name>Careervira</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Can you create a saving space link between WebUI and ComfyUI?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqlkra/r_can_you_create_a_saving_space_link_between/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqlkra/r_can_you_create_a_saving_space_link_between/"/>
        <updated>2024-02-14T12:19:38.000Z</updated>
        <summary type="html"><![CDATA[Good day gents, I just discovered Comfy not a long a go, might be transitioning from A1111. So it gotta be a method to save space by sharing the models files between these two apps
    submitted by    /u/qualaric  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML Course Recommendation]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqljss/d_ml_course_recommendation/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqljss/d_ml_course_recommendation/"/>
        <updated>2024-02-14T12:18:05.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am especially interested in learning more about the field of data science. This includes learning about all the fundamentals from algorithms and underlying mathmatical theories to types of machine learning. I want to use python since i find it easy to use and i will be using it at work a lot soon. What i am missing right now is the ability to think of problems i can solve. By that i mean small easy projects to learn the basics. I feel like i need to be guided through this learning process just like i am being guided in class at my university. Which courses can you recommend? What did they teach you? What would you describe the learning process to be like?
    submitted by    /u/rThilo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] QA role in Machine Learning projects]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqlgck/d_qa_role_in_machine_learning_projects/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqlgck/d_qa_role_in_machine_learning_projects/"/>
        <updated>2024-02-14T12:12:32.000Z</updated>
        <summary type="html"><![CDATA[In my work experience I've met a situation, where we have to understand the role of QA in ML projects.
 I know that it's an extremely specific situation, just because ML testing is the job that Engineer does with the training and after.
 So my question is: do you have any experience with QA application directly in ML projects, or related literature to start?
    submitted by    /u/thattallsoldier  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Humanity doesn’t mean selfishness”]]></title>
        <id>https://medium.com/p/015e48113a4b</id>
        <link href="https://medium.com/@iqrah191999/humanity-doesnt-mean-selfishness-015e48113a4b?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:12:02.000Z</updated>
        <summary type="html"><![CDATA[Continue reading on Medium »]]></summary>
        <author>
            <name>Iqra Hamid</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How To Do More In 2 Hours Than Others Do In 2 Weeks]]></title>
        <id>https://medium.com/p/ac9178184232</id>
        <link href="https://medium.com/write-a-catalyst/how-to-do-more-in-2-hours-than-others-do-in-2-weeks-ac9178184232?source=rss------deep_learning-5"/>
        <updated>2024-02-14T12:11:41.000Z</updated>
        <summary type="html"><![CDATA[Image: Bing Image Generator
Continue reading on Write A Catalyst »]]></summary>
        <author>
            <name>JaffaBranding</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Vec Decoded]]></title>
        <id>https://medium.com/p/1d44822d4b3c</id>
        <link href="https://medium.com/@ypredofficial/word2vec-decoded-1d44822d4b3c?source=rss------deep_learning-5"/>
        <updated>2024-02-14T11:39:41.000Z</updated>
        <summary type="html"><![CDATA[Word2Vec is a popular technique in natural language processing for word embedding, where words are represented as dense vectors in a…
Continue reading on Medium »]]></summary>
        <author>
            <name>Ashish Malhotra</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Tutorial — Week 03]]></title>
        <id>https://medium.com/p/666b84788099</id>
        <link href="https://medium.com/@anju75061/deep-learning-tutorial-week-03-666b84788099?source=rss------deep_learning-5"/>
        <updated>2024-02-14T11:31:44.000Z</updated>
        <summary type="html"><![CDATA[Last week we learned about how multi-layer perceptron, cost function, hyper-parameters, weights and bias work. 
Now, in this week we will…
Continue reading on Medium »]]></summary>
        <author>
            <name>Anju Reddy K</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kontemplasi Soal Diri dan Pribadi]]></title>
        <id>https://medium.com/p/58c6b8af26ee</id>
        <link href="https://medium.com/@Makotoma/kontemplasi-soal-diri-dan-pribadi-58c6b8af26ee?source=rss------deep_learning-5"/>
        <updated>2024-02-14T11:18:37.000Z</updated>
        <summary type="html"><![CDATA[Jam 2 pagi itu mending tidur, bukan mikir 
Continue reading on Medium »]]></summary>
        <author>
            <name>Makomingu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word Embedding and why it’s used over TFIDF and BoW]]></title>
        <id>https://medium.com/p/e5792a6355d7</id>
        <link href="https://medium.com/@ypredofficial/word-embedding-and-why-its-used-over-tfidf-and-bow-e5792a6355d7?source=rss------deep_learning-5"/>
        <updated>2024-02-14T11:03:11.000Z</updated>
        <summary type="html"><![CDATA[Word embedding is a technique used in natural language processing (NLP) to represent words as dense vectors of real numbers in a…
Continue reading on Medium »]]></summary>
        <author>
            <name>Ashish Malhotra</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLTK, SSL Certificate Error, No module named pip]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aqk2h7/nltk_ssl_certificate_error_no_module_named_pip/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aqk2h7/nltk_ssl_certificate_error_no_module_named_pip/"/>
        <updated>2024-02-14T10:46:43.000Z</updated>
        <summary type="html"><![CDATA[I am having some serious issues with nltk.download() method. Specifically, I am struggling with a SSL certificate problem (solutions posted in other treads don't seem to work, probably due to some interferences from pip) and .zip format handling.
 Operating system: Sonoma 14.2.1. Ide: Pycharm. Interpreter: Pycharm/Projects/*Projectname*/.venv/bin/python (alternative: usr/local/bin/python3.12, but it doesn't matter which interpreter as same problems arise with both).
 1- I have installed NLTK with pycharm in the virtualenv of the project. (Note: I also have NLTK downloaded also in the local library)
 2- While working with the Python console, I can successfully import NLTK, as well as nltk.corpus. However, when it comes to nltk.download(*corpus*), it raises a well-known error (discussed in o…]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] World Model on Million-Length Video And Language With RingAttention - UC Berkeley 2024 - Is able to describe a clip in an over an hour long video with over 500 clips with near perfect accuracy! - Is open source!]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqjrc8/r_world_model_on_millionlength_video_and_language/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqjrc8/r_world_model_on_millionlength_video_and_language/"/>
        <updated>2024-02-14T10:26:03.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.08268 
 Github: https://github.com/LargeWorldModel/LWM 
 Models: https://huggingface.co/LargeWorldModel !
 Abstract:
  
Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Making my bookshelves clickable with computer vision]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqjp5d/p_making_my_bookshelves_clickable_with_computer/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqjp5d/p_making_my_bookshelves_clickable_with_computer/"/>
        <updated>2024-02-14T10:21:46.000Z</updated>
        <summary type="html"><![CDATA[I built a system that lets you take a photo of a bookshelf and create an interactive HTML web page where you can click on books in an image to learn more about each one.
 The tech stack for this project is:
  
Grounded SAM to retrieve polygons for books.
 OpenCV + supervision transformations to prepare books for OCR.
 GPT-4 with Vision for OCR
 Google Books API to get book metadata.
 HTML + SVG generation to create the final web page.
  
I wrote about how I built this project on my blog.
 Try the demo.
 I'd love feedback on how I can improve the book detection rate for better performance. Training a custom segmentation model on book spines might work, but I am cognizant about how much data I might need for that.
 The red polygons below indicate segmented books that, in the demo, are clickable:
 https://preview.redd.it/p9w4rgsn1jic1.png?width=1260&format=png&auto=webp&s=35116c7eb9d1f5dab2b11375be9e2ff0e7163b78
    submitted by    /u/zerojames_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML/Data Analytics Tutors?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqjk16/mldata_analytics_tutors/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqjk16/mldata_analytics_tutors/"/>
        <updated>2024-02-14T10:12:02.000Z</updated>
        <summary type="html"><![CDATA[Hey. I'm a Supply Chain Analytics masters student, and have begun using Python for data analysis. It's all new to me. Are there any tutoring platforms for computer science that people use? Similar to Italki, Preply etc.? One-to-one learning!
 Thanks for any tips!
    submitted by    /u/Last-Joke-8961  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Graph RAG for Wikipedia]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqjib3/p_graph_rag_for_wikipedia/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqjib3/p_graph_rag_for_wikipedia/"/>
        <updated>2024-02-14T10:08:47.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/362o1bscziic1.jpg?width=2048&format=pjpg&auto=webp&s=ab28e5921bb06d3a1aec881323b48bfadaa835bc
 RAG in it's simplest form is a vector search as context with a LLM prompt.
 With the next version of txtai, we'll have a series of new graph-based RAG techniques. Think of this like a road trip with a number of pit stops.
 Say you're researching the early medieval history of England. Sure we can run a vector search for that. But what if we can instruct a query to traverse a number of concepts we're interested in?
 Let's take the example above. This is a network of Wikipedia articles (via txtai-wikipedia). The query traverses paths of history between the Roman Empire, Anglo-Saxon period, Viking period and ends with the Norman conquest. This rich dataset is then available as a library of context to downstream LLM prompts.
 Graph databases aren't new. The difference here is that txtai builds a vector store and uses that to automatically build a graph network weighted by vector similarity.
 Read this article for more: https://neuml.hashnode.dev/generate-knowledge-with-semantic-graphs-and-rag
    submitted by    /u/davidmezzetti  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Azure Functions: Building Data-Driven Solutions With Python]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqjhoh/azure_functions_building_datadriven_solutions/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqjhoh/azure_functions_building_datadriven_solutions/"/>
        <updated>2024-02-14T10:07:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Kairo1004  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph RAG for Wikipedia]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aqjeoe/graph_rag_for_wikipedia/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aqjeoe/graph_rag_for_wikipedia/"/>
        <updated>2024-02-14T10:01:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/davidmezzetti  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Free Machine Learning Courses Online might know]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqj5uf/best_free_machine_learning_courses_online_might/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqj5uf/best_free_machine_learning_courses_online_might/"/>
        <updated>2024-02-14T09:44:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sreeravan  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What to choose between job with little ML vs internship in ML company?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aqj2xx/what_to_choose_between_job_with_little_ml_vs/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aqj2xx/what_to_choose_between_job_with_little_ml_vs/"/>
        <updated>2024-02-14T09:38:18.000Z</updated>
        <summary type="html"><![CDATA[Hi! I have this dilemma right now. I'm currently working as a Computer Vision engineer in my company. Unfortunately, the majority of my tasks do not include any Computer vision, and it's often some code tasks or creating algorithms of how to automatically control robot with PID controllers and stuff. However, there are sometimes Computer Vision tasks such as using opencv trackers. That said, its important to notice, that I can have my time learning different stuff while working and nobody really pushes me there.
 I've also joined an ML internship in another company. The project I'm working on there is EDA analysis for tabular data. By the end of this internship, I will probably get an offer from the company, but I dont know whether Im gonna be working with Computer Vision.
 That said, Im currently in a situation, where, due to my university, I must choose between them. It's important to note that I want to grow as a Computer Vision developer. What should I choose between my job and internship? Can I become a Strong Junior dev just by searching and learning stuff from books and pet-projects?
    submitted by    /u/tepes_creature_8888  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding a low-dim subspace where points in the same cluster are close to each other [R]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqj2i0/finding_a_lowdim_subspace_where_points_in_the/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqj2i0/finding_a_lowdim_subspace_where_points_in_the/"/>
        <updated>2024-02-14T09:37:21.000Z</updated>
        <summary type="html"><![CDATA[I have an embeddings dataset consisting of some clusters of data points (I chose a priori which points belong to the same cluster). Currently, points belonging to the same cluster aren't necessarily close to each other in embedding space. I want to find a low-dimensional subspace such that if I project these embeddings onto that subspace, points belonging the same cluster will be close to each other. Different clusters don't necessarily need to be far apart in the low-dim space. I thought of an optimization problem to solve using SVD and the k-means cost function, but not sure if I can actually solve it. Was curious if anyone else has ideas/thoughts!
    submitted by    /u/oomydoomy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [P] Few questions about tortoise TTS and RVC.]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqiyt3/d_p_few_questions_about_tortoise_tts_and_rvc/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqiyt3/d_p_few_questions_about_tortoise_tts_and_rvc/"/>
        <updated>2024-02-14T09:29:54.000Z</updated>
        <summary type="html"><![CDATA[I have 3 hours high quality dataset with and the model will be used to do the same thing the dataset is from. My question is How many epoches should i let it run for and if i should cut down the dataset. 
 Also what is the difference between using 10, 10 second audio to make a voice and making my own entire model.
 i will be using the same dataset to train an RVC model, my plan is to run the tortoise output through the same model in RVC toupscale the quality and later further enhance it with adobe enhance.
 I would be grateful if someone knowledgeable on this topic helped me out because i am completey new to this, Thank you.
    submitted by    /u/Opurbobin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paper reproduction [D]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqiw8m/paper_reproduction_d/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqiw8m/paper_reproduction_d/"/>
        <updated>2024-02-14T09:24:17.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm a graduated computer science engineer, and i have a solid background with ML . My graduation project was research on adversarial machine learning in NLP. But since the graduation i didn't work on any ML related topics, so i kinda forgot many things, i want to reproduce a paper to refresh my memory and also get more hands-on work because i want to do thing to be included in my resume as well. My question is, is reproducing a paper the right choice for me right now? If so then what papers could i start with (I'm open for any thing in ML either CV, NLP, Multimodal, generative models, ..etc literally anything is fine with me as long as i will gain more knowledge and experience). And if not, then what are your suggestions for what i should do instead?
    submitted by    /u/Ineffable-1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paper reproduction]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqiuhe/paper_reproduction/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqiuhe/paper_reproduction/"/>
        <updated>2024-02-14T09:20:40.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm a graduated computer science engineer and i have a solid background with ML and my graduation project was a reasearch on adversarial Machine Learning in NLP. But since the graduation i didn't work on any ML related topics, so i kinda forgot many things, i want to reproduce a paper to refresh my memory and also get more hands-on work because i want to do work to be included in my resume as well. My question is, is reproducing a paper the right choice for me right now? If so then what papers could i start with (l'm open for any thing in ML either CV, NLP Multimodal, generative models,..etc literally anything is fine with me as long as i will gain more knowledge and experience). And if not then what are your suggestions for what i should do instead?
    submitted by    /u/Ineffable-1  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Solid Test Of Consciousness For AI]]></title>
        <id>https://medium.com/p/2cc7ad3aa0eb</id>
        <link href="https://medium.com/@kelixirr/a-solid-test-of-consciousness-for-ai-2cc7ad3aa0eb?source=rss------deep_learning-5"/>
        <updated>2024-02-14T09:17:34.000Z</updated>
        <summary type="html"><![CDATA[Continue reading on Medium »]]></summary>
        <author>
            <name>Amritesh Kumar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dummy models of Scikit-learn]]></title>
        <id>https://medium.com/p/3001f54c48e9</id>
        <link href="https://towardsdatascience.com/the-dummy-models-of-scikit-learn-3001f54c48e9?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-14T09:06:17.000Z</updated>
        <summary type="html"><![CDATA[Always keep a dummy by your side.
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Yoann Mocquin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-12BERT: BERT for K-12 education]]></title>
        <id>https://medium.com/p/a268cba1dd3b</id>
        <link href="https://medium.com/@deepdwivedi/k-12bert-bert-for-k-12-education-a268cba1dd3b?source=rss------deep_learning-5"/>
        <updated>2024-02-14T08:52:24.000Z</updated>
        <summary type="html"><![CDATA[This paper was accepted in AIED conference under short papers track , this paper summarises some of my work done at Extramarks Education…
Continue reading on Medium »]]></summary>
        <author>
            <name>Deep Dwivedi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best RNNs deep dive video/article/post ?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqidg6/d_best_rnns_deep_dive_videoarticlepost/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqidg6/d_best_rnns_deep_dive_videoarticlepost/"/>
        <updated>2024-02-14T08:45:17.000Z</updated>
        <summary type="html"><![CDATA[I'm learning RNN's, and every article and video I came across so far misses some details, no one is explaining everything in great details. I prefer reading or watching a material, where author is writing code not using ready RNN functions and is explaining everything in details like Andrej Karpathy would. (I read Karpathy's blog post on RNN, but there was no much code related to RNNs and article was mostly about LSTMs) 
    submitted by    /u/your_dream724  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introduction to PyTorch: A Powerful Framework for Deep Learning]]></title>
        <id>https://medium.com/p/104cada453de</id>
        <link href="https://blog.mirkopeters.com/introduction-to-pytorch-a-powerful-framework-for-deep-learning-104cada453de?source=rss------deep_learning-5"/>
        <updated>2024-02-14T08:38:18.000Z</updated>
        <summary type="html"><![CDATA[PyTorch is a popular and PyTorch 2 are popular libraries for deep learning. efficient framework for deep learning implementations. The…
Continue reading on Mirko Peters — Data & Analytics Blog »]]></summary>
        <author>
            <name>Mirko Peters</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Money Fine loan CUSTOMER care helpline Number ❾❽❽❸❹❶❽❹❼❽ !!✅]]></title>
        <id>https://medium.com/p/436dd6770904</id>
        <link href="https://medium.com/@kindlykumar07/money-fine-loan-customer-care-helpline-number-%E2%9D%BE%E2%9D%BD%E2%9D%BD%E2%9D%B8%E2%9D%B9%E2%9D%B6%E2%9D%BD%E2%9D%B9%E2%9D%BC%E2%9D%BD-436dd6770904?source=rss------deep_learning-5"/>
        <updated>2024-02-14T08:23:58.000Z</updated>
        <summary type="html"><![CDATA[Continue reading on Medium »]]></summary>
        <author>
            <name>Kindlykumar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[12 Best Online Courses for Machine Learning with Python- 2024]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqhbxp/12_best_online_courses_for_machine_learning_with/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqhbxp/12_best_online_courses_for_machine_learning_with/"/>
        <updated>2024-02-14T07:31:55.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Aqsa81  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will there be AI Winter 3.0? [D]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqh7e7/will_there_be_ai_winter_30_d/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqh7e7/will_there_be_ai_winter_30_d/"/>
        <updated>2024-02-14T07:23:14.000Z</updated>
        <summary type="html"><![CDATA[Where do you think this trend is going?
 ​
 https://preview.redd.it/cr0wroqo3iic1.png?width=899&format=png&auto=webp&s=79b9b9ecd16702eeb70344972fa650e735f27569
 ​
 Image source: https://www.nature.com/articles/s42256-023-00735-0
 View Poll
    submitted by    /u/we_are_mammals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[These are the types of AI you need to know in 2024]]></title>
        <id>https://medium.com/p/e6c7e16457a4</id>
        <link href="https://medium.com/@ninaandhercomputer/these-are-the-types-of-ai-you-need-to-know-in-2024-e6c7e16457a4?source=rss------deep_learning-5"/>
        <updated>2024-02-14T06:54:28.000Z</updated>
        <summary type="html"><![CDATA[Your cheat sheet on the main types of AI that are relevant and why they are important. A guide for everyone in tech or otherwise.
Continue reading on Medium »]]></summary>
        <author>
            <name>Nina Chan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn AI]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqgob7/learn_ai/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqgob7/learn_ai/"/>
        <updated>2024-02-14T06:48:48.000Z</updated>
        <summary type="html"><![CDATA[I have taken an introductory course in ML covering basic ML and few things about neural nets. I am fascinated by tools like GPT and stable diffusion. I would like to know how they work. Can you guys recommend me how should I start my journey?
 Thanks
    submitted by    /u/Traditional-Olive194  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve your brute force attacks with ChatGPT]]></title>
        <id>https://medium.com/p/36994d7370a3</id>
        <link href="https://medium.com/@simon.hugo59/improve-your-brute-force-attacks-with-chatgpt-36994d7370a3?source=rss------deep_learning-5"/>
        <updated>2024-02-14T06:44:35.000Z</updated>
        <summary type="html"><![CDATA[Image from MIT Technology Review 
Continue reading on Medium »]]></summary>
        <author>
            <name>Simon Hugo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM Tutorial 16 — Transfer Learning in Large Language Models]]></title>
        <id>https://medium.com/p/ae8053c428b4</id>
        <link href="https://ai.plainenglish.io/llm-tutorial-16-transfer-learning-in-large-language-models-ae8053c428b4?source=rss------deep_learning-5"/>
        <updated>2024-02-14T06:16:35.000Z</updated>
        <summary type="html"><![CDATA[Learn how to use transfer learning for adapting large language models to new tasks or domains.
Continue reading on Artificial Intelligence in Plain English »]]></summary>
        <author>
            <name>Ayşe Kübra Kuyucu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[7 Best Generative AI Projects of 2024]]></title>
        <id>https://medium.com/p/3c5375b879cf</id>
        <link href="https://aqsazafar81.medium.com/7-best-generative-ai-projects-of-2024-3c5375b879cf?source=rss------deep_learning-5"/>
        <updated>2024-02-14T06:14:16.000Z</updated>
        <summary type="html"><![CDATA[If you’re interested in how computers can make art, music, and stories, you’re in for a treat. In 2024, computers are getting really good…
Continue reading on Medium »]]></summary>
        <author>
            <name>Aqsazafar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Synthetic Data — The Million Dollar Question]]></title>
        <id>https://medium.com/p/a54701d1b621</id>
        <link href="https://towardsdatascience.com/evaluating-synthetic-data-the-million-dollar-question-a54701d1b621?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-14T06:12:20.000Z</updated>
        <author>
            <name>Andrew Skabar, PhD</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stream Ordering: How And Why a Geo-scientist Sometimes Needed to Rank Rivers on a Map]]></title>
        <id>https://medium.com/p/360dce356df5</id>
        <link href="https://towardsdatascience.com/stream-ordering-how-and-why-a-geo-scientist-sometimes-needed-to-rank-rivers-on-a-map-360dce356df5?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-14T06:09:20.000Z</updated>
        <author>
            <name>Mikhail Sarafanov</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to think about Tensors/Vector dimensions intuitively?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aqfi8a/how_to_think_about_tensorsvector_dimensions/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aqfi8a/how_to_think_about_tensorsvector_dimensions/"/>
        <updated>2024-02-14T05:37:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/jdjddhdjdjdj  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to think about Tensors/Vector dimensions intuitively?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqfh73/how_to_think_about_tensorsvector_dimensions/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqfh73/how_to_think_about_tensorsvector_dimensions/"/>
        <updated>2024-02-14T05:36:06.000Z</updated>
        <summary type="html"><![CDATA[I have recently started to get in to ML/DL. I can understand concepts and ideas. But whenever i am going through a repository I have issues following the data flows (trying to understand how data is moving around the model). Especially data transformations/ reshaping that keep happening. How does one think about these more intuitively? How did you overcome this struggle in your initial days. What was your journey with these problems?
    submitted by    /u/jdjddhdjdjdj  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Open Images Dataset v7]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqfcts/p_open_images_dataset_v7/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqfcts/p_open_images_dataset_v7/"/>
        <updated>2024-02-14T05:29:20.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I’m not sure this is the right space to ask this type of question. I’m kinda new on this so sorry if it’s not.
 Yesterday I started trying to create a script that detects some sort of category in some provided images. However in order to train the model I saw that I could download a lot of photos, with the detections already done from Open Images Dataset. However I don’t seem to find any way to download all the photos (or at least a good amount) from a specific category (airplanes for example). 
 Does anyone who knows how to work with this type of project can help me? Thank you a lot.
    submitted by    /u/goncalosm01  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Strategies for Predictive Maintenance]]></title>
        <id>https://medium.com/p/9f1f40d8958a</id>
        <link href="https://medium.com/@albertomoccardi/deep-learning-strategies-for-predictive-maintenance-9f1f40d8958a?source=rss------deep_learning-5"/>
        <updated>2024-02-14T05:29:00.000Z</updated>
        <summary type="html"><![CDATA[A case study on N-CMAPSS Datasets
Continue reading on Medium »]]></summary>
        <author>
            <name>Albertomoccardi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] VAEs for classification]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqf4io/d_vaes_for_classification/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqf4io/d_vaes_for_classification/"/>
        <updated>2024-02-14T05:16:11.000Z</updated>
        <summary type="html"><![CDATA[Straight to the point: 
 is it possible for the latent space of a VAE to learn a classification not fed to the model? As an example let’s say you had a bunch of images of cats and dogs together, no labels, could a VAE model learn implicitly whether it’s a cat or a dog via the latent space?
 As another question, could you use the loss of a VAE in tandem with a classification model and use the loss of the VAE to gauge the likelihood that a sample is one of the labels in the first place?
    submitted by    /u/Radiant_Walrus3007  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] A problem that seems like a ML problem.]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqewye/d_a_problem_that_seems_like_a_ml_problem/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqewye/d_a_problem_that_seems_like_a_ml_problem/"/>
        <updated>2024-02-14T05:04:31.000Z</updated>
        <summary type="html"><![CDATA[Hello. 
 I work on a system that is highly parameterized i.e. has a high number of parameters (binary values or a range of integers). These parameters are not independent. Although a lot of the possible combinations are not valid, they still result in a very high number of possible configurations. 
 Now, the task at hand is to find the best configuration of this parameterized system that will maximize a metric that is directly measurable, subject to the input setting. Again the input space is non-trivially large. 
 It seems like a classical machine learning problem but seems more of a simulation-type problem where given an ideal world where I have infinite resources, I would run all system configurations against the input setting in question and find the setting that maximizes my metric in question. In “test time”, I will use this information in hand to run the system in the most optimal setting. 
 Does this problem setting sound close to any existing well-researched area? Thanks. 
 PS - I am being cryptic as I am not in a position to disclose the exact system in question.
 https://preview.redd.it/frgvz33wghic1.png?width=1292&format=png&auto=webp&s=5ea8d9720e185d0cb7de3fc3c29a5593f342c5d4
    submitted by    /u/Traditional_Two7396  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need to Finetune LLMs on Chat Data]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aqe74t/need_to_finetune_llms_on_chat_data/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aqe74t/need_to_finetune_llms_on_chat_data/"/>
        <updated>2024-02-14T04:25:23.000Z</updated>
        <summary type="html"><![CDATA[Hello, I've hundreds of transcripts having conversations between agent and customer. I've been working on finetuning mistral using QLoRa. The objective is to make a chatbot or virtual agent. However, after finetuning, instead of a single response, the model generates an entire conversation. My prompt looks something like - 
 Conversation :
 {A Small Snippet from a Transcript}
 Agent :
 { }
 Should I change the prompt? Does anyone have any experience on finetuning on chat/transcript data? Any help would be highly appreciated.
    submitted by    /u/Evermore2307  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need to Finetune LLMs on chat/Transcript Data [D]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqe5v4/need_to_finetune_llms_on_chattranscript_data_d/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqe5v4/need_to_finetune_llms_on_chattranscript_data_d/"/>
        <updated>2024-02-14T04:23:28.000Z</updated>
        <summary type="html"><![CDATA[Hello, I've hundreds of transcripts having conversations between agent and customer. I've been working on finetuning mistral using QLoRa. The objective is to make a chatbot or virtual agent. However, after finetuning, instead of a single response, the model generates an entire conversation. My prompt looks something like - 
 Conversation :
 {A Small Snippet from a Transcript}
 Agent :
 { }
 Should I change the prompt? Does anyone have any experience on finetuning on chat/transcript data? Any help would be highly appreciated.
    submitted by    /u/Evermore2307  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Seeking Advice: Cloud Certification for AI/ML Career Path]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqdde4/d_seeking_advice_cloud_certification_for_aiml/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqdde4/d_seeking_advice_cloud_certification_for_aiml/"/>
        <updated>2024-02-14T03:42:50.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm currently pursuing a Master's in AI and have a year of experience as a software developer. As I'm aspiring to break into machine learning/AI or data science roles, I've been contemplating adding a cloud certification to my portfolio. Given the significance of cloud infrastructure in deploying AI models and handling big data, I'm considering the Google Associate Cloud Engineer certification. However, with the rising popularity of Microsoft's cloud solutions, I'm at a crossroads.
 My primary goal is to enhance my employability and secure a well-paying job in the AI/ML or data science domain. While I have a few projects under my belt, I believe a cloud certification could potentially make my resume stand out.
 I'm seeking advice on a few fronts:
  
Relevance of Cloud Certifications: How beneficial is a cloud certification, specifically for someone aiming for AI/ML or data science roles? Does it significantly impact job prospects?
 Google Cloud vs. Microsoft Azure: Given the industry trends and job market demands, would you recommend Google Cloud or Microsoft Azure? Or is there another platform I should consider?
 Investment Worth: Considering the time and financial investment, do you think pursuing a cloud certification is a strategic move for someone with my background and goals?
  
Any insights, experiences, or recommendations would be greatly appreciated. I'm here to learn from those who've navigated similar paths or have insights into the industry trends. Thank you in advance for your guidance!
    submitted by    /u/No_Masterpiece_1430  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faq chatbot]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aqda31/faq_chatbot/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aqda31/faq_chatbot/"/>
        <updated>2024-02-14T03:37:59.000Z</updated>
        <summary type="html"><![CDATA[I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.
    submitted by    /u/Intelligent_Usual392  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faq chatbot]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aqd902/faq_chatbot/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aqd902/faq_chatbot/"/>
        <updated>2024-02-14T03:36:23.000Z</updated>
        <summary type="html"><![CDATA[I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.
    submitted by    /u/Intelligent_Usual392  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Benchmarking retrieval across context lengths]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqd24k/d_benchmarking_retrieval_across_context_lengths/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqd24k/d_benchmarking_retrieval_across_context_lengths/"/>
        <updated>2024-02-14T03:26:19.000Z</updated>
        <summary type="html"><![CDATA[The following “needle in a haystack” test for GPT 4 went viral earlier this year, showing 100% retrieval for the first 64k tokens:
 https://github.com/gkamradt/LLMTest_NeedleInAHaystack
 Is this considered a valid test among machine learning experts? If it is valid, then has it been replicated anywhere else? It seems unlikely that this would be the only public implementation of the test if it was valid.
 If it is not valid, what method might be?
 Finally, overall how many tokens of context do you personally think GPT 4 can remember well?
    submitted by    /u/Ok_Elephant_1806  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[d] Pointer on Using Knowledge graphs in RAG]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqcdq8/d_pointer_on_using_knowledge_graphs_in_rag/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqcdq8/d_pointer_on_using_knowledge_graphs_in_rag/"/>
        <updated>2024-02-14T02:52:25.000Z</updated>
        <summary type="html"><![CDATA[I'm curious to learn more about using knowledge graphs in retrieve-and-generate (RAG) systems. RAG involves retrieving external knowledge to help generate responses, so it seems like knowledge graphs could be very useful.
 Some specific questions I have:
  
What types of knowledge graphs work best for RAG applications? Do domain-specific graphs tend to be more useful compared to large, general graphs?
 What are some effective techniques for querying and retrieving relevant knowledge from graphs to generate responses? Are there any best practices?
 How feasible is it to keep knowledge graphs updated as new information emerges? Does the graph need to be static or can RAG systems handle frequent graph updates?
 Can knowledge graphs help with tasks like disambiguation of entities and concepts when generating responses?
 Are there any good open source knowledge graphs out there that can be pre-trained with RAG models? Or examples of systems that showcase using graphs well?
  
I'd appreciate any insight you can offer around integrating knowledge graphs into RAG workflows. Feel free to point me to any papers, examples, or other materials too. Looking forward to learning more about this area.
    submitted by    /u/Electrical_Study_617  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Practical tips or learned experiences fine tuning a ViT for a classification exercise]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqc9t7/d_practical_tips_or_learned_experiences_fine/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqc9t7/d_practical_tips_or_learned_experiences_fine/"/>
        <updated>2024-02-14T02:47:00.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm currently working with a pre-trained ViT (ViT_B_16) for downstream tasks. I'm in the process of fine-tuning it with approximately 13,000 images distributed across 100 classes. My approach involves following PyTorch official tutorials and conducting training on a Cloud Tesla T4 (g4dn.8xlarge). Each epoch takes around 14 minutes, and for the initial run, I've arbitrarily set 20 epochs.
 For additional context, my model specifications include a batch size of 32, and I'm utilizing an Adam optimizer with a learning rate set to 1e-3. I have a few questions:
  
Is a 14-minute duration per epoch reasonable?
 Are there ways to decrease the training duration? Would you suggest a linear learning rate scheduler, increasing the batch size, or adjusting the DataLoader number of workers?
 Did I overlook anything by not incorporating weight decay, epsilon, or betas?
  
I'm feeling a bit anxious about the setup and would greatly appreciate any advice.
 Many thanks for your time.
    submitted by    /u/Numerous_Speed_9107  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Camera options for detecting and tracking small objects on the groung]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aqbeix/d_camera_options_for_detecting_and_tracking_small/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aqbeix/d_camera_options_for_detecting_and_tracking_small/"/>
        <updated>2024-02-14T02:04:19.000Z</updated>
        <summary type="html"><![CDATA[Looking for some help with a hobby project. I would like to detect small objects (2-10mm size) on the ground while on the move. So camera would be attached to the vehicle going up to 10km/h and looking for objects on the ground. Distance between the camera and ground would be about 500mm. I have would like to use Jetson Nano/Xavier which I already have. My biggest worry is the camera - any idea what fps/sensor size I would need to get clear images?
    submitted by    /u/mrbronec  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overfitting]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq9lh6/overfitting/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq9lh6/overfitting/"/>
        <updated>2024-02-14T00:38:09.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I've been trying to understand how to detect an overfitting model, but the explanations I've come across are often overly complex. I know that it's easy to spot an underfitting model when metrics like accuracy, recall, and F1 score are low, for example, around 0.5 in a classification model. But how do you detect overfitting using this approach? Or perhaps you use another method?
    submitted by    /u/Esmasta97  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] If I have a sampling strategy A, B, and C and B perform the best. Would that still be true if data scaled?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq8p38/d_if_i_have_a_sampling_strategy_a_b_and_c_and_b/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq8p38/d_if_i_have_a_sampling_strategy_a_b_and_c_and_b/"/>
        <updated>2024-02-13T23:58:36.000Z</updated>
        <summary type="html"><![CDATA[So I've been testing different sampling strategies for NLP data with binary labels (Even, stratified, and the middle between the two on the label). I've been testing them on 20% of the data and have found that the middle sampling strategy has done the best so far. If I scale to 100% of the data what reasons might the middle one no longer be the best?
    submitted by    /u/DolantheMFWizard  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical tips or learned experiences fine tuning a ViT for a classification exercise]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq8nu6/practical_tips_or_learned_experiences_fine_tuning/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq8nu6/practical_tips_or_learned_experiences_fine_tuning/"/>
        <updated>2024-02-13T23:57:05.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm currently working with a pre-trained ViT (ViT_B_16) for downstream tasks. I'm in the process of fine-tuning it with approximately 13,000 images distributed across 100 classes. My approach involves following PyTorch official tutorials and conducting training on a Cloud Tesla T4 (g4dn.8xlarge). Each epoch takes around 14 minutes, and for the initial run, I've arbitrarily set 20 epochs.
 For additional context, my model specifications include a batch size of 32, and I'm utilizing an Adam optimizer with a learning rate set to 1e-3. I have a few questions:
  
Is a 14-minute duration per epoch reasonable?
 Are there ways to decrease the training duration? Would you suggest a linear learning rate scheduler, increasing the batch size, or adjusting the DataLoader number of workers?
 Did I overlook anything by not incorporating weight decay, epsilon, or betas?
  
I'm feeling a bit anxious about the setup and would greatly appreciate any advice.
 Many thanks for your time.
    submitted by    /u/Numerous_Speed_9107  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternative for a securities price prediction project?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq8lm6/alternative_for_a_securities_price_prediction/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq8lm6/alternative_for_a_securities_price_prediction/"/>
        <updated>2024-02-13T23:54:25.000Z</updated>
        <summary type="html"><![CDATA[I really enjoy learning about finance and I've been wanting to apply my machine learning on a project in that field. I understand that time-series forecasting of stocks and such has been done billions of times, and that there are algorithms which simplify that task a lot, so for both of those reasons my project would be pointless. I really wanted to do something related to this segment, but any way i turn i end up bumping my head into "too simple" or "not a good concept" for ML.
 Any ideas? I'm really all out of luck.
    submitted by    /u/avaqueue  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Penerapan Algoritma Machine Learning pada Visualisasi Data]]></title>
        <id>https://medium.com/p/2f2a0c44dd46</id>
        <link href="https://medium.com/@mfavianardrarazan/penerapan-algoritma-machine-learning-pada-visualisasi-data-2f2a0c44dd46?source=rss------deep_learning-5"/>
        <updated>2024-02-13T23:46:19.000Z</updated>
        <summary type="html"><![CDATA[Definisi Machine Learning
Continue reading on Medium »]]></summary>
        <author>
            <name>Mfavianardrarazan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Information retrieval/search]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq8dvw/d_information_retrievalsearch/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq8dvw/d_information_retrievalsearch/"/>
        <updated>2024-02-13T23:45:16.000Z</updated>
        <summary type="html"><![CDATA[I am looking for documentation on building a search engine. Specifically around handling queries and building embeddings for them.
 Some of the use cases can be long queries, maintaining long context, spelling mistakes, handling multiple conditions, rewriting, expansion, query intent , NLU.
 I will probably build it using RAG+LLM but I think the basic principles will still apply. Any suggestions on where/what to read up?
    submitted by    /u/Worldly-Pen-8101  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] What is a robust pre-trained Word2Vec model?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq8cm5/p_what_is_a_robust_pretrained_word2vec_model/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq8cm5/p_what_is_a_robust_pretrained_word2vec_model/"/>
        <updated>2024-02-13T23:43:46.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to build an RNN to predict sentences, but I need to start with a good Word2Vec model that is robust to things like numbers (in non-word form so: 1,2,3), human names, and so on. I have data, but new data can come in with words not previously seen, thus the need for a robust Word2Vec model. Any suggestions?
 Note: I can't use Transformers for this problem due to certain problem constraints since I know the most common response will be to use a pre-trained Transformer.
    submitted by    /u/DolantheMFWizard  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python code for chatgpt API [R]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq8aw9/python_code_for_chatgpt_api_r/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq8aw9/python_code_for_chatgpt_api_r/"/>
        <updated>2024-02-13T23:41:42.000Z</updated>
        <summary type="html"><![CDATA[My Python code interacts successfully with the ChatGPT API; however, the results it yields differ from what I expect. Outputs from ChatGPT are typically more elaborate and extended, but the responses I receive from my API calls are brief and lack detail. Despite tweaking the temperature and token values, I haven't seen an improvement. I would appreciate any assistance with this issue.
 def get_completion(prompt, model="gpt-4", temperature=0.7, max_tokens=5000):
 messages = [{"role": "user", "content": prompt}]
 response = openai.ChatCompletion.create(
 model=model,
 messages=messages,
 temperature=temperature,
 max_tokens=max_tokens,
 )
 return response.choices[0].message["content"]
 ​
 ChatGPT API response : Plano, Texas is known for its affluent population and highly prioritized educatio…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI Design Patterns: A Comprehensive Guide]]></title>
        <id>https://medium.com/p/41425a40d7d0</id>
        <link href="https://towardsdatascience.com/generative-ai-design-patterns-a-comprehensive-guide-41425a40d7d0?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T23:19:59.000Z</updated>
        <summary type="html"><![CDATA[Architecture patterns and mental models for working with Large Language Models]]></summary>
        <author>
            <name>Vincent Koc</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Speech Synthesis with Mamba: Beginner friendly notebook + code]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq7rd5/p_speech_synthesis_with_mamba_beginner_friendly/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq7rd5/p_speech_synthesis_with_mamba_beginner_friendly/"/>
        <updated>2024-02-13T23:18:44.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I came across this post last month and found it super interesting. I'm a developer advocate at Determined AI and am always looking to learn new things, so I wanted to work through it myself. Super well written blog post by u/ExaminationNo8522 helped too.
 Anyways, I wanted to go through it and reproduce for myself on a different dataset, and port to Determined. The result is a beginner friendly notebook + blog post. Check these out if you're interested.
 And of course, let me know if you have thoughts/feedback/comments/issues!
    submitted by    /u/ishabytes  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] YOLOv5 - Memory Usage]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq7n16/d_yolov5_memory_usage/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq7n16/d_yolov5_memory_usage/"/>
        <updated>2024-02-13T23:13:42.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning,
 ​
 I have recently been training a custom YOLOv5 dataset for a project I am working on. I notice that when I run train.py, initially the system loads up my RAM with what I assume are the model weights before running the training? 
 ​
 I say this because my GPU has near-zero usage for the first minute or so while my RAM usage goes from baseline to almost 16 GB used by Vscode. Is this normal behavior? I am running the YOLOv5m pretrained weights which is a 21.2 M parameter model. GPU memory usage peaks at around 8.3 GB/12 GB. 
 ​
 I am training on 1280x1280 px images and a batch size of 4. The larger images are required for my application as I need to identify small features. The batch size is limited by my GPU memory as far as I can tell. 
 ​
 Just wanted to see if anyone else has seen this or if the high RAM usage points to some inefficiency or memory leak.
 ​
 Thanks!
 ​
 ​
 ​
    submitted by    /u/LuckyBucky77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I have an unorthodox genetic algorithm and I'm wondering if there is already something published in the literature that explains a similar algorithm]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq7gh4/i_have_an_unorthodox_genetic_algorithm_and_im/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq7gh4/i_have_an_unorthodox_genetic_algorithm_and_im/"/>
        <updated>2024-02-13T23:05:55.000Z</updated>
        <summary type="html"><![CDATA[I developed a genetic algorithm that, in my opinion, has some unorthodox characteristics and I would like to find out if there is something similar already published in the literature. I don't have much experience with genetic algorithms so I don't really know where to start searching, so I'm wondering if anyone here has heard or seen of an algorithm with characteristics similar to the following:
  
The population of size M is divided into two disjoint groups; the top K individuals ("top" in terms of their fitness value) constitute the "upper bin", and the rest M - K individuals constitute the "lower bin".
 During parent selection (before performing crossover), a pair of individuals is chosen in one of three possible ways: both individuals are randomly chosen from the upper bin, both indiv…]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's the standard practice for setting initialization prompts and maintaining context when switching LLMs within the same conversation?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq78ao/d_whats_the_standard_practice_for_setting/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq78ao/d_whats_the_standard_practice_for_setting/"/>
        <updated>2024-02-13T22:56:36.000Z</updated>
        <summary type="html"><![CDATA[Hi I am trying to build a modularized LLM application using Langchain where within any individual conversation the app can seamless switch between multiple LLMs to respond to the query, for example:
  
User: What's 1+ 1?
 App (GPT-3.5): 1+1 is 2
 User: Concatenate the last name of the current president of the US with the answer from your last response
 App (Gemini Ultra): Biden2
  
I have two technical questions that I hope this subreddit can help answer:
  
What's the standard practice for setting the initialization prompts or background prompts? For example I want to tell this App that "your name is Bob", and I want this App to continuously remember it's Bob regardless how long the conversation has gotten or any switching between LLMs. Do I set this at the beginning of the conversation or before every single response?
 What's the standard practice for conversation memory management when there's switching of LLM involved within one conversation? Do I store all the conversation history within a vector database and do a index search prior to any individual response?
  
   submitted by    /u/Try_StockAnalystGPT  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do you reassemble full images from models that use patches?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aq7894/how_do_you_reassemble_full_images_from_models/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aq7894/how_do_you_reassemble_full_images_from_models/"/>
        <updated>2024-02-13T22:56:32.000Z</updated>
        <summary type="html"><![CDATA[In order to do image processing on large or irregularly shaped images one strategy is to divide the input image into patches, say 64x64 pixels. How do you reassemble those patches into a large image again after inference? Obviously you can just concatenate them, but that would risk edges where the patches meet. Sliding windows with overlap and averaging can result in blurring. This is less of a problem when doing segmentation, but what about other image processing?
 If you're doing super resolution or style transfer or anything that results in a new natural image, how would you merge patches back into a full size image? Are there any standard ways of doing it, or even comparisons between multiple methods?
 I tried searching, but all I can find is how to split images into patches, not the other way around.
    submitted by    /u/bearnaisepudding  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShuffleNet: Revolutionizing Mobile Deep Learning]]></title>
        <id>https://medium.com/p/e15237239f47</id>
        <link href="https://medium.com/aimonks/shufflenet-revolutionizing-mobile-deep-learning-e15237239f47?source=rss------deep_learning-5"/>
        <updated>2024-02-13T22:18:18.000Z</updated>
        <summary type="html"><![CDATA[Introduction
Continue reading on  . »]]></summary>
        <author>
            <name>Everton Gomede, PhD</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Healing through love ❤️]]></title>
        <id>https://medium.com/p/185ae0d6fc7b</id>
        <link href="https://medium.com/@tryggvason.david.najma/healing-through-love-%EF%B8%8F-185ae0d6fc7b?source=rss------deep_learning-5"/>
        <updated>2024-02-13T22:16:01.000Z</updated>
        <summary type="html"><![CDATA[Loving through healing  
Continue reading on Medium »]]></summary>
        <author>
            <name>Najma David Tryggvason</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DP-Auditorium: A flexible library for auditing differential privacy]]></title>
        <id>http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html</id>
        <link href="http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html"/>
        <updated>2024-02-13T22:11:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Mónica Ribero Díaz, Research Scientist, Google Research





Differential privacy (DP) is a property of randomized mechanisms that limit the influence of any individual user’s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies across industries and government applications (e.g., the US census) without compromising individual user identities.  As its adoption increases, it’s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, researchers compared six sparse vector technique (SVT) variations and found that only two…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abraham De Moivre, His Famous Theorem, and the Birth of the Normal Curve]]></title>
        <id>https://medium.com/p/ee11ab5f9f20</id>
        <link href="https://towardsdatascience.com/abraham-de-moivre-his-famous-theorem-and-the-birth-of-the-normal-curve-ee11ab5f9f20?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T21:37:50.000Z</updated>
        <summary type="html"><![CDATA[The life and times of Abraham De Moivre, his famous theorem, and how it set the stage for the discovery of the Central Limit Theorem]]></summary>
        <author>
            <name>Sachin Date</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aq58wq/diffusion_of_thoughts_chainofthought_reasoning_in/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aq58wq/diffusion_of_thoughts_chainofthought_reasoning_in/"/>
        <updated>2024-02-13T21:34:33.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.07754
 Code: https://github.com/HKUNLP/diffusion-of-thoughts
 Abstract:
  
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.
  
   submitted by    /u/FastestGPU  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq572l/r_diffusion_of_thoughts_chainofthought_reasoning/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq572l/r_diffusion_of_thoughts_chainofthought_reasoning/"/>
        <updated>2024-02-13T21:32:22.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.07754
 Code: https://github.com/HKUNLP/diffusion-of-thoughts
 Abstract:
  
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.
  
   submitted by    /u/FastestGPU  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Molding the Imagination: Using AI to Create New 3D-Printable Objects]]></title>
        <id>https://medium.com/p/cf3682f8563b</id>
        <link href="https://towardsdatascience.com/molding-the-imagination-using-ai-to-create-new-3d-printable-objects-cf3682f8563b?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T21:31:18.000Z</updated>
        <summary type="html"><![CDATA[Transforming your ideas into tangible artifacts using Midjourney and open-source projects: Shap-E, MVDream, and threestudio
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Robert A. Gonsalves</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VectorBT for Quantitative Analysis in Python]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq516w/vectorbt_for_quantitative_analysis_in_python/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq516w/vectorbt_for_quantitative_analysis_in_python/"/>
        <updated>2024-02-13T21:25:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/fancypigollo  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Scaling Laws for Fine-Grained Mixture of Experts]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq4sp9/r_scaling_laws_for_finegrained_mixture_of_experts/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq4sp9/r_scaling_laws_for_finegrained_mixture_of_experts/"/>
        <updated>2024-02-13T21:16:20.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.07871
 Code: https://github.com/llm-random/llm-random
 Abstract:
  
Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
  
   submitted by    /u/FastestGPU  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning and AI for healthcare]]></title>
        <id>https://medium.com/p/9978f54bf441</id>
        <link href="https://medium.com/@harsh.vardhan7695/machine-learning-and-ai-for-healthcare-9978f54bf441?source=rss------deep_learning-5"/>
        <updated>2024-02-13T20:57:44.000Z</updated>
        <summary type="html"><![CDATA[Introduction
Continue reading on Medium »]]></summary>
        <author>
            <name>Harsh Vardhan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seven Technologies to Watch in 2024 According to Nature]]></title>
        <id>https://medium.com/p/72174b78e560</id>
        <link href="https://medium.com/@jhoansfuentes1999/seven-technologies-to-watch-in-2024-according-to-nature-72174b78e560?source=rss------deep_learning-5"/>
        <updated>2024-02-13T20:51:33.000Z</updated>
        <summary type="html"><![CDATA[1- Deep Learning for protein design
Continue reading on Medium »]]></summary>
        <author>
            <name>Jhoan Sebastián Fuentes Hernández</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Offline Reinforcement Learning]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq40xo/mutual_information_regularized_offline/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq40xo/mutual_information_regularized_offline/"/>
        <updated>2024-02-13T20:45:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LushousLightfoot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Most Advanced Libraries for Data Visualization and Analysis on the Web]]></title>
        <id>https://medium.com/p/e823535e0eb1</id>
        <link href="https://towardsdatascience.com/the-most-advanced-libraries-for-data-visualization-and-analysis-on-the-web-e823535e0eb1?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T20:39:41.000Z</updated>
        <summary type="html"><![CDATA[A careful selection looking for performance, flexibility, and richness of features.
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>LucianoSphere (Luciano Abriata, PhD)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT Vs Gemini]]></title>
        <id>https://medium.com/p/5272c8c53d0d</id>
        <link href="https://blog.gopenai.com/chatgpt-vs-gemini-5272c8c53d0d?source=rss------deep_learning-5"/>
        <updated>2024-02-13T20:35:00.000Z</updated>
        <summary type="html"><![CDATA[A turning point for AI?
Continue reading on GoPenAI »]]></summary>
        <author>
            <name>Debaprasann Bhoi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Train a model with our voices and music?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aq3jke/train_a_model_with_our_voices_and_music/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aq3jke/train_a_model_with_our_voices_and_music/"/>
        <updated>2024-02-13T20:25:14.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 In my family, we have this special kind of music and singing that is from a really small part in Europe, and I really want to keep it safe for the future and experiment with on an AI/LLM model (I don’t know that exact name).
 Is there any kind of model where I can teach it to use our voices and music, so I can make songs with it using my and family’s own voice and our music?
 Many thanks in advance!
    submitted by    /u/magicmetagic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLaMA 2: A Detailed Guide to Fine-Tuning the Large Language Model]]></title>
        <id>https://medium.com/p/8968f77bcd15</id>
        <link href="https://medium.com/@gobishangar11/llama-2-a-detailed-guide-to-fine-tuning-the-large-language-model-8968f77bcd15?source=rss------deep_learning-5"/>
        <updated>2024-02-13T20:14:05.000Z</updated>
        <summary type="html"><![CDATA[Large Language Models (LLMs):
Continue reading on Medium »]]></summary>
        <author>
            <name>Gobi Shangar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring AI’s Creativity with Visual Word Puzzles]]></title>
        <id>https://medium.com/p/cb1bd2f3f4bb</id>
        <link href="https://towardsdatascience.com/measuring-ais-creativity-with-visual-word-puzzles-cb1bd2f3f4bb?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T20:10:28.000Z</updated>
        <summary type="html"><![CDATA[How well can AI models solve (and create) rebus puzzles?
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Yennie Jun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can someone explain BERT to me? I am having trouble grasping how it is used.]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aq2y9q/can_someone_explain_bert_to_me_i_am_having/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aq2y9q/can_someone_explain_bert_to_me_i_am_having/"/>
        <updated>2024-02-13T20:01:24.000Z</updated>
        <summary type="html"><![CDATA[I understand what BERT is, and I am very new to my ML journey. I was wondering how it is built upon to create personalized things like chatbots, generating text summaries, etc. If anyone could provide me with some insight, I would greatly appreciate it!
    submitted by    /u/No-Buffalo-2565  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[40 Rules of love ♡]]></title>
        <id>https://medium.com/p/ba2cf24c95d0</id>
        <link href="https://medium.com/@zayisreading/40-rules-of-love-ba2cf24c95d0?source=rss------deep_learning-5"/>
        <updated>2024-02-13T19:55:58.000Z</updated>
        <summary type="html"><![CDATA[♡♡♡
Continue reading on Medium »]]></summary>
        <author>
            <name>Zayiszzz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sound Identification On Edge Microcontrollers]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aq2kuj/sound_identification_on_edge_microcontrollers/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aq2kuj/sound_identification_on_edge_microcontrollers/"/>
        <updated>2024-02-13T19:46:28.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I would like to deploy a simple app on an ESP32-S3 microcontroller that can identify when a cat meowing is heard. How feasible is that? How would you suggest I approach it?
 TensorFlow has TinyML, but it seems to be a lot of work to get it running on edge devices like the ESP32. Is that really the best option?
 TIA, -T
    submitted by    /u/gamename  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What’s the best way to manipulate a human?]]></title>
        <id>https://medium.com/p/e951b5b8c806</id>
        <link href="https://medium.com/@tepowendymakgatlhe/whats-the-best-way-to-manipulate-a-human-e951b5b8c806?source=rss------deep_learning-5"/>
        <updated>2024-02-13T19:38:45.000Z</updated>
        <summary type="html"><![CDATA[Continue reading on Medium »]]></summary>
        <author>
            <name>Tepo Wendy Makgatlhe</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq2dya/predicted_output_after_decoding_is_always_empty/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq2dya/predicted_output_after_decoding_is_always_empty/"/>
        <updated>2024-02-13T19:38:44.000Z</updated>
        <summary type="html"><![CDATA[Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.
 I created a new project to debug with, my real one is much more complicated, but this code is just to show my issue:
 ```import tensorflow as tf
 import numpy as np
 from keras.callbacks import EarlyStopping
 from keras.layers import Bidirectional, Dropout, BatchNormalization, Embedding, LSTM, Dense
 from keras.optimizers import Adam
 from keras.regularizers import l2
 from keras.models import Sequential
 from keras.preprocessing.sequence import pad_sequences
 from sklearn.model_selection import train_test_split
 user_prompts = np.array([
 "What's your favorite animal?",
 "What's your favorite movie?",
 "What's your favorite book?",
 "What's your favorite season?",
 "What's your favorit…]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine. [R]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq2czi/predicted_output_after_decoding_is_always_empty/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq2czi/predicted_output_after_decoding_is_always_empty/"/>
        <updated>2024-02-13T19:37:37.000Z</updated>
        <summary type="html"><![CDATA[Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.
 I created a new project to debug with, my real one is much more complicated, but this code is just to show my issue:
 ```import tensorflow as tf
 import numpy as np
 from keras.callbacks import EarlyStopping
 from keras.layers import Bidirectional, Dropout, BatchNormalization, Embedding, LSTM, Dense
 from keras.optimizers import Adam
 from keras.regularizers import l2
 from keras.models import Sequential
 from keras.preprocessing.sequence import pad_sequences
 from sklearn.model_selection import train_test_split
 user_prompts = np.array([
 "What's your favorite animal?",
 "What's your favorite movie?",
 "What's your favorite book?",
 "What's your favorite season?",
 "What's your favorit…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Gaussian Processes with GPytorch - More output than input data]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq2cx9/p_gaussian_processes_with_gpytorch_more_output/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq2cx9/p_gaussian_processes_with_gpytorch_more_output/"/>
        <updated>2024-02-13T19:37:33.000Z</updated>
        <summary type="html"><![CDATA[I am following the basic tutorial https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html to train a Gaussian Process for the following data
 train_x is a tensor of [4058, 12] train_y is a tensor of [4058, 140] 
 I get an error calculating the loss
 loss = -mll(output, train_y) 
 saying that output(model(train_x)) and train_ydon't have the same dimension. Given this, I tried MultitaskGPModelhttps://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html getting a very similar error
 RuntimeError: The size of tensor a (568120) must match the size of tensor b (8116) at non-singleton dimension 0 
 Apparently MultitaskGPModelrequires the same total number of entries to be equal. Is there a way to train a multiple-entry GP?
    submitted by    /u/WarpDrive2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Pleiadian Starseeds: Cosmic Kinfolk”]]></title>
        <id>https://medium.com/p/dd1beac356d5</id>
        <link href="https://medium.com/new-earth-consciousness/pleiadian-starseeds-cosmic-kinfolk-dd1beac356d5?source=rss------deep_learning-5"/>
        <updated>2024-02-13T19:35:53.000Z</updated>
        <summary type="html"><![CDATA[Good morning, good afternoon, and good evening I’ve been getting a lot of feedback on my different type a Star Seeds post that I thought I…
Continue reading on New Earth Consciousness »]]></summary>
        <author>
            <name>ENFINITE BALANCE</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performing Customer Analytics with LangChain and LLMs]]></title>
        <id>https://medium.com/p/0af4ea38f7b5</id>
        <link href="https://towardsdatascience.com/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T19:34:16.000Z</updated>
        <summary type="html"><![CDATA[Explore the potentials and constraints of LangChain for customer analytics, accompanied by practical implementation codes
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>John Leung</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How long does it take to finish all Kaggle Learn courses?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq282o/how_long_does_it_take_to_finish_all_kaggle_learn/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq282o/how_long_does_it_take_to_finish_all_kaggle_learn/"/>
        <updated>2024-02-13T19:31:55.000Z</updated>
        <summary type="html"><![CDATA[Will 5 days with 5 hours each day be enough?
    submitted by    /u/Icy_Broccoli_4162  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faq chatbot]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq1zsr/faq_chatbot/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq1zsr/faq_chatbot/"/>
        <updated>2024-02-13T19:22:35.000Z</updated>
        <summary type="html"><![CDATA[I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.
    submitted by    /u/Intelligent_Usual392  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Leverage Stable Diffusion Prior to detect contextual anomalies in real images]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq1hux/d_leverage_stable_diffusion_prior_to_detect/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq1hux/d_leverage_stable_diffusion_prior_to_detect/"/>
        <updated>2024-02-13T19:02:38.000Z</updated>
        <summary type="html"><![CDATA[Hello people. I've been thinking about how a model like Stable Diffusion trained on immense amounts of data could be used to detect anomalies in images in a zero-shot manner. For example, detecting folds and tears in old photos:
 https://preview.redd.it/wqlhl516deic1.png?width=663&format=png&auto=webp&s=e90deb650443f4d0f8c32e3d01d6c9b1941a21d9
 I know there are many works which use SD to inpaint the damaged areas given a mask; but what if we don't have the mask? There are also approaches such as the very cool DiffEdit, where we can use language to localise areas affected the most by a text prompt; but what about the cases where language just isn't precise enough? 
 If any images can be inverted to SD's latent space, could the inverted latents tell us something useful about how to detect the damage? Or are there any other properties of either the model or the defects which can be leveraged? Are there any works which try to leverage the distribution learned by SD* to detect the areas which should be inpainted? 
 *My intuition is that while SD has definitely been trained on images like this, the presence of deterioration is often not described precisely in language, i.e. this image's caption may be something along the lines of "old photo", and the folds are only one of the attributes which "old photo" will cover, another one being the sepia tone for example. 
 Any pointers and discussion are appreciated!
 ​
    submitted by    /u/35mmpy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Is All You Need]]></title>
        <id>https://medium.com/p/ae2d216d7f17</id>
        <link href="https://pub.aimind.so/attention-is-all-you-need-ae2d216d7f17?source=rss------deep_learning-5"/>
        <updated>2024-02-13T18:42:51.000Z</updated>
        <summary type="html"><![CDATA[How do Transformers work?
Continue reading on AI Mind »]]></summary>
        <author>
            <name>John Clayton Blanc</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visiting Nature’s Pharmacy: Using Computer Vision to Explore Wild Animals’ Self-Medication Patterns]]></title>
        <id>https://medium.com/p/5714f21efab4</id>
        <link href="https://medium.com/@mr.bioprospecting/visiting-natures-pharmacy-using-computer-vision-to-explore-wild-animals-self-medication-patterns-5714f21efab4?source=rss------deep_learning-5"/>
        <updated>2024-02-13T18:41:13.000Z</updated>
        <summary type="html"><![CDATA[In the intricate dance of the natural world, animals often resort to fascinating survival strategies, including self-medication through…
Continue reading on Medium »]]></summary>
        <author>
            <name>Sam Salmasi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the best way to download multiple JSON files from the same domain?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aq0vyo/what_is_the_best_way_to_download_multiple_json/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aq0vyo/what_is_the_best_way_to_download_multiple_json/"/>
        <updated>2024-02-13T18:39:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/alenathomasfc  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ICLR openreview visible?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq0ama/d_iclr_openreview_visible/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq0ama/d_iclr_openreview_visible/"/>
        <updated>2024-02-13T18:15:33.000Z</updated>
        <summary type="html"><![CDATA[I recently submit a paper to ICML, which is rejected from ICLR. I found that my paper in ICLR's openreview console is visible to everyone. Is it OK? As the title of the paper in ICLR and ICML are the same, ICML may not be perfectly anonymous then.
 Do I have to change the visibility manually?
    submitted by    /u/Shot-Button-9010  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [R] Topic for MSc in AI/ML]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aq06v5/d_r_topic_for_msc_in_aiml/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aq06v5/d_r_topic_for_msc_in_aiml/"/>
        <updated>2024-02-13T18:11:26.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Currently I am searching for a topic for my dissertation and I was thinking going for machine vision,
 my idea here is to apply ML to identify drugs or ilicit food in x-ray scan images.
 but I am kind of lost on where to start looking for or even if there is data available for training.
 any help is appreciated
    submitted by    /u/XicoLeite  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI Playground: Text-to-Image Stable Diffusion with Stability AI, Stable Diffusion XL, and CompVis on the Latest Intel® GPU]]></title>
        <id>https://www.kdnuggets.com/?p=164048</id>
        <link href="https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-text-to-image-stable-diffusion?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=generative-ai-playground-text-to-image-stable-diffusion-with-stability-ai-stable-diffusion-xl-and-compvis-on-the-latest-intel-gpu"/>
        <updated>2024-02-13T18:00:30.000Z</updated>
        <summary type="html"><![CDATA[Stable Diffusion models are revolutionizing digital artistry, transforming mere text into stunning, lifelike images. Explore further here.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Books on Linear Algebra for Machine Learning?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apzpon/books_on_linear_algebra_for_machine_learning/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apzpon/books_on_linear_algebra_for_machine_learning/"/>
        <updated>2024-02-13T17:52:39.000Z</updated>
        <summary type="html"><![CDATA[I’m doing a project which involves creating a neural network from the ground up. There’s plenty of resources that can tell me how to do that, but I’ve struggled to find any that go into depth on the linear algebra behind ML and neural networks. Are there any papers, books or other resources anyone could direct me to that provide a rigorous understanding of the MATH behind ML?
    submitted by    /u/Endeavor09  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming RNN Limitations: Dot Product Attention for Mitigating Vanishing Gradients]]></title>
        <id>https://medium.com/p/1fec01432d21</id>
        <link href="https://medium.com/@eugenesh4work/overcoming-rnn-limitations-dot-product-attention-for-mitigating-vanishing-gradients-1fec01432d21?source=rss------deep_learning-5"/>
        <updated>2024-02-13T17:51:31.000Z</updated>
        <summary type="html"><![CDATA[The vanishing gradient problem is a significant challenge in training deep neural networks, particularly Recurrent Neural Networks (RNNs)…
Continue reading on Medium »]]></summary>
        <author>
            <name>Eugene Shevchenko</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon-Based Sentiment Analysis Using R]]></title>
        <id>https://medium.com/p/5c1db85984a1</id>
        <link href="https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T17:50:27.000Z</updated>
        <author>
            <name>Okan Bulut</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Beginners; the concept you know, the struggles you don’t.]]></title>
        <id>https://medium.com/p/3d0e3bc39e78</id>
        <link href="https://medium.com/@atharvpal17/deep-learning-for-beginners-the-concept-you-know-the-struggles-you-dont-3d0e3bc39e78?source=rss------deep_learning-5"/>
        <updated>2024-02-13T17:46:07.000Z</updated>
        <summary type="html"><![CDATA[Ever embarked on a project with the enthusiasm of a toddler diving into a ball pit, only to realize it’s more like navigating a labyrinth…
Continue reading on Medium »]]></summary>
        <author>
            <name>Atharv Pal</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[𝐌𝐚𝐜𝐡𝐢𝐧𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠: 𝐒𝐮𝐩𝐞𝐫𝐯𝐢𝐬𝐞𝐝 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐌𝐨𝐝𝐞𝐥𝐬 🤖]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apzizc/%F0%9D%90%8C%F0%9D%90%9A%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%9E_%F0%9D%90%8B%F0%9D%90%9E%F0%9D%90%9A%F0%9D%90%AB%F0%9D%90%A7%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%A0_%F0%9D%90%92%F0%9D%90%AE%F0%9D%90%A9%F0%9D%90%9E%F0%9D%90%AB%F0%9D%90%AF%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%9E%F0%9D%90%9D_%F0%9D%90%92%F0%9D%90%AD%F0%9D%90%9A%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%9C%F0%9D%90%9A%F0%9D%90%A5_%F0%9D%90%8C%F0%9D%90%A8%F0%9D%90%9D%F0%9D%90%9E%F0%9D%90%A5%F0%9D%90%AC/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apzizc/%F0%9D%90%8C%F0%9D%90%9A%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%9E_%F0%9D%90%8B%F0%9D%90%9E%F0%9D%90%9A%F0%9D%90%AB%F0%9D%90%A7%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%A0_%F0%9D%90%92%F0%9D%90%AE%F0%9D%90%A9%F0%9D%90%9E%F0%9D%90%AB%F0%9D%90%AF%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%9E%F0%9D%90%9D_%F0%9D%90%92%F0%9D%90%AD%F0%9D%90%9A%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%9C%F0%9D%90%9A%F0%9D%90%A5_%F0%9D%90%8C%F0%9D%90%A8%F0%9D%90%9D%F0%9D%90%9E%F0%9D%90%A5%F0%9D%90%AC/"/>
        <updated>2024-02-13T17:45:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/victoriosus  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there a way to improve clustering by considering two different types of embeddings?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apzau5/d_is_there_a_way_to_improve_clustering_by/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apzau5/d_is_there_a_way_to_improve_clustering_by/"/>
        <updated>2024-02-13T17:36:25.000Z</updated>
        <summary type="html"><![CDATA[Suppose you generate embeddings using two different pre-trained encoders. Is there a way I can leverage these two types of embeddings to get an improved clustering of the data? Is there a term for this, or some prior literature I can read?
    submitted by    /u/fullgoopy_alchemist  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can’t Access Private Gradio Spaces API Endpoint Through Postman]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apz1ua/cant_access_private_gradio_spaces_api_endpoint/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apz1ua/cant_access_private_gradio_spaces_api_endpoint/"/>
        <updated>2024-02-13T17:26:34.000Z</updated>
        <summary type="html"><![CDATA[I am trying to access my Huggingface space through a Postman request. When I copy the exported curl command, it looks like this:
  curl --location 'https://MY_SPACE_NAME_HERE.hf.space/--replicas/RANDOMDIGITS/' \ --header 'Content-Type: application/json' \ --header 'Authorization: Bearer hf_REST_OF_MY_TOKEN_HERE' \ --data '{"data": ["data:image/jpeg;base64,PIC_IN_BASE64_HERE"]}'  
 But when I run this request, I get:
  { "detail": "Method Not Allowed" }  
 How do get this request to succeed? 
 Also my API endpoint works perfectly when I access it via the boilerplate python code that Gradio gives me. Do you know what the problem is?
    submitted by    /u/warpanomaly  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlock AI's Power: Top Free Courses on ChatGPT & Large Language Models| DeepLearning.AI]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apyyrd/unlock_ais_power_top_free_courses_on_chatgpt/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apyyrd/unlock_ais_power_top_free_courses_on_chatgpt/"/>
        <updated>2024-02-13T17:23:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/UseCreative4765  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there a need for compiled language in complex machine learning projects?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apyymt/d_is_there_a_need_for_compiled_language_in/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apyymt/d_is_there_a_need_for_compiled_language_in/"/>
        <updated>2024-02-13T17:22:57.000Z</updated>
        <summary type="html"><![CDATA[We all know that machine learning scene is dominated by Python, simple, yet slow language. From my experience languages like Rust, C++, C and Zig are on average 10x faster (it may vary a LOT of course, and the use of C-written modules doesn't help that much for multiple reasons), but they suffer from a need to compile, which takes some time out of development. Still for complex algorithms used in ML they have a clear advantage, once the development process slows down. Why do they have so small market share than? So far I have found literally ONE framework dedicated to run trained NN models called ggml. 
 Edit: I have forgot that TensorFlow is actually, mostly written in c++. So that counters my point about compiled languages not having much market share.
    submitted by    /u/LetsNya  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] 3K ML Build Discussion]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apyymg/d_3k_ml_build_discussion/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apyymg/d_3k_ml_build_discussion/"/>
        <updated>2024-02-13T17:22:56.000Z</updated>
        <summary type="html"><![CDATA[Hello all, 
 I am making a custom build pc and want to make sure I can use it for ML since I am taking ml courses currently and would also like to train and test my own models. Disclaimer the build was mostly meant for gaming in mind since that is another use case I will be using it for although I am open to changing parts to increase ML effectiveness.
 I am posting here to check if it’s compatible and effective for machine learning. I am open to part swap suggestions as well. 
 GPU: Aero 4080 super
 CPU: Intel Core i9-14900K
 RAM: Corsair Vengeance RGB 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory
 PSU: Corsair RMe (2023) 1200 W 80+ Gold Certified Fully Modular ATX Power Supply
 SSD: Crucial T500 W/Heatsink 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive
 Motherboard: Gigabyte Z790 AORUS ELITE AX ICE
 Cooler: Kraken Elite 360mm
 Case: Lian li vs nzxt (i am still deciding between nzxt mid-towers vs Lian Li O11 Dynamic EVO ATX Mid Tower Case, so for sizing you can keep both in mind however currently favoring lian li o11 slightly)
 Link to build: https://pcpartpicker.com/list/JQckmD
    submitted by    /u/justdoitjustice  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How are positional embedding solved when training and test length is different?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apyx4t/d_how_are_positional_embedding_solved_when/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apyx4t/d_how_are_positional_embedding_solved_when/"/>
        <updated>2024-02-13T17:21:21.000Z</updated>
        <summary type="html"><![CDATA[As per the title, how is the problem treated and what is the literature?
 Further, I have sequences that should be positioned on a 3D lattice but whatever, the point is the training is at way shorter length.
    submitted by    /u/reverendCappuccino  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Tesseract on images of text lines + transcriptions?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apyvgp/training_tesseract_on_images_of_text_lines/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apyvgp/training_tesseract_on_images_of_text_lines/"/>
        <updated>2024-02-13T17:19:30.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.
 Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.
 Any help is highly appreciated!
    submitted by    /u/SirVampyr  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Tesseract on images of text lines + transcriptions?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apyv59/training_tesseract_on_images_of_text_lines/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apyv59/training_tesseract_on_images_of_text_lines/"/>
        <updated>2024-02-13T17:19:07.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.
 Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.
 Any help is highly appreciated!
    submitted by    /u/SirVampyr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Training Tesseract on images of text lines + transcriptions?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apyuj5/p_training_tesseract_on_images_of_text_lines/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apyuj5/p_training_tesseract_on_images_of_text_lines/"/>
        <updated>2024-02-13T17:18:29.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.
 Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.
 Any help is highly appreciated!
    submitted by    /u/SirVampyr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Your Toxic Relationships]]></title>
        <id>https://medium.com/p/012eff24ce1b</id>
        <link href="https://medium.com/illumination/understanding-your-toxic-relationships-012eff24ce1b?source=rss------deep_learning-5"/>
        <updated>2024-02-13T17:07:16.000Z</updated>
        <summary type="html"><![CDATA[Healing With Self Awareness
Continue reading on ILLUMINATION »]]></summary>
        <author>
            <name>Joe Saviano</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3 Research-Driven Advanced Prompting Techniques for LLM Efficiency and Speed Optimization]]></title>
        <id>https://www.kdnuggets.com/?p=164018</id>
        <link href="https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization"/>
        <updated>2024-02-13T17:00:06.000Z</updated>
        <summary type="html"><![CDATA[This article has explored three promising prompting techniques that have been developed to reduce the occurrence of hallucinations in large language models.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-head in transformers]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apy9uf/multihead_in_transformers/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apy9uf/multihead_in_transformers/"/>
        <updated>2024-02-13T16:55:47.000Z</updated>
        <summary type="html"><![CDATA[I'm a newcomer to transformers and have a question about multi-head usage. I've gathered from various blogs that it helps capture word importance in different contexts. For instance, with 2 heads and a token size of 516, the embeddings split into two parts processed separately. If a word like 'king' represents gender and royalty in different dimensions, having them in separate halves enables the model to learn both aspects. However, if gender and royalty are in the same half, can the model still discern their importance individually, or is it limited to focusing on one aspect at a time?
    submitted by    /u/thestorytellerixvii  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which model benefited the most from hyperparameter tuning?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apxv7d/which_model_benefited_the_most_from/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apxv7d/which_model_benefited_the_most_from/"/>
        <updated>2024-02-13T16:39:43.000Z</updated>
        <summary type="html"><![CDATA[Is it possible to see which model benefited the most from hyperparameter tuning? I think we can say it's not the random forest classifier since its results are pretty good and even.
 https://preview.redd.it/y1c8q252sdic1.png?width=1766&format=png&auto=webp&s=4c681106c16a537fc54eec5d63e42c3784d1aa45
    submitted by    /u/nipaldi  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models - University of Washington 2024 - Over 10x faster in inference than existing systems!]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apxr2n/r_fiddler_cpugpu_orchestration_for_fast_inference/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apxr2n/r_fiddler_cpugpu_orchestration_for_fast_inference/"/>
        <updated>2024-02-13T16:34:59.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.07033 
 Github: https://github.com/efeslab/fiddler 
 Abstract:
  
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over 3 tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods.
  
https://preview.redd.it/q9l3fciyqdic1.jpg?width=1338&format=pjpg&auto=webp&s=2e39726c970c655d6ee39f2b68c323204c6b2289
 https://preview.redd.it/epjd0fiyqdic1.jpg?width=1661&format=pjpg&auto=webp&s=701a2d61f8ab50d054db0301a30e40119898dab6
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apxjn6/deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apxjn6/deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/"/>
        <updated>2024-02-13T16:26:39.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.
 vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).
 See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html
 In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.
 Please don't hesitate to share feedbacks about this article, it will be very much appreciated!
 Julien
    submitted by    /u/juliensalinas  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apxg42/d_deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apxg42/d_deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/"/>
        <updated>2024-02-13T16:22:44.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.
 vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).
 See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html
 In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.
 Please don't hesitate to share feedbacks about this article, it will be very much appreciated!
 Julien
    submitted by    /u/juliensalinas  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deploy LLaMA 2, Mistral, and Mixtral, on AWS EC2 with vLLM]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1apxedy/deploy_llama_2_mistral_and_mixtral_on_aws_ec2/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1apxedy/deploy_llama_2_mistral_and_mixtral_on_aws_ec2/"/>
        <updated>2024-02-13T16:20:54.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.
 vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).
 See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html
 In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.
 Please don't hesitate to share feedbacks about this article, it will be very much appreciated!
 Julien
    submitted by    /u/juliensalinas  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardware requirements]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apwzte/hardware_requirements/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apwzte/hardware_requirements/"/>
        <updated>2024-02-13T16:04:43.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I am on a Mac M1 and want to do model training on images, obviously no GPU. Last time I tried it took me forever just for a few iterations, when for good results I'd need minimum 400k and ideally more than a 1 million.
 I was interested into cloud computing solutions, but when I looked at the pricing (e.g. google cloud tensorflow) my wallet started to shake unexplainably.
 Do any of you have any recommendation in terms of what are the best options out there for my purpose?
    submitted by    /u/friedbat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] A framework to share analytics data in GStreamer]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apwwme/research_a_framework_to_share_analytics_data_in/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apwwme/research_a_framework_to_share_analytics_data_in/"/>
        <updated>2024-02-13T16:01:09.000Z</updated>
        <summary type="html"><![CDATA[GStreamer has long been the best framework to build pipelines to handle video streams, and in particular, live ones. Engineers have widely adopted it to build video analytics pipelines, and while many companies have indeed built their machine learning analysis framework around GStreamer, no one had made the effort to contribute upstream, until now. 
 https://www.collabora.com/news-and-blog/news-and-events/a-framework-to-share-analytics-data-in-gstreamer.html
    submitted by    /u/mfilion  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] OS-Copilot: Towards Generalist Computer Agents with Self-Improvement - Shanghai AI Laboratory 2024]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apwlkm/r_oscopilot_towards_generalist_computer_agents/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apwlkm/r_oscopilot_towards_generalist_computer_agents/"/>
        <updated>2024-02-13T15:48:33.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2402.07456 
 Github: https://github.com/OS-Copilot/FRIDAY 
 Abstract:
  
Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents. 
  
https://preview.redd.it/uzec8udohdic1.jpg?width=1655&format=pjpg&auto=webp&s=893b5561ca47c26c789b69925efdc26e5b783007
 https://preview.redd.it/vfwfwudohdic1.jpg?width=1653&format=pjpg&auto=webp&s=9eafc2a5ea0ad188a156d3de446508d82d9cc913
 https://preview.redd.it/lmi8rwdohdic1.jpg?width=1123&format=pjpg&auto=webp&s=dbc67b27585b980d0c592f9bd9f87f3ec6531f66
 https://preview.redd.it/20yo21eohdic1.jpg?width=1037&format=pjpg&auto=webp&s=72fab36d585b862eed4ff6c7deed2be0cd62f637
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Applications of GNNs - A survey (video)]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apwl8v/r_applications_of_gnns_a_survey_video/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apwl8v/r_applications_of_gnns_a_survey_video/"/>
        <updated>2024-02-13T15:48:08.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I'm sharing my overview explainer video of Graph Neural Networks (GNN) applications:
 🎥 https://youtu.be/9QH6jnwqrAk?si=nEARUXquZ0aetjCD
 I've compiled a batch of info in one video, highlighting recent breakthroughs and concrete applications of GNNs in 7 diverse areas.
 GNNs have been making rapid crazy strides recently. Despite much less hype than other AI buzzwords, they have powered numerous achievements in the last year alone.
 I plan to create more content on GNNs, like a short series that will dive into (some) technical details of how GNNs work and more. It would be very helpful to hear your thoughts on this one!
    submitted by    /u/mrx-ai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How much metadata is in a photo?]]></title>
        <id>https://www.johndcook.com/blog/?p=242879</id>
        <link href="https://www.johndcook.com/blog/2024/02/13/photo-metadata/"/>
        <updated>2024-02-13T15:44:47.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about the privacy implications of metadata in a PDF. This post will do the same for photos. You can see the metadata in a photo using exiftool. By default cameras include time and location data. I ran this tool on a photo I took in Seattle a few years […]
How much metadata is in a photo? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Math and Code Behind K-Means Clustering]]></title>
        <id>https://medium.com/p/795582423666</id>
        <link href="https://towardsdatascience.com/the-math-and-code-behind-k-means-clustering-795582423666?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T15:21:24.000Z</updated>
        <author>
            <name>Cristian Leo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite Automata Simulation for Leveraging AI-Assisted Systems]]></title>
        <id>https://medium.com/p/9d50b36bcbd3</id>
        <link href="https://towardsdatascience.com/bird-by-bird-using-finite-automata-9d50b36bcbd3?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T15:15:51.000Z</updated>
        <author>
            <name>Sofya Lipnitskaya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I am confused about what I should do]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apvhbe/i_am_confused_about_what_i_should_do/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apvhbe/i_am_confused_about_what_i_should_do/"/>
        <updated>2024-02-13T15:01:42.000Z</updated>
        <summary type="html"><![CDATA[I learn the basic classification and regression model(ex: SVR,decision tree,random forest etc).What should I do next???
 ​
    submitted by    /u/Personal-Novel-7171  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2024 Tech Trends: AI Breakthroughs & Development Insights from O’Reilly’s Free Report]]></title>
        <id>https://www.kdnuggets.com/?p=163981</id>
        <link href="https://www.kdnuggets.com/2024-tech-trends-ai-breakthroughs-development-insights-oreilly-free-report?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=2024-tech-trends-ai-breakthroughs-development-insights-from-oreillys-free-report"/>
        <updated>2024-02-13T15:00:51.000Z</updated>
        <summary type="html"><![CDATA[Want to prepare your tech career for 2024 and onwards? Have a look at O’Reilly’s FREE technology trends report.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My Code Run Without Error But The Resulted Model Clasified The Apple Leafes Disesases Incorrectly]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apve3o/my_code_run_without_error_but_the_resulted_model/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apve3o/my_code_run_without_error_but_the_resulted_model/"/>
        <updated>2024-02-13T14:57:55.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. I'm still a beginner. I want to ask what's wrong in my code that the resulted model didn't classify the apple leaf diseases correctly? the code run without error when I run the code, but the disease clasification is false. I trained this on 7k images apple leaves diseases dataset with 4 classes, each class has images around 1.8k. The total images of each class is not equal, is this affect the model clasification? or is there something wrong in my code below?
 ```python from google.colab import drive drive.mount('/content/gdrive')
 import zipfile zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/dataset/data9k.zip', 'r') zip_ref.extractall("/content/dataset") zip_ref.close()
 import tensorflow as tf from tensorflow.keras.applications.imagenet_utils import preprocess_input i…]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I love this paper but it’s barely been noticed.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49548</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/13/i-love-this-paper-but-its-barely-been-noticed/"/>
        <updated>2024-02-13T14:57:14.000Z</updated>
        <summary type="html"><![CDATA[Econ Journal Watch asked me and some others to contribute to an article, “What are your most underappreciated works?,” where each of us wrote 200 words or less about an article of ours that had received few citations. Here’s what … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lag-Llama: Open-Source Foundation Model for Time Series Forecasting]]></title>
        <id>https://medium.com/p/9afdfaf2bd7c</id>
        <link href="https://towardsdatascience.com/lag-llama-open-source-foundation-model-for-time-series-forecasting-9afdfaf2bd7c?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T14:52:49.000Z</updated>
        <summary type="html"><![CDATA[Explore the architecture of Lag-Llama and learn to apply it in a forecasting project using Python
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Marco Peixeiro</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] [P] 10 times faster LLM evaluation with bayesian optimization]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/"/>
        <updated>2024-02-13T14:51:30.000Z</updated>
        <summary type="html"><![CDATA[Recently I've been working on making LLM evaluations fast by using bayesian optimization to select a sensible subset.
 Bayesian optimization is used because it’s good for exploration / exploitation of expensive black box (paraphrase, LLM).
 Project link
 I would love to hear your thoughts and suggestions on this!
    submitted by    /u/b06901038g  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Why do object detection model adversaries look different from image classifiers]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/"/>
        <updated>2024-02-13T14:48:37.000Z</updated>
        <summary type="html"><![CDATA[Hello people. I was just messing around to see the behavior for adversarial attacks on image classifiers, and decided to try it with an object detector as well. I noticed that an untargeted adversarial attack on these models yielded some interested masks. The image classifier generated the usually expected noise mask that is popular, but the object detector under the same conditions generated a mask that closely resembles the objects in question. What is the reasoning behind this? Thank you for your help!
 The first picture is the adversarial mask for the object detector, the second one for an image classifier, and the last picture is the original picture.
 https://preview.redd.it/abo3yj7xddic1.png?width=425&format=png&auto=webp&s=0e73a11997b2c27a6f73832204862d97e5847b4a
 https://preview.redd.it/mbsi6k7xddic1.png?width=425&format=png&auto=webp&s=41de2eca4348afbddfb36154da514046b1be78be
 https://preview.redd.it/zusv0k7xddic1.jpg?width=400&format=pjpg&auto=webp&s=cf08f3911c24e10c632b12e42a976cd35ff3f490
    submitted by    /u/tatteredsky  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Setting A Dockerized Python Environment — The Hard Way]]></title>
        <id>https://medium.com/p/e62531bca7a0</id>
        <link href="https://towardsdatascience.com/setting-a-dockerized-python-environment-the-hard-way-e62531bca7a0?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T14:45:42.000Z</updated>
        <author>
            <name>Rami Krispin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it possible to train a os llm with my own data?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apuw1a/is_it_possible_to_train_a_os_llm_with_my_own_data/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apuw1a/is_it_possible_to_train_a_os_llm_with_my_own_data/"/>
        <updated>2024-02-13T14:34:44.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am working on an internal GPT of some sorts and I dont get the best results with RAG. My question is there a way to download an os llm from huggingface and to continue the training with the internal data. ? 
    submitted by    /u/Longjumping_Fruit843  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Am I misunderstanding what GPT-3 training data is?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apurzj/am_i_misunderstanding_what_gpt3_training_data_is/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apurzj/am_i_misunderstanding_what_gpt3_training_data_is/"/>
        <updated>2024-02-13T14:29:32.000Z</updated>
        <summary type="html"><![CDATA[Hi there I hope you're all well,
 I am writing a paper with my PhD supervisor and it has been noted that these values add up to 101%.
 Am I misunderstanding what this represents or should it be 100%?
 Thank you for any information you may be able to provide.
 ​
 ​
 https://preview.redd.it/xi33pf705dic1.png?width=948&format=png&auto=webp&s=01e29567fe9ae50fc4b8f2bd1219891bd83ae695
 ​
 https://preview.redd.it/5uz3zqlx4dic1.png?width=357&format=png&auto=webp&s=a857e8425a4b8ef23faff5e0d08ce53fde831c9c
    submitted by    /u/richard93UK  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Books and reference sites, newbie here]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aptd69/books_and_reference_sites_newbie_here/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aptd69/books_and_reference_sites_newbie_here/"/>
        <updated>2024-02-13T13:23:26.000Z</updated>
        <summary type="html"><![CDATA[guys can you suggest a reference or tutorial series or a book for machine learning ? my college professors sometimes don't explain what they are writing, like I understood the k-means algo , all the theory they taught but I am confused about polynomial regression and what is regularized linear regression
 a book or a reference site maybe helpful
 thanks for the upcoming answers !
    submitted by    /u/kichiDsimp  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How To Comment Your Python Code as a Data Scientist]]></title>
        <id>https://www.kdnuggets.com/?p=163961</id>
        <link href="https://www.kdnuggets.com/how-to-comment-your-python-code-as-a-data-scientist?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-comment-your-python-code-as-a-data-scientist"/>
        <updated>2024-02-13T13:00:18.000Z</updated>
        <summary type="html"><![CDATA[Don’t overlook these essential aspects of programming activity.]]></summary>
        <author>
            <name>Cornellius Yudha Wijaya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Borwein integrals]]></title>
        <id>https://www.johndcook.com/blog/?p=242881</id>
        <link href="https://www.johndcook.com/blog/2024/02/13/borwein-integrals/"/>
        <updated>2024-02-13T12:56:42.000Z</updated>
        <summary type="html"><![CDATA[The Borwein integrals introduced in [1] are a famous example of how proof-by-example can go wrong. Define sinc(x) as sin(x)/x. Then the following equations hold. However where δ ≈ 2.3 × 10−11. This is where many presentations end, concluding with the moral that a pattern can hold for a while and then stop. But I’d […]
The Borwein integrals first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Having a hard time with theory heavy courses]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apsdu9/having_a_hard_time_with_theory_heavy_courses/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apsdu9/having_a_hard_time_with_theory_heavy_courses/"/>
        <updated>2024-02-13T12:33:27.000Z</updated>
        <summary type="html"><![CDATA[I went through some courses where there's only theory and not practical work, like CS229 and Deepmind UCL Intro to RL, and I feel like I haven't learnt a thing. I tkae notes of everything shown in slides, written on boards but don't understand them at all. I miss what they are talking about/expaining and my notes makes no sense to me.
 I feel like I'd have learnt more if I just sat down and listened to what they said while looking at their displayed slides. I don't actually need these notes since I don't review them, for an exam or something else unlike I would in irl courses.
    submitted by    /u/open_23  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[2402.07901] FAST: Factorizable Attention for Speeding up Transformers]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/"/>
        <updated>2024-02-13T12:00:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Elven77AI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI in sports! slow-motion reviews [Project]]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aprjg8/ai_in_sports_slowmotion_reviews_project/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aprjg8/ai_in_sports_slowmotion_reviews_project/"/>
        <updated>2024-02-13T11:45:15.000Z</updated>
        <summary type="html"><![CDATA[A interesting approach using FILM: Frame Interpolation for Large Motion to further smooth down slow motion videos which are quite hard to watch and indecisive. 
 Video demo here
 Any suggestions or further improvements?
    submitted by    /u/ade17_in  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New to AI? Join Our Beginner Meetup!]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apqki2/new_to_ai_join_our_beginner_meetup/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apqki2/new_to_ai_join_our_beginner_meetup/"/>
        <updated>2024-02-13T10:42:03.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone! 
 Just wanted to post to see if anyone is interested in AI but not sure where to start? I'm hosting a no-pressure, beginner-friendly AI meetup on 21st February at 5pm AEST if anyone is interested. It's a great chance to:
  
Try hands-on AI activities.
 Ask questions in a supportive environment.
 Meet fellow AI newbies.
  
Details:
  
When: 21st February, 5pm AEST
 Where: Online
 RSVP: https://www.meetup.com/meetup-group-ynmkmrlc/events/299167242/?utm\_medium=referral&utm\_campaign=yourEvent\_savedevents\_share\_modal&utm\_source=link
  
Please feel to reach out with any questions and I hope to see you there!
    submitted by    /u/KeyMiddle32  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I need you help]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apq8jb/i_need_you_help/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apq8jb/i_need_you_help/"/>
        <updated>2024-02-13T10:19:31.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I'm a machine learning student trying to help friends who are organizing a festival.
 There will be around 300 volunteers and I thought I could automate the planning wich take a lot of time.
 There are different stands where volunteers are placed and this spread over two days : Friday and Saturday.
 Each volunteer chose their three favorite stands in an order of preference from a list.
 Here the file Pole.xlsx with the name of each stand in color
 They also indicate the people they would like to be with with the format :
 BARACK Obama, BARACK Obama, BARACK Obama, ...
 So quite a few conditions... I get an Excel extraction of the first answers.
 Here are the headers of the file I keep : 
 Inscriptions.xlsx
 I've already tried to do it with a greedy algorithm and Gale Shapley but I can't achieve much.
 Do you have any ideas for accomplishing this task ? 
 Thanks in advance.
    submitted by    /u/Hmouimaisnon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplified Transformers?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1appqes/simplified_transformers/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1appqes/simplified_transformers/"/>
        <updated>2024-02-13T09:43:38.000Z</updated>
        <summary type="html"><![CDATA[I understand how revolutionary the transformer architecture is. However, while studying it, it's crazy how much details this thing contains, which makes it harder for everyone to understand/enhance.
 My question is, have there been any attempts to build a simplified transformer architecture? something akin to how GRUs are kind of a simplified version of LSTMs. 
 Would really appreciate it if you could point me to any papers that have tried to accomplish that, or maybe find any unnecessary/less necessary elements in the original 2017 paper.
    submitted by    /u/Salloum-A  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Books to Learn Tensorflow in 2024 for beginners & Advanced -]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1appe1h/best_books_to_learn_tensorflow_in_2024_for/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1appe1h/best_books_to_learn_tensorflow_in_2024_for/"/>
        <updated>2024-02-13T09:18:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sreeravan  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I need to deploy an AI model wrapped by a REST API to the cloud. Which cloud provider should I use?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apoveo/i_need_to_deploy_an_ai_model_wrapped_by_a_rest/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apoveo/i_need_to_deploy_an_ai_model_wrapped_by_a_rest/"/>
        <updated>2024-02-13T08:40:44.000Z</updated>
        <summary type="html"><![CDATA[From what I understand, after a data scientist hands me the models binaries, I should load it into Pytorch. Then I create endpoints our client can call. The input of the client is placed through transformers, then fed into the model which spits out a prediction. I then return the prediction in the HTTP response.
 Is this done in any regular ol' CPU cloud VM like a Digital Ocean droplet? Or should I use deploy the model separately in a Paperspace GPU and call it from a Flask server in a droplet? The details are quite fuzzy for me on this.
    submitted by    /u/Radiant-Message9493  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Embeddings: Comprehensive Guide]]></title>
        <id>https://medium.com/p/afd97fce8fb5</id>
        <link href="https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T08:03:13.000Z</updated>
        <author>
            <name>Mariya Mansurova</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Distributions with Earth Mover’s Distance]]></title>
        <id>https://medium.com/p/71f714440923</id>
        <link href="https://towardsdatascience.com/comparison-of-distributions-with-earth-movers-distance-71f714440923?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T07:55:44.000Z</updated>
        <summary type="html"><![CDATA[Understanding EMD through theory and from-scratch calculation
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Jarom Hulet</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Linear Equations for ML?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apnf51/solving_linear_equations_for_ml/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apnf51/solving_linear_equations_for_ml/"/>
        <updated>2024-02-13T06:58:44.000Z</updated>
        <summary type="html"><![CDATA[I’m reading the book “Math for Machine Learning” and there’s a Linear Algebra section that focuses on solving linear equations (reduced row echelon form, elementary transformations, calculating the inverse). How applicable is this to learning ML. Should I actually solve these linear equations by hand to prepare to understand ML?
    submitted by    /u/Droski_  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R]PFAS might be a better embedding space for words compared to vectors and possibly even more compute efficient]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/"/>
        <updated>2024-02-13T05:13:48.000Z</updated>
        <summary type="html"><![CDATA[https://huggingface.co/blog/TuringsSolutions/pfafresearch
 So this blog went over most peoples heads but basically it captures a lot more information of words compared to vectors it actually seems like it could potentially be better then vectors when scaled up. It has more dimensionality compared to vectors. Other geometric representations are being looked into as well like triangle space, square space and more to find the optimal geometric encoding for word relationships that might be better at modeling information compared to vectors. It has slight score improvements on some benchmarks so as a proof of concept it really does seem to work. Essentially how it works is it converts words to a fractal style numerical embedding to capture richer information.
 https://arxiv.org/abs/2402.06184
 Ironically training was visualized as a fractal recently too so it shouldn't be surprising a word2fractal style embedding process was theorized as well.
    submitted by    /u/TheCrazyAcademic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My company gave me a 500-dollar education budget. Looking for recommendations]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aplhu5/my_company_gave_me_a_500dollar_education_budget/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aplhu5/my_company_gave_me_a_500dollar_education_budget/"/>
        <updated>2024-02-13T05:05:59.000Z</updated>
        <summary type="html"><![CDATA[Any recommendations on courses, books, or other materials that are worth paying for? TIA.
    submitted by    /u/myotheraccount7071  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Need Help Creating a TFLite ModelMaker EfficientDet-Lite0 Detector-like Wrapper for YOLOv8]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apkro9/p_need_help_creating_a_tflite_modelmaker/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apkro9/p_need_help_creating_a_tflite_modelmaker/"/>
        <updated>2024-02-13T04:27:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Geo_The_Legend  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferential Insights: How Confidence Intervals Illuminate the Ames Real Estate Market]]></title>
        <id>https://machinelearningmastery.com/?p=15635</id>
        <link href="https://machinelearningmastery.com/inferential-insights-confidence-intervals/"/>
        <updated>2024-02-13T03:29:30.000Z</updated>
        <summary type="html"><![CDATA[In the vast universe of data, it’s not always about what we can see but rather what we can infer. Confidence intervals, a cornerstone of inferential statistics, empower us to make educated guesses about a larger population based on our sample data. Using the Ames Housing dataset, let’s unravel the concept of confidence intervals and […]
The post Inferential Insights: How Confidence Intervals Illuminate the Ames Real Estate Market appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any good ML workshops in the US?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apjf83/any_good_ml_workshops_in_the_us/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apjf83/any_good_ml_workshops_in_the_us/"/>
        <updated>2024-02-13T03:19:51.000Z</updated>
        <summary type="html"><![CDATA[I’m an engineer in tech industry working with mostly ML research scientists. My dept is encouraging me to take a course or tutorial/workshop and I want to use the opportunity to learn more in the ML space. Wondering if there were any good workshops in person you all know of? I’ve got a baseline understanding and have taken ML coursework, but would love to find a workshop / week long course that deep dives into an area or multiple areas of ML.
    submitted by    /u/drdrrr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing Bite-Sized Coding Problems for ML]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1aphj8p/announcing_bitesized_coding_problems_for_ml/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1aphj8p/announcing_bitesized_coding_problems_for_ml/"/>
        <updated>2024-02-13T01:49:20.000Z</updated>
        <summary type="html"><![CDATA[Hey guys,
 u/NeetCode and I are excited to announce coding problems for AI/ML that you can solve in your browser and run against test cases. They assume no prior background knowledge in AI/ML. They work up from linear regression to coding and training a GPT chat model from scratch!
 For each problem, I also created a 5-10 minute background video covering the concepts needed to solve the problem (or quiz, for the topics that have multiple choice quizzes to go along with them) as well as a solution video.
 All the videos for the problems, and 2x a week concept overviews on different ML topics (suggestions welcome!) can be found on my channel: https://www.youtube.com/@GPTandChill
 The problem list can be found here on NeetCode's site https://neetcode.io/practice?subpage=practice&tab=coreSkills&topic=Machine%20Learning OR here on my site https://www.gptandchill.ai/leetcode-for-ml
 And here are Navi's posts for some additional context:
 https://x.com/neetcode1/status/1756997643556041191?s=20
 https://www.linkedin.com/posts/activity-7162822685037674496-i0Yo?utm_source=share&utm_medium=member_desktop
 Let us know if you like this kind of educational content or have any feedback!
    submitted by    /u/GPTandChill  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Preprocessing Help - Beginner Here]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aphhrr/data_preprocessing_help_beginner_here/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aphhrr/data_preprocessing_help_beginner_here/"/>
        <updated>2024-02-13T01:47:24.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/s13qle1kc9ic1.png?width=1546&format=png&auto=webp&s=d67b8be7f6036eec39988b5a741f16edb6d0ac6e
 Beginner Here - i uploaded a snippet of the dataset 
 Am trying to create the infamous price prediction model for houses.
 I uploaded my dataset, reduced some data columns, splitted the independent variables and the target variable, then split them into train and test datasets, detected(using histograms and boxplots) and deleted some outliers, scaled the features, and label encoded the non-numerical data [ oneHotEncoding is impossible because of TOO MANY unique values] and finally trained my model - linear regression using Sklearn.
 I scaled the test independent variables and I got an R2 score of about 0.2 and a high MAE score.
 Problem: I can't understand why the score are bad. The steps of the model training are correct. So probably the data preprocessing is wrong. I tried other model training such as knn regression and random forest regression but ended up with bad results. 
 More details:
 - Deleted "City" and "Province" columns since I have longitude and latitude.
 - Label encoded "Address". Removing does not do anything and I think it should not be removed as the prices changes drastically between different values of this feature.
    submitted by    /u/gtfryh352  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Transformer based language model from scratch]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aphaic/training_transformer_based_language_model_from/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aphaic/training_transformer_based_language_model_from/"/>
        <updated>2024-02-13T01:37:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gubberex  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Transformer based language model from scratch]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aph3iw/training_transformer_based_language_model_from/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aph3iw/training_transformer_based_language_model_from/"/>
        <updated>2024-02-13T01:28:36.000Z</updated>
        <summary type="html"><![CDATA[Hi All,
 I tried to implement Attention is all you need paper to train language models based on the Transformer architecture, but failing to train these models. The loss or perplexity for that matter, reduces to some extent, but than it becomes stagnant. I am running out of ideas here, like what could be the reason for that. To access the scripts click here. Any help in this regard is appreciated.
 Thanks in advance.
 Cheers 
    submitted by    /u/gubberex  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Machine Learning Engineers work on robotics?]]></title>
        <id>https://www.reddit.com/r/learnmachinelearning/comments/1apgtuh/do_machine_learning_engineers_work_on_robotics/</id>
        <link href="https://www.reddit.com/r/learnmachinelearning/comments/1apgtuh/do_machine_learning_engineers_work_on_robotics/"/>
        <updated>2024-02-13T01:15:41.000Z</updated>
        <summary type="html"><![CDATA[I'm very interested in both Robotics and AI, and I was wondering whether or not I could be a Machine Learning Engineer in robotics. 
    submitted by    /u/Daniu_13  
 [link]   [comments]]]></summary>
        <author>
            <name>Learn Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Tracking Healthcare Domain Conference Deadlines]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/"/>
        <updated>2024-02-13T01:14:39.000Z</updated>
        <summary type="html"><![CDATA[I was having trouble tracking deadlines for healthcare ML conferences, so I updated the deadlines website, Added 6 conferences/workshops for healthcare domain.
 https://openlifescience-ai.github.io/ai-deadlines/?sub=ML,CV,NLP,SP 
 Now track easily the paper submission deadline & even add it to your Google calendar.
 See a conference missing? Please contribute here: https://github.com/openlifescience-ai/ai-deadlines
 https://preview.redd.it/dfxxlep679ic1.png?width=829&format=png&auto=webp&s=686aa600ad863b26d28211021e802708a79957a8
    submitted by    /u/aadityaura  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity Analysis for Unobserved Confounding]]></title>
        <id>https://medium.com/p/465970a969e0</id>
        <link href="https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-13T00:27:50.000Z</updated>
        <summary type="html"><![CDATA[How to know the unknowable in observational studies]]></summary>
        <author>
            <name>Ugur Yildirim</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource-constrained Stereo Singing Voice Cancellation]]></title>
        <id>resource-constrained</id>
        <link href="https://machinelearning.apple.com/research/resource-constrained"/>
        <updated>2024-02-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not just NVIDIA: GPU programming that runs everywhere]]></title>
        <id>https://pythonspeed.com/articles/gpu-without-cuda/</id>
        <link href="https://pythonspeed.com/articles/gpu-without-cuda/"/>
        <updated>2024-02-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[If you’re doing computations on a GPU, NVIDIA is the default, alongside its CUDA libraries.
Some libraries like PyTorch support do support AMD GPUs and Macs.
But from the re-implementations of NumPy, SciPy, and Pandas in the RAPIDS project, to Numba’s GPU support, NVIDIA has best software support in the Python world.
Sticking to NVIDIA-specific software has some downsides, however:
It won’t run on modern Mac laptops.
Testing in CI is more difficult: you need custom runners that have NVIDIA GPUs.
You can’t use any other GPUs you might have access to, like AMD GPUs.
What can you do if you want to use GPUs in a portable manner?
In this article we’ll cover one option, the wgpu-py library.
Read more...]]></summary>
        <author>
            <name>Python⇒Speed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whats in your RAG setup? [D]]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/"/>
        <updated>2024-02-12T22:14:07.000Z</updated>
        <summary type="html"><![CDATA[What frameworks and libraries are you using in your RAG? 
 I'm most curious if LangChain is as popular as it was?
 Here's mine at a high-level: 
  
 langchain to use OpenAI for creating embeddings
 Pinecone for storing embedding
 langchain to load document splitters and characters splitters for chunking
 Mongo for conversations memory
  
​
    submitted by    /u/EnvironmentalDepth62  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need help in advancing in AI field as a new grad]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apcefp/need_help_in_advancing_in_ai_field_as_a_new_grad/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apcefp/need_help_in_advancing_in_ai_field_as_a_new_grad/"/>
        <updated>2024-02-12T22:01:56.000Z</updated>
        <summary type="html"><![CDATA[I studied computer engineering (2023 pass out) in India and am currently employed as a machine learning engineer at a growing startup
 ​
 A little background about me 
 - Average Student in College
 - Can do average coding in python, Data Analysis, SQL, ML/DL, CV/NLP, GenAI, etc. 
 - Tried academia in my last year but felt like it was not my cup of tea
 - Hunt for internships and got one; handled the whole ML part alone for 3-4 months since the previous team left
 - Placed at the same startup as MLE (~Rs.12 LPA)
 - Spent first time out of my hometown alone in new city.
 - Felt good in initial 3-4 months where I had parties, worked day-night and had fun
 - Slowly I realized how fast things are moving in the outside world and I was still stuck at GPT2 
 ​
 My problem:
 - As the field progres…]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help, my GPT2 model responds to prompts with nonsense answers.]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1apbxr2/help_my_gpt2_model_responds_to_prompts_with/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1apbxr2/help_my_gpt2_model_responds_to_prompts_with/"/>
        <updated>2024-02-12T21:43:29.000Z</updated>
        <summary type="html"><![CDATA[I have been trying to feed my model with data related to chess rules, but the model responds nonsensically. 
 The data I have used is a plain text file which is not very large. But what I don't know is whether the problem is that the data set is too small or that the information is not well structured. can anyone give me some guidance? 
 How should a plain text dataset be structured? And if I convert it to csv what structure should it have? Also, if I want to create a much bigger dataset with the chess rules theme, how can I do it? Thanks anyway
    submitted by    /u/Guilty0121  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question regarding the training process of a neural network]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1apbbwv/question_regarding_the_training_process_of_a/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1apbbwv/question_regarding_the_training_process_of_a/"/>
        <updated>2024-02-12T21:18:56.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I am currently training a neural network for a regression task (to predict prices in housing dataset). I have been testing out different parameter combinations on my network but noticed something a bit weird. When training my model, the first epoch always as an abnormal large value that suddenly decreases by a lot:
 Epoch 1/20 loss: 0.4525 - mse: 0.4508 - val_loss: 0.1827 - val_mse: 0.0653
 Epoch 2/20 - loss: 0.1556 - mse: 0.0618 - val_loss: 0.1367 - val_mse: 0.0552 
 Epoch 3/20 loss: 0.1451 - mse: 0.0577 - val_loss: 0.1396 - val_mse: 0.0592 
 Epoch 4/20 loss: 0.1455 - mse: 0.0581 - val_loss: 0.1369 - val_mse: 0.0564 
 Epoch 5/20 loss: 0.1289 - mse: 0.0497 - val_loss: 0.0932 - val_mse: 0.0422 
 Epoch 6/20 - loss: 0.1300 - mse: 0.0502 - val_loss: 0.1053 - val_mse: 0.0482 
 Epoch 7/20 - loss: 0.1255 - mse: 0.0476 - val_loss: 0.1056 - val_mse: 0.0454 
 Epoch 8/20 - loss: 0.1271 - mse: 0.0482 - val_loss: 0.1416 - val_mse: 0.0537 
 Epoch 9/20 - loss: 0.1279 - mse: 0.0489 - val_loss: 0.1217 - val_mse: 0.0507 
 Epoch 10/20 - loss: 0.1232 - mse: 0.0489 - val_loss: 0.1232 - val_mse: 0.0464 
 I have tried batch normalization, dropout, learning_rate and do not see any improvements.
 My issue here is when looking at the plots of the train and validation loss i am unable to clearly see the difference between both metrics. I believe my model is appropriate for the data but i am unable to reduce that single first epoch any ideas? I have bellow the metrics that i have for both train and test. In this scenario should I not emphasize this plot as much? Thank you 
 Measure Train Test 
 MSE 0.03 0.04 
 MAE 0.09 0.09 
 R-squared 0.81 0.76 
    submitted by    /u/Minute-Fix-1493  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why does it matter that RMSNorm is faster than LayerNorm in transformers?]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/"/>
        <updated>2024-02-12T21:09:53.000Z</updated>
        <summary type="html"><![CDATA[A large fraction of recently released LLMs are using RMSNorm instead of LayerNorm.
 The original RMSNorm paper (https://arxiv.org/pdf/1910.07467.pdf) and most references I've seen argue that RMSNorm is better than LayerNorm because it is much more computationally efficient.
 However, LayerNorm is a tiny fraction of overall compute, so it's not clear to me why that speedup would help very much. Asymptotically, LayerNorm is O(d_model), while there are components like the MLP that are O(d_model2 ), or attention that is O(d_model*seq_len + d_model2 ).
 Is it just that the mean centering part of LayerNorm is not all that useful, and so RMSNorm gives you a minor efficiency boost without any important loss in expressivity? Or does RMSNorm have other benefits I'm not seeing?
    submitted by    /u/kei147  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Interview with Tri Dao, Stanford: On FlashAttention and sparsity, quantization, and efficient inference]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/"/>
        <updated>2024-02-12T20:20:59.000Z</updated>
        <summary type="html"><![CDATA[New episode of Imbue's Generally Intelligent podcast with Tri Dao, author of FlashAttention and Chief Scientist at Together AI.
 Some topics covered:
  
Taking a contrarian bet on recurrent connections over attention
 Using data augmentation to encode knowledge into models
 Designing algorithms that take advantage of hardware
  
Listen to the conversation:
  
Spotify
 Apple Podcasts
 Pocket Casts
 Highlights and referenced papers
  
   submitted by    /u/thejashGI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Has anyone studied computational linguistics/NLP in TRENTO: “Computational and Theoretical Modelling of Language and Cognition”]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ap9qdd/has_anyone_studied_computational_linguisticsnlp/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ap9qdd/has_anyone_studied_computational_linguisticsnlp/"/>
        <updated>2024-02-12T20:15:43.000Z</updated>
        <summary type="html"><![CDATA[I’d already written a post about my interest in choosing a master's degree in computational linguistics. I'm actually still undecided whether to pursue the academic world in traditional linguistics or to delve into NLP and computational linguistics. For the master's degree, I had already looked at the one in Language Technologies at Pisa and Konstanz ( Germany) and they both seemed really good until I saw the “Computational and Theoretical Modelling of Language and Cognition” course in Trento, which seems much, much better than the one in Pisa.
 I wanted to know in particular how is this MA, if it's really as great as it seems, and especially how affordable it is for a student who doesn't have a background in mathematics and computer science (I've only taken one statistics exam, but nothing too serious). I saw that the course doesn't require programming or math knowledge, however, there are mandatory exams in Mathematics and Machine Learning, and I wanted to know how feasible these courses actually are, whether they are very mathematical or more practical without requiring basic knowledge. Also, is it possible to find a job in the field after graduation, does the university provide connections with companies for its students?
    submitted by    /u/aquilaa91  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the best model for multiclass classification]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ap9fvz/what_is_the_best_model_for_multiclass/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ap9fvz/what_is_the_best_model_for_multiclass/"/>
        <updated>2024-02-12T20:04:37.000Z</updated>
        <summary type="html"><![CDATA[I have a csv file with 70000 rows and 20 columns. I want to make a model where I can input column values and the model will predict the column header for me. What is the best model to predict column headers using column values? How do I approach this task in a step by step manner?
    submitted by    /u/Leading_Particular60  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] The boundary of neural network trainability is fractal - Jascha Sohl-Dickstein]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/"/>
        <updated>2024-02-12T19:50:07.000Z</updated>
        <summary type="html"><![CDATA[Worth reading just for the neat visualizations.
 https://sohl-dickstein.github.io/2024/02/12/fractal.html
 https://arxiv.org/abs/2402.06184
    submitted by    /u/currentscurrents  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Beam: Data Processing, Data Pipelines, Dataflow and Flex Templates]]></title>
        <id>https://medium.com/p/2902224aabf3</id>
        <link href="https://towardsdatascience.com/apache-beam-data-processing-data-pipelines-dataflow-and-flex-templates-2902224aabf3?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T19:40:21.000Z</updated>
        <summary type="html"><![CDATA[In this first article, we’re exploring Apache Beam, from a simple pipeline to a more complicated one, using GCP Dataflow. Let’s learn what…
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Stefano Bosisio</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Build a Graph-based Neural Network for Anomaly Detection in 6 Steps]]></title>
        <id>https://medium.com/p/a7dc47723788</id>
        <link href="https://towardsdatascience.com/how-to-build-a-graph-based-neural-network-for-anomaly-detection-in-6-steps-a7dc47723788?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T19:22:38.000Z</updated>
        <summary type="html"><![CDATA[Learn to build a Graph Convolutional Network that can handle heterogeneous graph data for link prediction
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Claudia Ng</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powerful Collaboration of AI Agents with CrewAI]]></title>
        <id>https://medium.com/p/17f84378430b</id>
        <link href="https://towardsdatascience.com/powerful-collaboration-of-ai-agents-with-crewai-17f84378430b?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T19:12:44.000Z</updated>
        <summary type="html"><![CDATA[A hands-on marketing use case
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Toon Beerten</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding KL Divergence Intuitively]]></title>
        <id>https://medium.com/p/a4e876f9a532</id>
        <link href="https://towardsdatascience.com/understanding-kl-divergence-intuitively-a4e876f9a532?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T19:05:57.000Z</updated>
        <summary type="html"><![CDATA[A constructive approach to measuring distribution differences.]]></summary>
        <author>
            <name>Mohammed Mohammed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Data for Machine Learning]]></title>
        <id>https://www.kdnuggets.com/?p=163953</id>
        <link href="https://www.kdnuggets.com/synthetic-data-for-machine-learning?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=synthetic-data-for-machine-learning"/>
        <updated>2024-02-12T17:00:56.000Z</updated>
        <summary type="html"><![CDATA[You don't always have high-quality labeled datasets for supervised machine learning. Learn about why you should augment your real data with synthetic data as well as the ways to generate it.]]></summary>
        <author>
            <name>Michael Galarnyk</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Visualization of Geospatial Big Data by Lexcube! (Python)]]></title>
        <id>https://medium.com/p/a57512cabd69</id>
        <link href="https://towardsdatascience.com/3d-visualization-of-geospatial-big-data-by-lexcube-python-a57512cabd69?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T16:09:16.000Z</updated>
        <summary type="html"><![CDATA[Learn how to work with Lexcube, a Python package for data visualization in the space-time domain!
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Mahyar Aboutalebi, Ph.D.</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] KV Cache is huge and bottlenecks LLM inference. We quantize them to 2bit in a finetuning-free + plug-and-play fashion.]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/"/>
        <updated>2024-02-12T16:00:37.000Z</updated>
        <summary type="html"><![CDATA[It is well known that batch inference is a common practice for efficient LLM serving (which is one primary reason why services like ChatGPT have an initial delay). This batching practice is motivated by the fact that inference latency is mostly limited by the I/O cost of model loading but not the actual compute, where serving multiple requests in a batched manner adds tolerable latency increase while bringing in massive savings on cost per token. However, one issue of batched inference (or long context tasks, or both) is the massive KV cache required. As illustrated in this previous paper by Jeff Dean: a 500B+ model with bs=512 and seqlen=2048 has a total KV cache about 3TB — this is 3 times the model weight and brings another I/O challenge as the GPU will need to load the entire KV cache …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Biggest Weakness Of Boosting Trees]]></title>
        <id>https://medium.com/p/a5d7b15f3d1d</id>
        <link href="https://towardsdatascience.com/the-biggest-weakness-of-boosting-trees-a5d7b15f3d1d?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T15:24:32.000Z</updated>
        <summary type="html"><![CDATA[Why distribution drifts can really hurt your models
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Jacky Kaub</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Ad-Based Revenue Options for Gemini Advanced]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ap266r/exploring_adbased_revenue_options_for_gemini/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ap266r/exploring_adbased_revenue_options_for_gemini/"/>
        <updated>2024-02-12T15:10:52.000Z</updated>
        <summary type="html"><![CDATA[Dear Google AI Team,
 As a supporter of Gemini Advanced, I understand the need for sustainable revenue models. Could a careful implementation of advertisements be a feasible solution? Here are some thoughts, emphasizing responsible integration:
 Contextual Emphasis: Focus on text-based ads directly related to conversation topics, ensuring relevancy for users. Limited and Unobtrusive: A small number of clearly marked ads in designated areas would minimize disruption to the user experience. Prioritizing User Needs: Gemini Advanced's core task must remain providing helpful and unbiased information. Sponsored results, if utilized, should only appear when highly relevant and should be clearly distinguished from organic answers. I believe a mindful approach to ads could potentially create a viable revenue stream while safeguarding Gemini Advanced's reputation as a reliable and trustworthy AI tool.
 Thank you for your consideration!
 Sincerely, Mehdi
    submitted by    /u/General-Antelope-241  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Data Engineering Course for Beginners]]></title>
        <id>https://www.kdnuggets.com/?p=163946</id>
        <link href="https://www.kdnuggets.com/free-data-engineering-course-for-beginners?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=free-data-engineering-course-for-beginners"/>
        <updated>2024-02-12T15:00:58.000Z</updated>
        <summary type="html"><![CDATA[Interested in data engineering but don't know where to start? Get up to speed in data engineering fundamentals with this free course.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Torment executioners in Reno, Nevada, keep tormenting us with their publications.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50145</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/12/torment-executioners-in-reno-nevada/"/>
        <updated>2024-02-12T14:55:05.000Z</updated>
        <summary type="html"><![CDATA[The above figures come from this article which is listed on this Orcid page (with further background here): Horrifying as all this is, at least from the standpoint of students and faculty at the University of Nevada, not to mention … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help with Rasa Likert survey issue]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ap1pbl/help_with_rasa_likert_survey_issue/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ap1pbl/help_with_rasa_likert_survey_issue/"/>
        <updated>2024-02-12T14:49:49.000Z</updated>
        <summary type="html"><![CDATA[Hey folks, I am currently developing a Rasa Chatbot to conduct a survey and I am having some issues.
 The survey in question uses a Likert scale thought, meaning that the answers to the vast majority of questions are "strongly agree", "agree", "neutral", "disagree" and "strongly disagree". I am using the "forms" with each of my questions having its own "slot". The issue that I am having is that because all of the questions have the same responses the chatbot doesn't know from content which answer applies to each question, I am unsure if my understanding of Rasa is flawed or if I am missing something but I don't know how I should go about fixing this.
 I'm aware this is a long shot but if anyone has any ideas or suggestions they would be much appreciated, I have added some of the code that …]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[7 Lessons from an ML Internship at Intel]]></title>
        <id>https://medium.com/p/ccb3a116f6eb</id>
        <link href="https://towardsdatascience.com/7-lessons-from-an-ml-internship-at-intel-ccb3a116f6eb?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T14:37:22.000Z</updated>
        <summary type="html"><![CDATA[Automation, machine learning and LLMs in the chip industry
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Conor O'Sullivan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python Could Know Your Holidays No Matter Which Country You Live]]></title>
        <id>https://medium.com/p/ea0f9ce61719</id>
        <link href="https://towardsdatascience.com/python-could-know-your-holidays-no-matter-which-country-you-live-ea0f9ce61719?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T14:37:12.000Z</updated>
        <summary type="html"><![CDATA[Get Holidays from Any Country, Any Year, Any date
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Christopher Tao</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Processing with Gemini Pro]]></title>
        <id>https://pyimagesearch.com/?p=43125</id>
        <link href="https://pyimagesearch.com/2024/02/12/image-processing-with-gemini-pro/"/>
        <updated>2024-02-12T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents Image Processing with Gemini Pro Getting Started with Gemini Pro: An Overview Gemini Pro Setup Integrating Google AI Python SDK with Gemini Pro Image Processing with Gemini Pro: Python Code Generation Comprehensive List of GenAI Models Compatible…
The post Image Processing with Gemini Pro appeared first on PyImageSearch.]]></summary>
        <author>
            <name>Aditya Sharma</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursion in Python Demystified]]></title>
        <id>https://medium.com/p/d3b3b28ba121</id>
        <link href="https://towardsdatascience.com/recursion-in-python-demystified-d3b3b28ba121?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-12T13:06:35.000Z</updated>
        <summary type="html"><![CDATA[The article shows simple examples of flat and nested recursion patterns in Python.
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Marcin Kozak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Avoiding Multiprocessing Errors in Bash Shell]]></title>
        <id>https://www.johndcook.com/blog/?p=242801</id>
        <link href="https://www.johndcook.com/blog/2024/02/12/avoiding-multiprocessing-errors-in-bash-shell/"/>
        <updated>2024-02-12T13:03:53.000Z</updated>
        <summary type="html"><![CDATA[Suppose you have two Linux processes trying to modify a file at the same time and you don’t want them stepping on each other’s work and making a mess.  A common solution is to use a “lock” mechanism (a.k.a. “mutex”). One process “locks the lock” and by this action has sole ownership of a […]
Avoiding Multiprocessing Errors in Bash Shell first appeared on John D. Cook.]]></summary>
        <author>
            <name>Wayne Joubert</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn Data Science on a Budget]]></title>
        <id>https://www.kdnuggets.com/?p=163939</id>
        <link href="https://www.kdnuggets.com/learn-data-science-on-a-budget?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=learn-data-science-on-a-budget"/>
        <updated>2024-02-12T13:00:56.000Z</updated>
        <summary type="html"><![CDATA[This blog will go through platforms and courses you can take that will get you from 0-100 on your data science knowledge.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Hardware do you use for your private NLP projects?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aoxls0/what_hardware_do_you_use_for_your_private_nlp/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aoxls0/what_hardware_do_you_use_for_your_private_nlp/"/>
        <updated>2024-02-12T11:09:53.000Z</updated>
        <summary type="html"><![CDATA[Since my laptop broke down and I am kicked out of my unis server after graduation, I am lost on how to provide the GPU Power for my NLP Projects now and what a new laptop should provide in RAM etc. and not get too expensive. What du you look for in your private hardware decisions when money is crucial? And do you rent yourself to a server elsewhere somehow, or do you get a device with the right GPU/CPU power?
    submitted by    /u/nyliaw  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advanced RAG Techniques]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aovm9x/advanced_rag_techniques/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aovm9x/advanced_rag_techniques/"/>
        <updated>2024-02-12T08:49:32.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Here is an attempt to summarize different RAG Techniques for improved retrieval.
 The video goes through
  
Long Context re-ordering,
 Small-to-Big
  
And many more
 https://www.youtube.com/watch?v=YpcENPDn9u4&t=1s
    submitted by    /u/Mosh_98  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommended model for numerous features and a binary output?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aothfs/recommended_model_for_numerous_features_and_a/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aothfs/recommended_model_for_numerous_features_and_a/"/>
        <updated>2024-02-12T06:23:45.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a suitable algorithm to predict whether a customer will purchase an item or not based on a set of features. The output is binary - either 0 (no purchase) or 1 (purchase). The goal is to get the highest possible percentage of correct predictions
 Some additional info
  
There's an significant amount of available features, up to 50 different ones. 
 
There's an abundance of labeled data - close to a million entries that can be utilized
 
Only 1% of the entries have are purchased (minority class), the remaining 99% aren't (majority class), meaning there is an imbalance.
 
 I'm providing a sample below with only part of the features.
  
 item name customer GEO customer gender device type OS customer age Employed Annual Income Married Has Children Purchased Item? 
  
 Vacuum Cleaner XYZ USA Female PC Windows 45 Yes 200k Yes Yes No 
 
 Please advise as to what type of alghorithm to utilize for this scenario, that are likely to have the highest % of accurate predictions.
    submitted by    /u/miabananaz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Transfer with target images provided.]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aot79r/style_transfer_with_target_images_provided/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aot79r/style_transfer_with_target_images_provided/"/>
        <updated>2024-02-12T06:06:24.000Z</updated>
        <summary type="html"><![CDATA[In most style transfer models I have seen, there is no target image but rather an image classifier network that uses internal features learned which is used to identify the style of each image. The training error is calculated by checking if the new image learns the same features of the 2 input kmages. What if I was to create a synthetic data that contained the target image that both styles mixed would output to. What should I use as loss function here? MSE? I read somewhere that it doesn't work well. What other loss functions can be best used in the case where the target images are provided and output of the model should match the target image?
    submitted by    /u/silently--here  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will the people that develop and create AI be replaced by AI?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aosl1o/will_the_people_that_develop_and_create_ai_be/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aosl1o/will_the_people_that_develop_and_create_ai_be/"/>
        <updated>2024-02-12T05:30:47.000Z</updated>
        <summary type="html"><![CDATA[Will machine learning engineers and data scientists be replaced by AI?
    submitted by    /u/Daniu_13  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sklearn.quantile has quantilerandomforestregrssor, i want help to use same model to predict mean prediction too, as i am predicting for quantiles]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aoshzm/sklearnquantile_has_quantilerandomforestregrssor/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aoshzm/sklearnquantile_has_quantilerandomforestregrssor/"/>
        <updated>2024-02-12T05:25:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sea-Coconut-3833  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nudging ChatGPT to invent books you have no time to read]]></title>
        <id>https://yanirseroussi.com/2024/02/12/nudging-chatgpt-to-invent-books-you-have-no-time-to-read/</id>
        <link href="https://yanirseroussi.com/2024/02/12/nudging-chatgpt-to-invent-books-you-have-no-time-to-read/"/>
        <updated>2024-02-12T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[Getting ChatGPT Plus to elaborate on possible book content and produce a PDF cheatsheet, with the goal of learning about its capabilities.]]></summary>
        <author>
            <name>Yanir Seroussi | Data &amp; AI for Impact</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[are trees/random forests still used given the advances in neural networks?]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aok4n1/are_treesrandom_forests_still_used_given_the/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aok4n1/are_treesrandom_forests_still_used_given_the/"/>
        <updated>2024-02-11T22:14:49.000Z</updated>
        <summary type="html"><![CDATA[I found a thread from a few years back that was saying yes because they are easier/cheaper to implement and even may perform better when there are fewer features with little interaction between them. 
 I was wondering if that's still true in 2024.
    submitted by    /u/Apprehensive_Act4898  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text classification to 1-5]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1aoiiwm/text_classification_to_15/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1aoiiwm/text_classification_to_15/"/>
        <updated>2024-02-11T21:05:49.000Z</updated>
        <summary type="html"><![CDATA[Hi, I have a question.
 So I have a CSV file with transcripts which is text and the each transcript has a rating from 1-5 (this can be used as training data). I have another CSV with transcripts but no ratings. I want to be able to predict the ratings. What model do you suggest for something like this or how do you suggest I go about this? Thank you so much for you help!
    submitted by    /u/Eem2323  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it really worth a MA degree in Computational linguistics, especially in Gemrman ( Konstanz/ Tubingen) university? Are there really many job opportunities afterward ?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aogmth/is_it_really_worth_a_ma_degree_in_computational/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aogmth/is_it_really_worth_a_ma_degree_in_computational/"/>
        <updated>2024-02-11T19:47:09.000Z</updated>
        <summary type="html"><![CDATA[Can someone tell me their own experience, especially about finding a job after a degree in Language technology / computational linguistics ?
 I am about to graduate in a three-year course in foreign languages and literatures, I have taken various linguistics exams and am currently working on a thesis in historical linguistics. I have a great passion for linguistics and have become particularly interested in computational linguistics. At the moment, my professor of historical linguistics is very interested in me and has asked me to stay, not to leave, and always talks to me, even if not explicitly, about pursuing a Ph.D. in the future. Therefore, the choice was between staying at my university and continuing with the faculty of languages and cultures of Asia with a specialization in historical linguistics with the possibility of taking 3-4 courses in computational linguistics, or attending the university for NLP and computational linguistics in Germany at Konstanz or Tubingen, as they seem to be the only ones that accept students who do not have a background in computer science and do not require too much basic knowledge.
 I was wondering what the job prospects are after this degree, are there many job opportunities? Do these two programs (Konstanz / Tubingen) manage to easily place students in the workforce?
 Since I doubt that with just a few computational linguistics exams from my university I will really be able to find a job in these fields."
    submitted by    /u/aquilaa91  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://old.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</id>
        <link href="https://old.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/"/>
        <updated>2024-02-11T16:00:18.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Signal Separation]]></title>
        <id>https://medium.com/p/769f43b46779</id>
        <link href="https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-11T15:55:45.000Z</updated>
        <summary type="html"><![CDATA[Understand Semantic Structures with Transformers and Topic Modeling]]></summary>
        <author>
            <name>Márton Kardos</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech to Text to Speech with AI Using Python — a How-To Guide]]></title>
        <id>https://medium.com/p/ee9b0b0ef082</id>
        <link href="https://towardsdatascience.com/speech-to-text-to-speech-with-ai-using-python-a-how-to-guide-ee9b0b0ef082?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-11T15:47:58.000Z</updated>
        <summary type="html"><![CDATA[How to Create a Speech-to-Text-to-Speech Program]]></summary>
        <author>
            <name>Naomi Kriger</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LingoNaut Language Assistant]]></title>
        <id>https://medium.com/p/6abe3e8b045c</id>
        <link href="https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-11T15:17:09.000Z</updated>
        <summary type="html"><![CDATA[Multilingual Learning with an Ollama-Python Walkie-Talkie]]></summary>
        <author>
            <name>Nate Cibik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Structure and Organise a Streamlit App]]></title>
        <id>https://medium.com/p/e66b65ece369</id>
        <link href="https://towardsdatascience.com/how-to-structure-and-organise-a-streamlit-app-e66b65ece369?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-11T15:04:44.000Z</updated>
        <summary type="html"><![CDATA[Bringing Order to a Python Streamlit App Through an Organised Project Folder Structure
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Andy McDonald</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Machine Learning Engineer 🤖 #269 -     Writing a Search Engine in 80 Lines of Python,
    Trillion Row Challenge,
    Comparing LLMs to Lawyers,
    AI Calls Now Illegal in US
    + more 🚀]]></title>
        <id>https://kill-the-newsletter.com/alternates/e5alayne5z5znfhu.html</id>
        <link href="https://kill-the-newsletter.com/alternates/e5alayne5z5znfhu.html"/>
        <updated>2024-02-11T14:48:40.000Z</updated>
        <summary type="html"><![CDATA[The Machine Learning Engineer 🤖 #269#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…]]></summary>
        <author>
            <name>the ml engineer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clinical trials that are designed to fail]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48813</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/11/clinical-trials-that-are-designed-to-fail/"/>
        <updated>2024-02-11T14:38:34.000Z</updated>
        <summary type="html"><![CDATA[Mark Palko points us to a recent update by Robert Yeh et al. of the famous randomized parachute-jumping trial: Palko writes: I also love the way they dot all the i’s and cross all the t’s. The whole thing is … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Chrome Extension to Filter Hateful Speech and Negative Comments - Advice Needed for College Project.]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ao5r6s/developing_a_chrome_extension_to_filter_hateful/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ao5r6s/developing_a_chrome_extension_to_filter_hateful/"/>
        <updated>2024-02-11T11:17:22.000Z</updated>
        <summary type="html"><![CDATA[Hey Reddit community,
 ​
 My college project team consisting of two members is working on developing a Chrome extension aimed at filtering out hateful speech and negative comments encountered by users while surfing the web. Our goal is to create a tool that promotes a more positive online experience for users.
 Details:
 Functionality: Our extension will monitor the websites users visit in real-time. When it detects any content containing hateful speech or negative comments, it will automatically hide or filter out such content from the user's view.
 Feasibility: While we're excited about this project, we're also mindful of its feasibility. As two members, we want to ensure that the scope of the project is manageable within the given timeframe and resources. We're open to any suggestions or advice on how to approach the development process efficiently.
 Technical Considerations: We're particularly interested in insights on the technical aspects of building such an extension. Should we focus on specific programming languages or frameworks? Are there any existing libraries or APIs that could assist us in detecting and filtering out negative content effectively?
 Ethical Concerns: Additionally, we're aware of the ethical implications of content moderation. How can we strike a balance between filtering harmful content and respecting freedom of expression? Are there any best practices or guidelines we should adhere to in this regard?
 Any advice, tips, or resources you can provide would be greatly appreciated. We're eager to learn and make meaningful contributions to this project. Thanks in advance for your help!
    submitted by    /u/jerry_10_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Chrome Extension to Filter Hateful Speech and Negative Comments - Advice Needed for College Project Team]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ao5h4a/developing_a_chrome_extension_to_filter_hateful/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ao5h4a/developing_a_chrome_extension_to_filter_hateful/"/>
        <updated>2024-02-11T10:58:59.000Z</updated>
        <summary type="html"><![CDATA[Hey Reddit community,
 ​
 My college project team consisting of two members is working on developing a Chrome extension aimed at filtering out hateful speech and negative comments encountered by users while surfing the web. Our goal is to create a tool that promotes a more positive online experience for users.
 Details:
 Functionality: Our extension will monitor the websites users visit in real-time. When it detects any content containing hateful speech or negative comments, it will automatically hide or filter out such content from the user's view.
 Feasibility: While we're excited about this project, we're also mindful of its feasibility. As two members, we want to ensure that the scope of the project is manageable within the given timeframe and resources. We're open to any suggestions or advice on how to approach the development process efficiently.
 Technical Considerations: We're particularly interested in insights on the technical aspects of building such an extension. Should we focus on specific programming languages or frameworks? Are there any existing libraries or APIs that could assist us in detecting and filtering out negative content effectively?
 Ethical Concerns: Additionally, we're aware of the ethical implications of content moderation. How can we strike a balance between filtering harmful content and respecting freedom of expression? Are there any best practices or guidelines we should adhere to in this regard?
 Any advice, tips, or resources you can provide would be greatly appreciated. We're eager to learn and make meaningful contributions with this project. Thanks in advance for your help!
    submitted by    /u/jerry_10_  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where have you seen rule-based text classification being applied?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ao52n8/where_have_you_seen_rulebased_text_classification/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ao52n8/where_have_you_seen_rulebased_text_classification/"/>
        <updated>2024-02-11T10:31:12.000Z</updated>
        <summary type="html"><![CDATA[I wrote a rule-based text classification app specialized in my language (Vietnamese) and would like to know where else this app is useful. For example if you input the prompt (which is use a bunch of keywords), e.g. fish 50k, then it will automatically label/classify the prompt like this: - Object: fish - Type of Object: food - Place of transaction: market - Type of place of transaction: offline - Consumer: myself - Type of consumer: myself - Price: 50000 VND
 The program can make this classification based on a config you declare, e.g.: yaml - Dimension name: Object Classification: - Food: fish, meat - Appliance: computer, speaker Default value: meat ... 
 Which problems do you see this app will be useful? In general, where have you seen rule-based text classification being applied? Especially in the context of ChatGPT and its GPT store? I think rule-based classification is much cheaper and more accurate than statistical-based classification. Is that correct?
    submitted by    /u/Ooker777  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Get alerted when any new code is released for a given paper or topic! Would love your feedback :)]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ao467y/get_alerted_when_any_new_code_is_released_for_a/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ao467y/get_alerted_when_any_new_code_is_released_for_a/"/>
        <updated>2024-02-11T09:27:47.000Z</updated>
        <summary type="html"><![CDATA[Just built out something for this community I thought i'd personally share 🙂 Would love your feedback :)
 You can now get alerted when any new code is released for a given paper or topic! You can select any paper or topic as you're browsing the internet (Google, Scholar, Arxiv, IEEE, etc.)
 Just install the code finder extension (Chrome: https://chromewebstore.google.com/detail/ai-code-finder-for-papers/aikkeehnlfpamidigaffhfmgbkdeheil | Firefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/ | Edge: https://microsoftedge.microsoft.com/addons/detail/get-papers-with-code-ever/mflbgfojghoglejmalekheopgadjmlkm), click on any bell/alert icon you come across while browsing and follow the next steps on the screen 🙂
 Also, with alerts 
  
get the latest developments in your area of interest delivered straight to your inbox.
 Author's newest work: be the first to know when an author releases new papers.
  
​
 https://preview.redd.it/wsssu3pcdxhc1.png?width=3074&format=png&auto=webp&s=c695d93161e47f4233850c8781cda99f76d8c97e
 https://preview.redd.it/jshzj3pcdxhc1.png?width=1848&format=png&auto=webp&s=cd64d93e9535dc920a5ec633e8781db42326f902
 https://preview.redd.it/t0s7g3pcdxhc1.png?width=1890&format=png&auto=webp&s=e077f6ed446d4211cb89d4aee4674eb8f8ad0293
    submitted by    /u/MLtinkerer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Gemini Ultra Outshines ChatGPT 4: Performance Comparison]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ao3vjs/why_gemini_ultra_outshines_chatgpt_4_performance/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ao3vjs/why_gemini_ultra_outshines_chatgpt_4_performance/"/>
        <updated>2024-02-11T09:07:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/UseCreative4765  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Appraisel Feedback for Real Estate]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ao3sqg/analyzing_appraisel_feedback_for_real_estate/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ao3sqg/analyzing_appraisel_feedback_for_real_estate/"/>
        <updated>2024-02-11T09:01:49.000Z</updated>
        <summary type="html"><![CDATA[I'm excited to share that I'm diving into my first big natural language processing project focused on analyzing feedback related to the appraisal of real estate properties. I'll be working with Dutch text and aiming to classify various aspects of feedback to improve property evaluation processes.
 Here's a (translated) sample text I'll be working with:
 "I believe the appraisal of my appartment is excessively high, particularly when compared to recent sales in our neighborhood. Additionally, the apartments you've selected as comparables for my unit are not suitable matches. I kindly request that you reassess the valuation."
 For this project, I've identified several key classifications (around 20) that the model needs to detect, in the above text it should for example find the following classifications.
  
Appraisal too high
 Comparison with neighbourhood
 Properties used for appraisal not similar
  
To tackle this task, I'm considering starting with a pre-trained multilingual model like BERT-multilingual and fine tuning it with a dataset I've classified by hand. However, I'm open to suggestions and would love to hear your thoughts if you've worked on similar projects or have insights into the best approaches for classifying (Dutch) text in NLP. If you know of any similar projects online that would be very helpful as well! I'll be using Python for the implementation since I've been doing a lot of image classification tasks using Python and Tensorflow before.
 Any tips about wether to classify it sentence by sentence or the full text at once, how I could best go about this or any other helpful advice is very welcome!
    submitted by    /u/AnterosNL  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Industrial engineer thinking to get into machine learning]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ao1u2l/industrial_engineer_thinking_to_get_into_machine/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ao1u2l/industrial_engineer_thinking_to_get_into_machine/"/>
        <updated>2024-02-11T06:50:40.000Z</updated>
        <summary type="html"><![CDATA[I'm industrial engineer and planning to work in machine learning do you advice me to do that ? and if yes how should i start? 
    submitted by    /u/Lopsided-Adeptness42  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis on Arabic reviews]]></title>
        <id>https://www.reddit.com/r/MLQuestions/comments/1ao0fxs/sentiment_analysis_on_arabic_reviews/</id>
        <link href="https://www.reddit.com/r/MLQuestions/comments/1ao0fxs/sentiment_analysis_on_arabic_reviews/"/>
        <updated>2024-02-11T05:25:40.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset with review text in arabic and also a rating column(out of 100) It is a part of a data science assessment for a jon opportunity. The objective is to perform sentiment analysis and produce insights on positive,neutral,negative reviews. I am unable to decide whether I should: 1. Simply use the rating column to get sentiment(pos/neu/neg) and then start getting insights? May or may not train a supervised training model. 2. Go via the route of an unsupervised setting using these reviews and then analyse which were identified as pos/neu/neg
    submitted by    /u/Vishesh1597  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning Questions</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This-way-up and Knuth arrows]]></title>
        <id>https://www.johndcook.com/blog/?p=242847</id>
        <link href="https://www.johndcook.com/blog/2024/02/10/this-way-up-and-knuth-arrows/"/>
        <updated>2024-02-10T22:32:03.000Z</updated>
        <summary type="html"><![CDATA[I was looking today at a cardboard box that had the “this way up” symbol on it and wondered whether there is a Unicode value for it. Apparently not. But there is an ISO code for it: ISO 7000 symbol 0623. It’s an international standard symbol for indicating how to orient a package. The name […]
This-way-up and Knuth arrows first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pandas for Data Engineers]]></title>
        <id>https://medium.com/p/a191965ac538</id>
        <link href="https://towardsdatascience.com/pandas-for-data-engineers-a191965ac538?source=rss----7f60cf5620c9---4"/>
        <updated>2024-02-10T16:50:07.000Z</updated>
        <summary type="html"><![CDATA[Advanced techniques to process and load data efficiently
Continue reading on Towards Data Science »]]></summary>
        <author>
            <name>Mike Shakhomirov</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factoring pseudoprimes]]></title>
        <id>https://www.johndcook.com/blog/?p=242839</id>
        <link href="https://www.johndcook.com/blog/2024/02/10/factoring-pseudoprimes/"/>
        <updated>2024-02-10T15:45:25.000Z</updated>
        <summary type="html"><![CDATA[Fermat’s little theorem says that if p is a prime number, then for any positive integer b < p we hve bp-1 = 1 (mod p). This theorem gives a necessary but not sufficient condition for a number to be prime. Fermat’s primality test The converse of Fermat’s little theorem is not always true, but […]
Factoring pseudoprimes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If school funding doesn’t really matter, why do people want their kid’s school to be well funded?]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48826</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/10/if-school-funding-doesnt-really-matter-why-does-everyone-want-their-kid-to-go-to-a-well-funded-school/"/>
        <updated>2024-02-10T14:59:08.000Z</updated>
        <summary type="html"><![CDATA[A question came up about the effects of school funding and student performance, and we were referred to this review article from a few years ago by Larry Hedges, Terri Pigott, Joshua Polanin, Ann Marie Ryan, Charles Tocci, and Ryan … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Different Databases | Choosing The Right Database For your...]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1anee81/understanding_different_databases_choosing_the/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1anee81/understanding_different_databases_choosing_the/"/>
        <updated>2024-02-10T11:35:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Party330  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is this NLP problem called?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ancuj4/what_is_this_nlp_problem_called/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ancuj4/what_is_this_nlp_problem_called/"/>
        <updated>2024-02-10T09:47:59.000Z</updated>
        <summary type="html"><![CDATA[Been searching the web for a few days now and have been unable to find the exact problem name and definition, so I’m turning to the experts. 
 I have some documents of text and want to find the relevant sections of the text related to some key word or words. For example, say the document is about practical steps for how to take care of the pooping habits of common pets, and I search “dog” or “canine”. I’d like to get all sections of the text (there could be multiple) related to this discussion for dogs. Note that the text is fundamentally about taking care of animals pooping, not about animals themselves, so there could be parts of text relevant for dogs without the direct mention of them (likely around the sections where there is direct mention of them, but not always). Also note that by searching canine, I’d like to more or less get the same return as searching dog, even though the term is less commonly used. This second problem seems not nearly as hard, given both terms have very similar semantic meaning. The first is trickier given the model must understand relations and properties of the searched entity. 
 Another example could be searching for “Donald Trump” in a news article about recent developments in republican candidates’ campaigns and how the party currently fairs against the democrats. It would be nice to get all discussions of Trump but also segments about how republicans are doing versus democrats since this is relevant for Trump. 
 What is this problem called exactly? I’ve toyed around using a GPT and it yields reasonable results with the right prompts but perhaps there are more direct approaches.
    submitted by    /u/AnonQuantGuy  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[University of Cincinnati MS Business Analytics Summer 2024 Information Session]]></title>
        <id>https://www.kdnuggets.com/?p=163920</id>
        <link href="https://www.kdnuggets.com/2024/02/uc-business-analytics-summer-2024-information-session?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=university-of-cincinnati-ms-business-analytics-summer-2024-information-session"/>
        <updated>2024-02-09T18:10:21.000Z</updated>
        <summary type="html"><![CDATA[Don't miss this chance to chart your course toward a successful career in business analytics. Reserve your spot now and embark on a journey of knowledge and growth!]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sprachumfrage]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amtdf0/sprachumfrage/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amtdf0/sprachumfrage/"/>
        <updated>2024-02-09T17:21:14.000Z</updated>
        <summary type="html"><![CDATA[Hallo,
 ich bin auf der Suche nach einem muttersprachlichen Urteil für französische Sprache. Für mein Examen im Fach Französisch führe ich eine Sprachuntersuchung durch zu prädikativen bzw. nicht prädikativen Adjektiven. Ich muss nun herausfinden, ob sich folgende Adjektive (in den Beispielen), von denen ich vermute, es seien ausschließlich nicht prädikative Adjektive, sich möglicherweise doch prädikativ verwenden lassen... Könnte ich hierzu ein muttersprachliches Urteil zur Korrektheit dieser Sätze bekommen? Die Sätze habe ich so geformt, dass eine prädikative Verwendung der Adjektive vorliegt.
 - l'autorité et la confiance sont possibles
 - l'effort et la lassitude sont physiques .
 - l'épouvante et la stupeur sont générales
 -l'équité et l'égalité sont absolues
 - le paca et le porc-épic sont alongées
 - leur coeur et leur cerveau sont pareils - le coeur est pareil?
 -la rancune et la haine sont éternelles
 -une puissance et une étendue sont égales - la puissance est égal?
 -la mortification et la fuite sont continuelles
 -l'oppression et la tristesse sont extrêmes
 -la religion et l' humanité sont inférieures (minderwertig)
 Funktionieren diese Sätze in dieser Schreibart wirklich?
 Über eine Rückmeldung wäre ich allen französischen Muttersprachlern sehr dankbar!!!
    submitted by    /u/shiningstar12xh  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should Duolingo create an 18+ language course?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amt2d9/should_duolingo_create_an_18_language_course/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amt2d9/should_duolingo_create_an_18_language_course/"/>
        <updated>2024-02-09T17:08:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Summer_19_  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Only Free Course You Need To Become a MLOps Engineer]]></title>
        <id>https://www.kdnuggets.com/?p=163889</id>
        <link href="https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-mlops-engineer?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-only-free-course-you-need-to-become-a-mlops-engineer"/>
        <updated>2024-02-09T15:00:58.000Z</updated>
        <summary type="html"><![CDATA[Unlock the secrets to building, deploying, and monitoring models like a pro.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hey, here’s some free money for you!  Just lend your name to this university and they’ll pay you $1000 for every article you publish!]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50122</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/09/hey-heres-some-free-money-for-you-just-lend-your-name-to-these-universities-and-theyll-pay-you-1000-for-every-article-you-publish/"/>
        <updated>2024-02-09T14:37:05.000Z</updated>
        <summary type="html"><![CDATA[Remember that absolutely ridiculous claim that scientific citations are worth $100,000 each? It appears that someone is taking this literally. Or, nearly so. Nick Wise has the story: A couple of months ago a professor received the following email, which … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSCN #287: AI Act vs US Fair Use Trial - Which matters more now?]]></title>
        <id>https://kill-the-newsletter.com/alternates/f0uuxsjowsfqzzwx.html</id>
        <link href="https://kill-the-newsletter.com/alternates/f0uuxsjowsfqzzwx.html"/>
        <updated>2024-02-09T14:21:07.000Z</updated>
        <summary type="html"><![CDATA[96
            
        
        
        
        
        
        DSCN #287: AI Act vs US Fair Use Trial - Which matters more now?

    
		p{
			margin:10px 0;
			padding:0;
		}
		table{
			border-collapse:collapse;
		}
		h1,h2,h3,h4,h5,h6{
			display:block;
			margin:0;
			padding:0;
		}
		img,a img{
			border:0;
			height:auto;
			outline:none;
			text-decoration:none;
		}
		body,#bodyTable,#bodyCell{
			height:100%;
			margin:0;
			padding:0;
			width:100%;
		}
		.mcnPreviewText{
			display:none !important;
		}
		#outlook a{
			padding:0;
		}
		img{
			-ms-interpolation-mode:bicubic;
		}
		table{
			mso-table-lspace:0pt;
			mso-table-rspace:0pt;
		}
		.ReadMsgBody{
			width:100%;
		}
		.ExternalClass{
			width:100%;
		}
		p,a,li,td,blockquote{
			mso-line-height-rule:exactly;
		}
		a…]]></summary>
        <author>
            <name>Data Science Community newsletter sign-up</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Cheap Books to Master Machine Learning]]></title>
        <id>https://www.kdnuggets.com/?p=163851</id>
        <link href="https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-cheap-books-to-master-machine-learning"/>
        <updated>2024-02-09T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Machine Learning is a skill that everyone should have, and these cheap books would facilitate that learning process.]]></summary>
        <author>
            <name>Cornellius Yudha Wijaya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Unsupervised Log Anomaly Detection]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amlugx/d_unsupervised_log_anomaly_detection/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amlugx/d_unsupervised_log_anomaly_detection/"/>
        <updated>2024-02-09T11:14:42.000Z</updated>
        <summary type="html"><![CDATA[Help!
 This is my first post on Reddit
 I am thinking about using the variational autoencoder model for anomaly detection . I have an Android Logs dataset. As the logs generated are a representative of time series type of data I thought about using the VAEs with an architecture involving LSTMs in the encoder and decoders. As far as what I know about the basics of the working of the autoencoder models , given a sequence size say n , during learning/training phase n-1 entries are used to generate a representation/prediction of the nth entry (log in this case) and then reconstruction error is minimized by the model.
 How could I approach ? Please suggest if there are mistakes in my approach. I was also thinking about converting the logs into word embeddings using Word2Vec and then using them as input for the VAE model.
 Please suggest latest techniques and tools that I must consider exploring and a sample approach to get started with for high accuracy in this situation.
 I'm seeking feedback on this approach. Do you spot any potential flaws or have suggestions for improvements? I'm open to any insights or experiences you can share!
 Looking forward to your input. Thanks!
    submitted by    /u/CheesecakeNatural393  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing sales transcripts]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amlb4r/analyzing_sales_transcripts/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amlb4r/analyzing_sales_transcripts/"/>
        <updated>2024-02-09T10:37:54.000Z</updated>
        <summary type="html"><![CDATA[I'm a digital marketer and our company uses Chorus.ai to record/transcribe sales calls. What I'd like to do is essentially export as many sales calls as possible and identify pain points, problems, objections, etc. to better inform our marketing material. The ultimate goal is to quantify some of the qualitative data we have. 
 I'm thinking about doing this with a combination of keywords (that we've identified and that possibly an AI tool could identify for us) + other AI tools out there to scrape/analyze 100+ calls. As I’m looking into this more I think ‘coding’ for themes is what I likely am trying to do.
 Any resources/tools for how to start doing this? The call transcripts are private company data and so using something like Chat GPT won't cut it due to privacy concerns.
    submitted by    /u/driedupkelp  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Named Entity Recognition] When adjusting labels after tokenization, do we include all subwords for BIO tags or just the ones that are originally labeled?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amjpqg/named_entity_recognition_when_adjusting_labels/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amjpqg/named_entity_recognition_when_adjusting_labels/"/>
        <updated>2024-02-09T08:40:43.000Z</updated>
        <summary type="html"><![CDATA[For example, I have a word in a text sequence that is "patterns" as in "The patterns were very pretty." In the original character-wise label sequence, only "pattern" is labeled and the "s" isn't. When we tokenize "patterns" using a pre-trained BERT tokenizer, the result is pat, ##tern, ##s.
 I'm trying to adjust the labels using the offsets_mapping that is returned by the tokenizer, but that leads to the problem that only pat and ##tern are labeled and ##s isn't.
 My question is, if we have labels that don't span the entire word and stop mid-subword, does that lead to performance problems?
    submitted by    /u/Seankala  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLP whitepaper comparison modelling ideas]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1amag5r/nlp_whitepaper_comparison_modelling_ideas/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1amag5r/nlp_whitepaper_comparison_modelling_ideas/"/>
        <updated>2024-02-09T00:08:18.000Z</updated>
        <summary type="html"><![CDATA[I'm looking to compare how government strategy and attitudes towards ageing poulations has evolved over time, and how they differ across a handful of European countries. 
 The idea would be to take a handful of government policy whitepapers that address the ageing population challenge; and then extract the most common 'words' or 'themes' for each document, and then cluster together policies and strategy based on similarity scores. 
 First thoughts around this would be to use TFIDF to identify important keywords for each specific whitepaper. I'd then use a word embeddings model like BERT to calculate measures of pairwise cosine similarity between documents, enabling me to cluster them in to groups. This would help to generate insight into which governments have similar strategies. 
 To look at how this attitudes and strategy have evolved over time, i'd then probably look to build separate models each year. 
 I wondered if anybody had any other ideas around this? Would be interesting to see if its possible to use a topic modelling approach like LDA or BERTopic to extract latent themes in each whitepaper instead. I wasnt sure if its possible to do this using only a few documents. In this particular case i'd only have a few whitepapers to work with (one per country per year) 
 Open to any creative ideas!
    submitted by    /u/LDM-88  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do comments in a LaTeX file change the output?]]></title>
        <id>https://www.johndcook.com/blog/?p=242837</id>
        <link href="https://www.johndcook.com/blog/2024/02/08/pdflatex-comment/"/>
        <updated>2024-02-08T18:31:24.000Z</updated>
        <summary type="html"><![CDATA[When you add a comment to a LaTeX file, it makes no visible change to the output. The comment is ignored as far as the appearance of the file. But is that comment somehow included in the file anyway? If you compile a LaTeX file to PDF, then edit it by throwing in a comment, […]
Do comments in a LaTeX file change the output? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating Today’s Data and AI Market Uncertainty]]></title>
        <id>https://www.kdnuggets.com/?p=163907</id>
        <link href="https://www.kdnuggets.com/2024/02/altair-navigating-todays-data-ai-market-uncertainty?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=navigating-todays-data-and-ai-market-uncertainty"/>
        <updated>2024-02-08T18:00:47.000Z</updated>
        <summary type="html"><![CDATA[It’s more important than ever to think long-term about the analytics partnerships you forge. Are you choosing technologies that will stand the test of time? Are you choosing companies with proven track records?]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen to those residuals]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50141</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/08/listen-to-those-residuals/"/>
        <updated>2024-02-08T17:55:20.000Z</updated>
        <summary type="html"><![CDATA[This is Jessica. Speaking of data sonification (or sensification), Hyeok, Yea Seul Kim, and I write:  Data sonification-mapping data variables to auditory variables, such as pitch or volume-is used for data accessibility, scientific exploration, and data-driven art (e.g., museum exhibitions) … Continue reading →]]></summary>
        <author>
            <name>Jessica Hullman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your PDF may reveal more than you intend]]></title>
        <id>https://www.johndcook.com/blog/?p=242803</id>
        <link href="https://www.johndcook.com/blog/2024/02/08/pdf-forensics/"/>
        <updated>2024-02-08T16:53:32.000Z</updated>
        <summary type="html"><![CDATA[When you create a PDF file, what you see is not all you get. There is metadata embedded in the file that might be useful. It also might reveal information you’d rather not reveal. The previous post looked at just the time stamp on a file. This post will look at more metadata, focusing on […]
Your PDF may reveal more than you intend first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 5 AI Coding Assistants You Must Try]]></title>
        <id>https://www.kdnuggets.com/?p=163869</id>
        <link href="https://www.kdnuggets.com/top-5-ai-coding-assistants-you-must-try?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=top-5-ai-coding-assistants-you-must-try"/>
        <updated>2024-02-08T15:00:31.000Z</updated>
        <summary type="html"><![CDATA[Discover the top AI coding assistants that can 10X your productivity overnight - #5 has the best autocomplete feature, and #1 is the most advanced code assistant tool ever seen!]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social penumbras predict political attitudes (my talk at Harvard on Monday Feb 12 at noon)]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50112</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/08/social-penumbras-predict-political-attitudes-my-talk-at-harvard-on-monday-feb-12-at-noon/"/>
        <updated>2024-02-08T14:56:13.000Z</updated>
        <summary type="html"><![CDATA[Monday, February 12, 2024, 12:00pm to 1:15pm Social penumbras predict political attitudes The political influence of a group is typically explained in terms of its size, geographic concentration, or the wealth and power of the group’s members. This article introduces … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If you save a file as PDF twice, you get two different files]]></title>
        <id>https://www.johndcook.com/blog/?p=242777</id>
        <link href="https://www.johndcook.com/blog/2024/02/08/pdf-metadata/"/>
        <updated>2024-02-08T13:27:28.000Z</updated>
        <summary type="html"><![CDATA[If you save a file as a PDF twice, you won’t get exactly the same file both times. To illustrate this, I created an LibreOffice document containing “Hello world.” and saved it twice, first as humpty.pdf then as dumpty.pdf. Then I compared the two files. % diff humpty.pdf dumpty.pdf Binary files humpty.pdf and dumpty.pdf differ […]
If you save a file as PDF twice, you get two different files first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Data Science Interview Book to Land Your Dream Job]]></title>
        <id>https://www.kdnuggets.com/?p=163845</id>
        <link href="https://www.kdnuggets.com/free-data-science-interview-book-to-land-your-dream-job?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=free-data-science-interview-book-to-land-your-dream-job"/>
        <updated>2024-02-08T13:00:45.000Z</updated>
        <summary type="html"><![CDATA[Are you preparing for your dream data science job but feeling overwhelmed by the vast amount of online resources? Look no further than this free and easily accessible web-based book to help you brush up on your skills and feel confident for your upcoming interview.]]></summary>
        <author>
            <name>Kanwal Mehreen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Analysis with Python]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50137</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/08/bayesian-analysis-with-python/"/>
        <updated>2024-02-08T12:46:55.000Z</updated>
        <summary type="html"><![CDATA[Osvaldo Martin writes: The third edition of Bayesian Analysis with Python serves as an introduction to the basic concepts of applied Bayesian modeling. It adopts a hands-on approach, guiding you through the process of building, exploring and expanding models using … Continue reading →]]></summary>
        <author>
            <name>Aki Vehtari</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opinions On NanoDL, A New Library For Developing Transformers From Scratch.]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1also9r/opinions_on_nanodl_a_new_library_for_developing/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1also9r/opinions_on_nanodl_a_new_library_for_developing/"/>
        <updated>2024-02-08T10:25:02.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I just published the developer version of NanoDL, a library for developing transformer models within the Jax/Flax ecosystem and would love your feedback!
 Key Features of NanoDL include:
 A wide array of blocks and layers, facilitating the creation of customised transformer models from scratch.
 An extensive selection of models like LlaMa2, Mistral, Mixtral, GPT3, GPT4 (inferred), T5, Whisper, ViT, Mixers, GAT, CLIP, and more, catering to a variety of tasks and applications.
 Data-parallel distributed trainers so developers can efficiently train large-scale models on multiple GPUs or TPUs, without the need for manual training loops.
 Dataloaders, making the process of data handling for Jax/Flax more straightforward and effective.
 Custom layers not found in Flax/Jax, such as RoPE, GQA, MQA, and SWin attention, allowing for more flexible model development.
 GPU/TPU-accelerated classical ML models like PCA, KMeans, Regression, Gaussian Processes etc., akin to SciKit Learn on GPU.
 Modular design so users can blend elements from various models, such as GPT, Mixtral, and LlaMa2, to craft unique hybrid transformer models.
 A range of advanced algorithms for NLP and computer vision tasks, such as Gaussian Blur, BLEU etc.
 Each model is contained in a single file with no external dependencies, so the source code can also be easily used.
 Checkout the repository for sample usage and more details: https://github.com/HMUNACHI/nanodl
 Ultimately, I want as many opinions as possible, next steps to consider, issues, even contributions.
 Note: I am working on the readme docs. For now, in the source codes, I include a comprehensive example on top of each model file in comments.
    submitted by    /u/Henrie_the_dreamer  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What specific problems in what domains and fields have the need to use rule-based text classification?]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1alqwpb/what_specific_problems_in_what_domains_and_fields/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1alqwpb/what_specific_problems_in_what_domains_and_fields/"/>
        <updated>2024-02-08T08:17:19.000Z</updated>
        <summary type="html"><![CDATA[In my understanding, there are two types of approaches in NLP: rule-based and statistic-based. Rule-based approach is simple, understandable and need not training, while statistic-based is better if the rules are complex and you have good training data.
 What domains, fields or industries have the need to use rule-based approach classification problems? I think there should be a review on how this technique is applied in various field, but I can't find one.
 My goal: I develop a rule-based classification tool for my language and now I'm looking for potential users. I want to know their needs and the insights of their fields.
    submitted by    /u/Ooker777  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mastering Pair Plots for Visualization and Hypothesis Creation in the Ames Housing Market]]></title>
        <id>https://machinelearningmastery.com/?p=15854</id>
        <link href="https://machinelearningmastery.com/pair-plots/"/>
        <updated>2024-02-08T02:11:59.000Z</updated>
        <summary type="html"><![CDATA[Navigating the complex landscape of real estate analytics involves unraveling distinct narratives shaped by various property features within the housing market data. Our exploration today takes us into the realm of a potent yet frequently overlooked data visualization tool: the pair plot. This versatile graphic not only sheds light on the robustness and orientation of […]
The post Mastering Pair Plots for Visualization and Hypothesis Creation in the Ames Housing Market appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When all else fails, add a code comment]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50131</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/07/when-all-else-fails-add-a-code-comment/"/>
        <updated>2024-02-07T20:00:32.000Z</updated>
        <summary type="html"><![CDATA[Another way of saying this is that you should treat inline code comments as a last resort when there is no other way to make your intentions clear. I used to teach a session of Andrew’s statistical communication class once … Continue reading →]]></summary>
        <author>
            <name>Bob Carpenter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on the Latest Intel® GPU]]></title>
        <id>https://www.kdnuggets.com/?p=163897</id>
        <link href="https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=generative-ai-playground-llms-with-camel-5b-and-open-llama-3b-on-the-latest-intel-gpu"/>
        <updated>2024-02-07T18:00:37.000Z</updated>
        <summary type="html"><![CDATA[Intel offers a thrilling glimpse into the next generation of AI, showcasing the power of Camel-5b and Open LLaMA 3B LLMs.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to detect bad data in your instruction tuning dataset (for better LLM fine-tuning)]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1al8elm/how_to_detect_bad_data_in_your_instruction_tuning/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1al8elm/how_to_detect_bad_data_in_your_instruction_tuning/"/>
        <updated>2024-02-07T17:29:05.000Z</updated>
        <summary type="html"><![CDATA[Hello Redditors!
 I've spent some time looking at instruction-tuning (aka LLM Alignment / Fine-Tuning) datasets and I've found that they inevitably have bad data lurking within them. This is often what’s preventing LLMs to go from demo to production, not more parameters/GPUs… However, bad instruction-response data is hard to detect manually.
 Applying our techniques below to the famous dolly-15k dataset immediately reveals all sorts of issues in this dataset (even though it was carefully curated by over 5000 employees): responses that are inaccurate, unhelpful, or poorly written, incomplete/vague instructions, and other sorts of bad language (toxic, PII, …)
 Data auto-detected to be bad can be filtered from the dataset or manually corrected. This is the fastest way to improve the quality of your existing instruction tuning data and your LLMs!
 Feel free to check out the code on Github to reproduce these findings or read more details here in our article which demonstrates automated techniques to catch low-quality data in any instruction tuning dataset.
    submitted by    /u/cmauck10  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis in Python: Going Beyond Bag of Words]]></title>
        <id>https://www.kdnuggets.com/?p=163834</id>
        <link href="https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=sentiment-analysis-in-python-going-beyond-bag-of-words"/>
        <updated>2024-02-07T15:00:57.000Z</updated>
        <summary type="html"><![CDATA[This code based tutorial provides a brief introduction to Sentiment Analysis, a method used to predict emotions, similar to a digital psychologist.]]></summary>
        <author>
            <name>Nate Rosidi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Replicability & Generalisability”:  Applying a discount factor to cost-effectiveness estimates.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50106</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/07/replicability-generalisability-applying-a-discount-factor-to-cost-effectiveness-estimates/"/>
        <updated>2024-02-07T14:54:22.000Z</updated>
        <summary type="html"><![CDATA[This one’s important. Matt Lerner points us to this report by Rosie Bettle, Replicability & Generalisability: A Guide to CEA discounts. “CEA” is cost-effectiveness analysis, and by “discounts” they mean what we’ve called the Edlin factor—“discount” is a better name … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Free Courses to Master Python for Data Science]]></title>
        <id>https://www.kdnuggets.com/?p=163827</id>
        <link href="https://www.kdnuggets.com/5-free-courses-to-master-python-for-data-science?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-courses-to-master-python-for-data-science"/>
        <updated>2024-02-07T13:00:52.000Z</updated>
        <summary type="html"><![CDATA[Want to learn Python to kickstart your career in data? Here are five free courses to help you master Python for data science.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I’ve been mistaken for a chatbot]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50133</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/06/ive-been-mistaken-for-a-chatbot/"/>
        <updated>2024-02-07T04:07:06.000Z</updated>
        <summary type="html"><![CDATA[… Or not, according to what language is allowed. At the start of the year I mentioned that I am on a bad roll with AI just now, and the start of that roll began in late November when I … Continue reading →]]></summary>
        <author>
            <name>Lizzie</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Fluency Polyglot Survey]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1aki4o8/natural_fluency_polyglot_survey/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1aki4o8/natural_fluency_polyglot_survey/"/>
        <updated>2024-02-06T19:25:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SlavXyy  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph neural networks in TensorFlow]]></title>
        <id>http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html</id>
        <link href="http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html"/>
        <updated>2024-02-06T19:17:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML




Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks. Discrete mathematics and computer science have a long history of formalizing such networks as graphs, consisting of nodes connected by edges in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. 




Graph neural networks, or GNNs for short, have emerged…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking Down DENSE_RANK(): A Step-by-Step Guide for SQL Enthusiasts]]></title>
        <id>https://www.kdnuggets.com/?p=163817</id>
        <link href="https://www.kdnuggets.com/breaking-down-denserank-a-step-by-step-guide-for-sql-enthusiasts?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=breaking-down-dense_rank-a-step-by-step-guide-for-sql-enthusiasts"/>
        <updated>2024-02-06T17:00:33.000Z</updated>
        <summary type="html"><![CDATA[This article introduced you to the world of ranking functions in SQL. We will cover the basics of how they work, how they're used, and how to avoid common pitfalls.]]></summary>
        <author>
            <name>John Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Low Precision Arithmetic Safe?]]></title>
        <id>https://www.johndcook.com/blog/?p=242706</id>
        <link href="https://www.johndcook.com/blog/2024/02/06/is-low-precision-arithmetic-safe/"/>
        <updated>2024-02-06T15:20:34.000Z</updated>
        <summary type="html"><![CDATA[The popularity of low precision arithmetic for computing has exploded since the 2017 release of the Nvidia Volta GPU. The half precision tensor cores of Volta offered a massive 16X performance gain over double precision for key operations. The “race to the bottom” for lower precision computations continues: some have even solved significant problems using […]
Is Low Precision Arithmetic Safe? first appeared on John D. Cook.]]></summary>
        <author>
            <name>Wayne Joubert</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 FREE Courses on AI and ChatGPT to Take You From 0-100]]></title>
        <id>https://www.kdnuggets.com/?p=163810</id>
        <link href="https://www.kdnuggets.com/5-free-courses-on-ai-and-chatgpt-to-take-you-from-0-100?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-courses-on-ai-and-chatgpt-to-take-you-from-0-100"/>
        <updated>2024-02-06T15:00:50.000Z</updated>
        <summary type="html"><![CDATA[Want to learn more about AI and ChatGPT in 2024 for FREE? Keep reading.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How likely is a random variable to be far from its center?]]></title>
        <id>https://www.johndcook.com/blog/?p=242737</id>
        <link href="https://www.johndcook.com/blog/2024/02/06/chevyshev-variations/"/>
        <updated>2024-02-06T14:33:55.000Z</updated>
        <summary type="html"><![CDATA[There are many answers to the question in the title: How likely is a random variable to be far from its center? The answers depend on how much you’re willing to assume about your random variable. The more you can assume, the stronger your conclusion. The answers also depend on what you mean by “center,” […]
How likely is a random variable to be far from its center? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It’s bezzle time:  The Dean of Engineering at the University of Nevada gets paid $372,127 a year and wrote a paper that’s so bad, you can’t believe it.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50113</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/06/its-bezzle-time-the-dean-of-engineering-at-the-university-of-nevada-gets-paid-372127-a-year-and-wrote-a-paper-thats-so-bad-you-cant-believe-it-i-mean-really-you-have-to-take-a-look-at-t/"/>
        <updated>2024-02-06T14:12:08.000Z</updated>
        <summary type="html"><![CDATA[“As we look to sleep and neuroscience for answers we can study flies specifically the Drosophila melanogaster we highlight in our research.” 1. The story Someone writes: I recently read a paper of yours in the Chronicle about how academic … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Essential Guide to SQL’s Execution Order]]></title>
        <id>https://www.kdnuggets.com/?p=163799</id>
        <link href="https://www.kdnuggets.com/the-essential-guide-to-sql-execution-order?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-essential-guide-to-sqls-execution-order"/>
        <updated>2024-02-06T13:00:05.000Z</updated>
        <summary type="html"><![CDATA[Discovering the Hidden Logic Behind SQL's Command Order.]]></summary>
        <author>
            <name>Josep Ferrer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Future software development may require fewer humans]]></title>
        <id>https://yanirseroussi.com/til/2024/02/06/future-software-development-may-require-fewer-humans/</id>
        <link href="https://yanirseroussi.com/til/2024/02/06/future-software-development-may-require-fewer-humans/"/>
        <updated>2024-02-06T06:15:00.000Z</updated>
        <summary type="html"><![CDATA[Reflecting on an interview with Jason Warner, CEO of poolside.]]></summary>
        <author>
            <name>Yanir Seroussi | Data &amp; AI for Impact</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cherry blossoms—not just another prediction competition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50128</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/05/cherry-blossoms-not-just-another-prediction-competition/"/>
        <updated>2024-02-05T22:49:20.000Z</updated>
        <summary type="html"><![CDATA[It’s back! As regular readers know, the Cherry Blossom Prediction Competition will run throughout February 2024. We challenge you to predict the bloom date of cherry trees at five locations throughout the world and win prizes. We’ve been promoting the … Continue reading →]]></summary>
        <author>
            <name>Lizzie</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using the term “visualization” for non-visual representation of data]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50126</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/05/using-the-term-visualization-for-non-visual-representation-of-data/"/>
        <updated>2024-02-05T22:06:44.000Z</updated>
        <summary type="html"><![CDATA[The other day we linked to a study whose purpose was to “investigate challenges faced by curators of data visualizations for blind and low-vision individuals.” JooYoung Seo, the organizer of that project, provides further background: With the exception of a … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Most Impressive NLP Minds]]></title>
        <id>https://www.reddit.com/r/LanguageTechnology/comments/1ajqvdd/most_impressive_nlp_minds/</id>
        <link href="https://www.reddit.com/r/LanguageTechnology/comments/1ajqvdd/most_impressive_nlp_minds/"/>
        <updated>2024-02-05T20:37:25.000Z</updated>
        <summary type="html"><![CDATA[Hi! I'm sorry if this has already been asked, and if it has kindly point me in the direction of that post and I would be most appreciative.
 ​
 My question: as people who know a lot more about the NLP landscape than I do, who are the people you consider to be the most impressive minds in the field and why?
 ​
 Any opinions would be amazing. Thank you!
    submitted by    /u/meltymeems  
 [link]   [comments]]]></summary>
        <author>
            <name>Natural Language Processing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Books, Courses, and Live Events to Learn Generative AI with O’Reilly]]></title>
        <id>https://www.kdnuggets.com/?p=163858</id>
        <link href="https://www.kdnuggets.com/books-courses-and-live-events-to-learn-generative-ai-with-oreilly?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=books-courses-and-live-events-to-learn-generative-ai-with-oreilly"/>
        <updated>2024-02-05T18:00:20.000Z</updated>
        <summary type="html"><![CDATA[If you are new to generative AI or an expert who wants to learn more, O’Reilly offers a range of resources to kickstart your generative AI journey.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Maturity: The Cornerstone of AI-Enabled Innovation]]></title>
        <id>https://www.kdnuggets.com/?p=163789</id>
        <link href="https://www.kdnuggets.com/data-maturity-the-cornerstone-of-ai-enabled-innovation?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=data-maturity-the-cornerstone-of-ai-enabled-innovation"/>
        <updated>2024-02-05T17:00:33.000Z</updated>
        <summary type="html"><![CDATA[This article outlines strategies for overcoming data maturity challenges and accelerating AI adoption.]]></summary>
        <author>
            <name>Oleg Royz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Surge in Tech Layoffs 2024: Who’s to Blame?]]></title>
        <id>https://www.kdnuggets.com/?p=163782</id>
        <link href="https://www.kdnuggets.com/the-surge-in-tech-layoffs-2024-who-to-blame?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-surge-in-tech-layoffs-2024-whos-to-blame"/>
        <updated>2024-02-05T15:00:05.000Z</updated>
        <summary type="html"><![CDATA[The number of tech layoffs since 2022 is constantly rising. Why is this happening?]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new argument for estimating the probability that your vote will be decisive]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49037</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/05/a-new-argument-estimating-the-probability-that-your-vote-will-be-decisive/"/>
        <updated>2024-02-05T14:40:52.000Z</updated>
        <summary type="html"><![CDATA[Toby Ord writes: I think you will like this short proof that puts a lower bound on the probability that one’s vote is decisive. It requires just one assumption (that the probability distribution over vote share is unimodal) and takes … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow]]></title>
        <id>https://pyimagesearch.com/?p=37872</id>
        <link href="https://pyimagesearch.com/2024/02/05/evaluating-siamese-network-accuracy-f1-score-precision-and-recall-with-keras-and-tensorflow/"/>
        <updated>2024-02-05T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow Building the Face Recognition Application with Siamese Networks Introduction to Model Evaluation in Face Recognition Introduction to Siamese Networks in Facial Recognition Systems Utilizing Siamese…
The post Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow appeared first on PyImageSearch.]]></summary>
        <author>
            <name>Shivam Chandhok</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Lake, You Call It? It’s a Data Swamp]]></title>
        <id>https://www.kdnuggets.com/?p=163774</id>
        <link href="https://www.kdnuggets.com/a-data-lake-you-call-it-it-a-data-swamp?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=a-data-lake-you-call-it-its-a-data-swamp"/>
        <updated>2024-02-05T13:00:51.000Z</updated>
        <summary type="html"><![CDATA[How and why the data lake architecture often fails to meet its promises. And how better governance helps mitigate such challenges.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Substance over titles: Your first data hire may be a data scientist]]></title>
        <id>https://yanirseroussi.com/2024/02/05/substance-over-titles-your-first-data-hire-may-be-a-data-scientist/</id>
        <link href="https://yanirseroussi.com/2024/02/05/substance-over-titles-your-first-data-hire-may-be-a-data-scientist/"/>
        <updated>2024-02-05T02:45:00.000Z</updated>
        <summary type="html"><![CDATA[Advice for hiring a startup’s first data person: match skills to business needs, consider contractors, and get help from data people.]]></summary>
        <author>
            <name>Yanir Seroussi | Data &amp; AI for Impact</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Relationships 101: Lessons from the Ames Housing Data]]></title>
        <id>https://machinelearningmastery.com/?p=15614</id>
        <link href="https://machinelearningmastery.com/feature-relationships-101/"/>
        <updated>2024-02-05T02:11:57.000Z</updated>
        <summary type="html"><![CDATA[In the realm of real estate, understanding the intricacies of property features and their impact on sale prices is paramount. In this exploration, we’ll dive deep into the Ames Housing dataset, shedding light on the relationships between various features and their correlation with the sale price. Harnessing the power of data visualization, we’ll unveil patterns, […]
The post Feature Relationships 101: Lessons from the Ames Housing Data appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Nesta uses NLP to process 7m job ads and shed light on the UK’s labor market]]></title>
        <id>blog:nesta-skills</id>
        <link href="https://explosion.ai/explosion.ai/blog/nesta-skills"/>
        <updated>2024-02-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A case study on Nesta’s workflow for extracting 7 million job ads to better understand UK skill demand, using a custom mapping step to match skills to any government taxonomy.]]></summary>
        <author>
            <name>India Kerle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating samplers with reference draws]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50120</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/04/evaluating-samplers-with-reference-draws/"/>
        <updated>2024-02-04T20:00:37.000Z</updated>
        <summary type="html"><![CDATA[I started thinking a bit more about what we’re doing when we use something like posteriordb (from Stan) or Inference Gym (from TensorFlow Probability) to evaluate a sampler. posteriordb gives you Stan programs and 10,000 reference draws from their posterior. … Continue reading →]]></summary>
        <author>
            <name>Bob Carpenter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Machine Learning Engineer 🤖 #268 -     Democratising LLM Inference, Google Forecast Foundation Model,
    Cyber-resilience Act, Stanford's Algorithms,
    LLaVA 1.16, ML Frameworks
    + more 🚀]]></title>
        <id>https://kill-the-newsletter.com/alternates/9qh1v0h0qrzxv4w8.html</id>
        <link href="https://kill-the-newsletter.com/alternates/9qh1v0h0qrzxv4w8.html"/>
        <updated>2024-02-04T15:08:56.000Z</updated>
        <summary type="html"><![CDATA[The Machine Learning Engineer 🤖 #268#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…]]></summary>
        <author>
            <name>the ml engineer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Don’t feed the trolls” and the troll semi-bluff]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49712</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/04/dont-feed-the-trolls-and-the-troll-semi-bluff/"/>
        <updated>2024-02-04T14:17:51.000Z</updated>
        <summary type="html"><![CDATA[I was dealing with some trolling in blog comments awhile ago and someone sent me an email of support, appreciating my patience in my responses to the troll. The standard advice is “Don’t feed the trolls,” but usually here it … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connecting the FFT and quadratic reciprocity]]></title>
        <id>https://www.johndcook.com/blog/?p=242694</id>
        <link href="https://www.johndcook.com/blog/2024/02/03/fft-reciprocity/"/>
        <updated>2024-02-03T19:59:27.000Z</updated>
        <summary type="html"><![CDATA[Some readers will look at the title of this post and think “Ah yes, the FFT. I use it all the time. But what is this quadratic reciprocity?” Others will look at the same title and think “Gauss called the quadratic reciprocity theorem the jewel in the crown of mathematics. But what is this FFT […]
Connecting the FFT and quadratic reciprocity first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A question about Lindley’s supra Bayesian method for expert probability assessment]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48802</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/03/a-question-about-lindleys-supra-bayesian-method-for-expert-probability-assessment/"/>
        <updated>2024-02-03T14:27:53.000Z</updated>
        <summary type="html"><![CDATA[Andy Solow writes: I wonder if you can help me with a question that has been bugging me for a while? I have been thinking about Lindley’s supra Bayesian method for expert probability assessment. Briefly, the model is that, conditional … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A decoder-only foundation model for time-series forecasting]]></title>
        <id>http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html</id>
        <link href="http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html"/>
        <updated>2024-02-02T19:07:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Rajat Sen and Yichen Zhou, Google Research





Time-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models performed well in the M5 competition).



At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as translation, retrieval-augmented generation, and code completion. Thes…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intervening on early readouts for mitigating spurious features and simplicity bias]]></title>
        <id>http://blog.research.google/2024/02/intervening-on-early-readouts-for.html</id>
        <link href="http://blog.research.google/2024/02/intervening-on-early-readouts-for.html"/>
        <updated>2024-02-02T17:49:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research




Machine learning models in the real world are often trained on limited data that may contain unintended statistical biases. For example, in the CELEBA celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as medical diagnosis. 





Surprisingly, recent work has also discovered an inherent tendency of deep networks to amplify such statistical biases, through the so-called simplicity bias of deep learning. T…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide]]></title>
        <id>https://www.kdnuggets.com/?p=163748</id>
        <link href="https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=openai-api-for-beginners-your-easy-to-follow-starter-guide"/>
        <updated>2024-02-02T15:00:07.000Z</updated>
        <summary type="html"><![CDATA[Learn how to use OpenAI Python API for accessing language, embedding, audio, vision, and image generation models.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Participants reported being hungrier when they walked into the café (mean = 7.38, SD = 2.20) than when they walked out [mean = 1.53, SD = 2.70, F(1, 75) = 107.68, P < 0.001]."]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49268</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/02/participants-reported-being-hungrier-when-they-walked-into-the-cafe-mean-7-38-sd-2-20-than-when-they-walked-out-mean-1-53-sd-2-70-f1-75-107-68-p-0-001/"/>
        <updated>2024-02-02T14:46:19.000Z</updated>
        <summary type="html"><![CDATA[Jonathan Falk came across this article and writes: Is there any possible weaker conclusion than “providing caloric information may help some adults with food decisions”? Is there any possible dataset which would contradict that conclusion? On one hand, gotta give … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Converting JSONs to Pandas DataFrames: Parsing Them the Right Way]]></title>
        <id>https://www.kdnuggets.com/?p=163710</id>
        <link href="https://www.kdnuggets.com/converting-jsons-to-pandas-dataframes-parsing-them-the-right-way?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=converting-jsons-to-pandas-dataframes-parsing-them-the-right-way"/>
        <updated>2024-02-02T13:00:10.000Z</updated>
        <summary type="html"><![CDATA[Navigating Complex Data Structures with Python's json_normalize.]]></summary>
        <author>
            <name>Josep Ferrer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-digit zip codes]]></title>
        <id>https://www.johndcook.com/blog/?p=242690</id>
        <link href="https://www.johndcook.com/blog/2024/02/01/two-digit-zip-codes/"/>
        <updated>2024-02-02T02:13:39.000Z</updated>
        <summary type="html"><![CDATA[It’s common to truncate US zip codes to the first three digits for privacy reasons. Truncating to the first two digits is less common, but occurs in some data sets. HIPAA Safe Harbor requires sparse 3-digit zip codes to be suppressed; even when rolled up to three digits some regions are still sparsely populated. How […]
Two-digit zip codes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Click here to help this researcher gather different takes on making data visualizations for blind people]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50107</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/01/click-here-to-help-this-researcher-gather-different-takes-on-making-data-visualizations-for-blind-people/"/>
        <updated>2024-02-02T01:37:16.000Z</updated>
        <summary type="html"><![CDATA[Here’s the survey, and here’s what it says: The purpose of this study is to investigate challenges faced by curators of data visualizations for blind and low-vision individuals. This includes, but is not limited to, graphs, charts, plots, diagrams, and … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Describing Images Fast and Slow: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes]]></title>
        <id>https://arxiv.org/abs/2402.01352</id>
        <link href="https://arxiv.org/abs/2402.01352"/>
        <updated>2024-02-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We use the spaCy library for tokenization, part-of-speech tagging, and lemmatization of the words in the descriptions.]]></summary>
        <author>
            <name>Explosion</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[26 Data Science Interview Questions You Should Know]]></title>
        <id>https://www.kdnuggets.com/?p=163701</id>
        <link href="https://www.kdnuggets.com/26-data-science-interview-questions-you-should-know?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=26-data-science-interview-questions-you-should-know"/>
        <updated>2024-02-01T15:00:59.000Z</updated>
        <summary type="html"><![CDATA[Learn about the most common questions asked during data science interviews. This blog covers non-technical, Python, SQL, statistics, data analysis, and machine learning questions.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lancet-bashing!]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49651</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/02/01/lancet-bashing/"/>
        <updated>2024-02-01T14:20:53.000Z</updated>
        <summary type="html"><![CDATA[Retraction Watch points to this fun article by Ashley Rindsberg, “The Lancet was made for political activism,” subtitled, For 200 years, it has thrived on melodrama and scandal. And they didn’t even mention Surgisphere (for more detail, see here) or … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 5 AI Podcasts You Can’t Miss in 2024]]></title>
        <id>https://www.kdnuggets.com/?p=163682</id>
        <link href="https://www.kdnuggets.com/top-5-ai-podcasts-you-cant-miss-in-2024?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=top-5-ai-podcasts-you-cant-miss-in-2024"/>
        <updated>2024-02-01T13:00:42.000Z</updated>
        <summary type="html"><![CDATA[Tune in to these 5 AI podcasts at the gym or on your commute to keep up to date with the world of AI.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Pre-training of Large Autoregressive Image Models]]></title>
        <id>autoregressive-image-models</id>
        <link href="https://machinelearning.apple.com/research/autoregressive-image-models"/>
        <updated>2024-02-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileDiffusion: Rapid text-to-image generation on-device]]></title>
        <id>http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</id>
        <link href="http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html"/>
        <updated>2024-01-31T21:59:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML




Text-to-image diffusion models have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., Stable Diffusion, DALL·E, and Imagen). While recent advancements in inference solutions on Android via MediaPipe and iOS via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
 

To that end, in “MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices”, we introduce a novel approach with the potential for rapid text-to-image generation on…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dictionaries, Classifying Variables, and Imputing Data in the Ames Dataset]]></title>
        <id>https://machinelearningmastery.com/?p=15473</id>
        <link href="https://machinelearningmastery.com/classifying_variables/"/>
        <updated>2024-01-31T19:56:26.000Z</updated>
        <summary type="html"><![CDATA[The real estate market is a complex ecosystem driven by numerous variables such as location, property features, market trends, and economic indicators. One dataset that offers a deep dive into this complexity is the Ames Housing dataset. Originating from Ames, Iowa, this dataset comprises various properties and their characteristics, ranging from the type of alley […]
The post Exploring Dictionaries, Classifying Variables, and Imputing Data in the Ames Dataset appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bessel zero spacing]]></title>
        <id>https://www.johndcook.com/blog/?p=242204</id>
        <link href="https://www.johndcook.com/blog/2024/01/31/bessel-zero-spacing/"/>
        <updated>2024-01-31T16:44:36.000Z</updated>
        <summary type="html"><![CDATA[Bessel functions are to polar coordinates what sines and cosines are to rectangular coordinates. This is why Bessel function often arise in applications with radial symmetry. The locations of the zeros of Bessel functions are important in application, and so you can find software for computing these zeros in mathematical libraries. In days gone by […]
Bessel zero spacing first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximizing Efficiency in Data Analysis with ChatGPT]]></title>
        <id>https://www.kdnuggets.com/?p=163691</id>
        <link href="https://www.kdnuggets.com/maximizing-efficiency-in-data-analysis-with-chatgpt?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=maximizing-efficiency-in-data-analysis-with-chatgpt"/>
        <updated>2024-01-31T15:00:26.000Z</updated>
        <summary type="html"><![CDATA[This article has provided a brief overview of ChatGPT and its capabilities. It also discussed the importance of efficient data analysis and the benefits of integrating it into the analysis process.]]></summary>
        <author>
            <name>Vijay Singh Khatri</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opposition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49647</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/31/opposition/"/>
        <updated>2024-01-31T14:53:52.000Z</updated>
        <summary type="html"><![CDATA[Following the recommendation of Elin in comments, I checked out the podcast, If Books Could Kill. It seemed like the kinda thing I might like: 2 guys going back and forth taking apart Gladwell, Freakonomics, David Brooks, Nudge, and other … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Free Courses to Break Into Data Analytics]]></title>
        <id>https://www.kdnuggets.com/?p=163676</id>
        <link href="https://www.kdnuggets.com/5-free-courses-to-break-into-data-analytics?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-courses-to-break-into-data-analytics"/>
        <updated>2024-01-31T13:00:10.000Z</updated>
        <summary type="html"><![CDATA[Looking to make a career in data analytics? Take the first steps today with these free courses.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning]]></title>
        <id>http://www.nowpublishers.com/article/Details/MAL-106</id>
        <link href="http://www.nowpublishers.com/article/Details/MAL-106"/>
        <updated>2024-01-30T23:00:00.000Z</updated>
        <summary type="html"><![CDATA[Abstract
Decision-making systems based on AI and machine learning
have been used throughout a wide range of real-world
scenarios, including healthcare, law enforcement, education,
and finance. It is no longer far-fetched to envision a future
where autonomous systems will drive entire business decisions
and, more broadly, support large-scale decision-making
infrastructure to solve society’s most challenging problems.
Issues of unfairness and discrimination are pervasive when
decisions are being made by humans, and remain (or are potentially
amplified) when decisions are made using machines
with little transparency, accountability, and fairness. In this
monograph, we introduce a framework for causal fairness
analysis with the intent of filling in this gap, i.e., understanding,
modeling, and possibly solving issues of fairness
in decision-making settings.
The main insight of our approach will be to link the quantification
of the disparities present in the observed data
with the underlying, often unobserved, collection of causal
mechanisms that generate the disparity in the first place, 
a challenge we call the Fundamental Problem of Causal
Fairness Analysis (FPCFA). In order to solve the FPCFA,
we study the problem of decomposing variations and empirical
measures of fairness that attribute such variations
to structural mechanisms and different units of the population.
Our effort culminates in the Fairness Map, the first
systematic attempt to organize and explain the relationship
between various criteria found in the literature. Finally, we
study which causal assumptions are minimally needed for
performing causal fairness analysis and propose the Fairness
Cookbook, which allows one to assess the existence of
disparate impact and disparate treatment.
Suggested Citation
Drago Plečko and Elias Bareinboim (2024), "Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning", Foundations and Trends® in Machine Learning: Vol. 17: No. 3, pp 304-589. http://dx.doi.org/10.1561/2200000106]]></summary>
        <author>
            <name>Articles for MAL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum criteria for studies evaluating human decision-making]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50096</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/30/minimum-criteria-for-studies-evaluating-human-decision-making/"/>
        <updated>2024-01-30T18:45:59.000Z</updated>
        <summary type="html"><![CDATA[This is Jessica. A while back on the blog I shared some opinions about studies of human-decision making, such as to understand how visualizations or displays of model predictions and explanations impact people’s behavior. My view is essentially that a … Continue reading →]]></summary>
        <author>
            <name>Jessica Hullman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why LLMs Used Alone Can’t Address Your Company’s Predictive Needs]]></title>
        <id>https://www.kdnuggets.com/?p=163726</id>
        <link href="https://www.kdnuggets.com/2024/01/pecan-llms-used-alone-cant-address-companys-predictive-needs?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=why-llms-used-alone-cant-address-your-companys-predictive-needs"/>
        <updated>2024-01-30T18:00:01.000Z</updated>
        <summary type="html"><![CDATA[LLMs aren't the right tool for most business applications. Find out why — and learn which AI techniques are a better match.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does ChatGPT Have The Potential To Become A New Chess Super Grandmaster?]]></title>
        <id>https://www.kdnuggets.com/?p=163658</id>
        <link href="https://www.kdnuggets.com/does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster"/>
        <updated>2024-01-30T17:00:02.000Z</updated>
        <summary type="html"><![CDATA[Case study of LLM's ability to learn, generalize, and be creative.]]></summary>
        <author>
            <name>Nikola Greb</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coloring the queen’s graph]]></title>
        <id>https://www.johndcook.com/blog/?p=241794</id>
        <link href="https://www.johndcook.com/blog/2024/01/30/coloring-the-queens-graph/"/>
        <updated>2024-01-30T15:04:57.000Z</updated>
        <summary type="html"><![CDATA[Suppose we have an n × n chessboard. The case n = 8 is of course most common, but we consider all positive integer values of n. The graph of a chess piece has an edge between two squares if and only if the piece can legally move between the two squares. Now suppose we […]
Coloring the queen’s graph first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Step by Step Guide to Reading and Understanding SQL Queries]]></title>
        <id>https://www.kdnuggets.com/?p=163651</id>
        <link href="https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=a-step-by-step-guide-to-reading-and-understanding-sql-queries"/>
        <updated>2024-01-30T15:00:42.000Z</updated>
        <summary type="html"><![CDATA[Complex queries seem intimidating, but this guide gives you insight into how to work more easily with SQL queries.]]></summary>
        <author>
            <name>Cornellius Yudha Wijaya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hey, I got tagged by RetractoBot!]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50065</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/30/retractobot-you-cited-a-retracted-paper/"/>
        <updated>2024-01-30T14:42:52.000Z</updated>
        <summary type="html"><![CDATA[A message came in my inbox from “The RetractoBot Team, University of Oxford,” with subject line, “RetractoBot: You cited a retracted paper”: That’s funny! When we cited that paper by Lacour and Green, we already knew it was no good. … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Free University Courses to Ace Coding Interviews]]></title>
        <id>https://www.kdnuggets.com/?p=163644</id>
        <link href="https://www.kdnuggets.com/5-free-university-courses-to-ace-coding-interviews?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-university-courses-to-ace-coding-interviews"/>
        <updated>2024-01-30T13:00:57.000Z</updated>
        <summary type="html"><![CDATA[For acing coding interviews, you need to have a rock solid foundation in data structures and algorithms. Check out these free university courses to help you in your journey.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview of Composite Outcome Scales & Statistical Approaches for Analyzing Them]]></title>
        <id>https://fharrell.com/talk/cos/</id>
        <link href="https://fharrell.com/talk/cos/"/>
        <updated>2024-01-30T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[Slides
Elaborations
Video]]></summary>
        <author>
            <name>Frank Harrell</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Profiling your Numba code]]></title>
        <id>https://pythonspeed.com/articles/numba-profiling/</id>
        <link href="https://pythonspeed.com/articles/numba-profiling/"/>
        <updated>2024-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[pre {
    font-size: 90% !important;
}


If you’re writing numeric Python code, Numba can be a great way to speed up your program.
By compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python.
In other words, it’s similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python.
Numba code isn’t always as fast as it could be, however.
This is where profiling is useful: it can find at least some of the bottlenecks in your code.
In this article we’ll cover:
Profila, a new profiler I’ve released that is specifically designed for Numba code.
The limits of profiling.
There are many potential performance enhancements that a profiler can’t and won’t help you discover.
Read more...]]></summary>
        <author>
            <name>Python⇒Speed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Data to Map: Visualizing Ames House Prices with Python]]></title>
        <id>https://machinelearningmastery.com/?p=15741</id>
        <link href="https://machinelearningmastery.com/data-to-map-geospatial/"/>
        <updated>2024-01-29T22:07:30.000Z</updated>
        <summary type="html"><![CDATA[Geospatial visualization has become an essential tool for understanding and representing data in a geographical context. It plays a pivotal role in various real-world applications, from urban planning and environmental studies to real estate and transportation. For instance, city planners might use geospatial data to optimize public transportation routes, while real estate professionals could leverage […]
The post From Data to Map: Visualizing Ames House Prices with Python appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fun with Dååta: Reference librarian edition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50094</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/29/fun-with-daata-reference-librarian/"/>
        <updated>2024-01-29T19:56:26.000Z</updated>
        <summary type="html"><![CDATA[Rasmuth Bååth reports the following fun story in a blog post, The source of the cake dataset (it’s a hierarchical modeling example included with the R package lme4). Rasmuth writes, While looking for a dataset to illustrate a simple hierarchical … Continue reading →]]></summary>
        <author>
            <name>Bob Carpenter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regex to match SWIFT-BIC codes]]></title>
        <id>https://www.johndcook.com/blog/?p=241456</id>
        <link href="https://www.johndcook.com/blog/2024/01/29/swift/"/>
        <updated>2024-01-29T19:47:52.000Z</updated>
        <summary type="html"><![CDATA[A SWIFT-BIC number identifies a bank, not a particular bank account. The BIC part stands for Bank Identifier Code. I had to look up the structure of SWIFT-BIC codes recently, and here it is: Four letters to identify the bank Two letters to identify the country Two letters or digits to identify the location Optionally, […]
Regex to match SWIFT-BIC codes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Processing: Bridging Human Communication with AI]]></title>
        <id>https://www.kdnuggets.com/?p=163595</id>
        <link href="https://www.kdnuggets.com/natural-language-processing-bridging-human-communication-with-ai?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=natural-language-processing-bridging-human-communication-with-ai"/>
        <updated>2024-01-29T17:00:16.000Z</updated>
        <summary type="html"><![CDATA[The post highlights real-world examples of NLP use cases across industries. It also covers NLP's objectives, challenges, and latest research developments.]]></summary>
        <author>
            <name>Olena Zherebetska</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Colossal-LLaMA-2: Low Cost and High-quality Domain-specific LLM Solution Using LLaMA and…]]></title>
        <id>https://medium.com/p/26d2e4b9fd92</id>
        <link href="https://medium.com/pytorch/colossal-llama-2-low-cost-and-high-quality-domain-specific-llm-solution-using-llama-and-26d2e4b9fd92?source=rss----512b8efdf2e7---4"/>
        <updated>2024-01-29T16:40:21.000Z</updated>
        <summary type="html"><![CDATA[The most prominent distinction between LLaMA-1 and LLaMA-2 lies in the incorporation of higher-quality corpora, a pivotal factor…]]></summary>
        <author>
            <name>Yang You</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What I Learned From Using ChatGPT for Data Science]]></title>
        <id>https://www.kdnuggets.com/?p=163578</id>
        <link href="https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=what-i-learned-from-using-chatgpt-for-data-science"/>
        <updated>2024-01-29T15:00:51.000Z</updated>
        <summary type="html"><![CDATA[ChatGPT can be a great tool for data scientists. Here’s what I learned about where it excels and where it is less so.]]></summary>
        <author>
            <name>Nate Rosidi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“When will AI be able to do scientific research both cheaper and better than us, thus effectively obsoleting humans?”]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48180</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/29/when-will-ai-be-able-to-do-scientific-research-both-cheaper-and-better-than-us-thus-effectively-obsoleting-humans/"/>
        <updated>2024-01-29T14:59:38.000Z</updated>
        <summary type="html"><![CDATA[Alexey Guzey asks: How much have you thought about AI and when will AI be able to do scientific research both cheaper and better than us, thus effectively obsoleting humans? My first reply: I guess that AI can already do … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL)]]></title>
        <id>https://pyimagesearch.com/?p=40831</id>
        <link href="https://pyimagesearch.com/2024/01/29/adversarial-learning-with-keras-and-tensorflow-part-3-exploring-adversarial-attacks-using-neural-structured-learning-nsl/"/>
        <updated>2024-01-29T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL) Introduction to Advanced Adversarial Techniques in Machine Learning Harnessing NSL for Robust Model Training: Insights from Part 2 Deep Dive into…
The post Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL) appeared first on PyImageSearch.]]></summary>
        <author>
            <name>Shivam Chandhok</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn with LinkedIn: Free Courses About AI]]></title>
        <id>https://www.kdnuggets.com/?p=163571</id>
        <link href="https://www.kdnuggets.com/learn-with-linkedin-free-courses-about-ai?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=learn-with-linkedin-free-courses-about-ai"/>
        <updated>2024-01-29T13:00:14.000Z</updated>
        <summary type="html"><![CDATA[Want to learn about AI? You can for FREE with LinkedIn.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acoustic Model Fusion for End-to-end Speech Recognition]]></title>
        <id>acoustic-model-fusion</id>
        <link href="https://machinelearning.apple.com/research/acoustic-model-fusion"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted its accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale Training of Foundation Models for Wearable Biosignals]]></title>
        <id>large-scale-training</id>
        <link href="https://machinelearning.apple.com/research/large-scale-training"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite the widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding]]></title>
        <id>flexible-keyword</id>
        <link href="https://machinelearning.apple.com/research/flexible-keyword"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (for example, large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder, which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Salient Representations and Label Variance Modeling in Dimensional Speech Emotion Analysis]]></title>
        <id>investigating-salient-representation</id>
        <link href="https://machinelearning.apple.com/research/investigating-salient-representation"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Representations from models such as Bidirectional Encoder Representations from Transformers (BERT) and Hidden units BERT (HuBERT) have helped to achieve state-of-the-art performance in dimensional speech emotion recognition. Both HuBERT, and BERT models generate fairly large dimensional representations, and such models were not trained with emotion recognition task in mind. Such large dimensional representations result in speech emotion models with large parameter size, resulting in both memory and computational cost complexities. In this work, we investigate the selection of representations…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-ML: Collaborative Machine Learning Model Building for Developing Dataset Design Practices]]></title>
        <id>coml</id>
        <link href="https://machinelearning.apple.com/research/coml"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are fundamentally shaped by data, and building inclusive ML systems requires significant considerations around how to design representative datasets. Yet, few novice-oriented ML modeling tools are designed to foster hands-on learning of dataset design practices, including how to design for data diversity and inspect for data quality.
To this end, we outline a set of four data design practices (DDPs) for designing inclusive ML models and share how we designed a tablet-based application called Co-ML to foster the learning of DDPs through a collbaborative ML model…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates]]></title>
        <id>user-level-differentially</id>
        <link href="https://machinelearning.apple.com/research/user-level-differentially"/>
        <updated>2024-01-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We study differentially private stochastic convex optimization (DP-SCO) under user-level privacy where each user may hold multiple data items. Existing work for  user-level DP-SCO either requires super-polynomial runtime or requires number of users that grows polynomially with the dimensionality of the problem. We develop new algorithms for user-level DP-SCO that obtain optimal rates, run in polynomial time, and require a number of users that grow logarithmically in the dimension. Moreover, our algorithms are the first  to obtain optimal rates for non-smooth functions in polynomial time. These…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Machine Learning Engineer 🤖 #267 -     Sampling in Large Language Models
    UK Govt's GenAI Framework,    Why ML is Hard, AI Fundamentals, MLOps Events, ML Frameworks
    + more 🚀]]></title>
        <id>https://kill-the-newsletter.com/alternates/1krzbyojxlwbn7bo.html</id>
        <link href="https://kill-the-newsletter.com/alternates/1krzbyojxlwbn7bo.html"/>
        <updated>2024-01-28T15:39:34.000Z</updated>
        <summary type="html"><![CDATA[The Machine Learning Engineer 🤖 #267#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…]]></summary>
        <author>
            <name>the ml engineer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sympathy for the Nudgelords:  Vermeule endorsing stupid and dangerous election-fraud claims and Levitt promoting climate change denial are like cool dudes in the 60s wearing Che T-shirts and thinking Chairman Mao was cool—we think they’re playing with fire, they think they’re cute contrarians pointing out contradictions in the system.  For a certain kind of person, it’s fun to be a rogue.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48933</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/28/sy/"/>
        <updated>2024-01-28T14:04:58.000Z</updated>
        <summary type="html"><![CDATA[A few months ago I wrote about some disturbing stuff I’d been hearing about from Harvard Law School professors Cass Sunstein and Adrian Vermeuele. The two of them wrote an article back in 2005 writing, “a refusal to impose [the … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad takes on chaos theory]]></title>
        <id>https://www.johndcook.com/blog/?p=240612</id>
        <link href="https://www.johndcook.com/blog/2024/01/27/butterflies-dont-work-that-way/"/>
        <updated>2024-01-27T16:42:00.000Z</updated>
        <summary type="html"><![CDATA[I just finished reading The Three Body Problem. At the end of the book is a preview of Cixin Liu’s book Supernova Era. A bit of dialog in that preview stood out to me because it is touches on themes I’ve written about before. “I’ve heard about that. When a butterfly flaps its wings, there’s […]
Bad takes on chaos theory first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The paradox of replication studies:  A good analyst has special data analysis and interpretation skills.  But it’s considered a bad or surprising thing that if you give the same data to different analysts, they come to different conclusions.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49625</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/27/pt/"/>
        <updated>2024-01-27T14:04:58.000Z</updated>
        <summary type="html"><![CDATA[Benjamin Kircup writes: I think you will be very interested to see this preprint that is making the rounds: Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology (ecoevorxiv.org) I see several … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed-input matrix multiplication performance optimizations]]></title>
        <id>http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html</id>
        <link href="http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html"/>
        <updated>2024-01-26T19:56:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Manish Gupta, Staff Software Engineer, Google Research




AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs).  LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver tens of exaflops of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively.



The bulk of an LLM’s memory and compute are consumed by weights in matrix multiplication operations. Using narrower data types reduces memory consumption. For example, storing weights in the 8-bit integer (i.e., U8 or S8)…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[They solved the human-statistical reasoning interface back in the 80s]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50072</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/26/they-solved-the-human-statistical-reasoning-interface-back-in-the-80s/"/>
        <updated>2024-01-26T18:59:05.000Z</updated>
        <summary type="html"><![CDATA[Eytan Adar pointed me to a video, Reasoning Under Uncertainty, from the historical ACM SIGCHI (computer-human interaction) Video Project. Beyond fashionable hairstyles, it demos interfaces from a software curriculum to teach high school students statistical reasoning in 1989. They’ve got … Continue reading →]]></summary>
        <author>
            <name>Jessica Hullman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Ways To Make Code Run Faster]]></title>
        <id>https://www.johndcook.com/blog/?p=236641</id>
        <link href="https://www.johndcook.com/blog/2024/01/26/making-code-run-faster/"/>
        <updated>2024-01-26T18:25:11.000Z</updated>
        <summary type="html"><![CDATA[The news from Meta last week is a vivid reminder of the importance of making code run faster and more power-efficiently. Meta intends to purchase 350,000 Nvidia H100 GPUs this year [1]. Assuming 350W TDP [2] and $0.1621 per kW-h [3] average US energy cost, one expects a figure of $174 million per year in […]
New Ways To Make Code Run Faster first appeared on John D. Cook.]]></summary>
        <author>
            <name>Wayne Joubert</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Only Free Course You Need To Become a Professional Data Engineer]]></title>
        <id>https://www.kdnuggets.com/?p=163520</id>
        <link href="https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-professional-data-engineer?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-only-free-course-you-need-to-become-a-professional-data-engineer"/>
        <updated>2024-01-26T15:00:48.000Z</updated>
        <summary type="html"><![CDATA[Data Engineering ZoomCamp offers free access to reading materials, video tutorials, assignments, homeworks, projects, and workshops.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The importance of measurement, and how you can draw ridiculous conclusions from your statistical analyses if you don’t think carefully about measurement . . . Leamer (1983) got it.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49259</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/26/the-importance-of-measurement-and-how-you-can-draw-ridiculous-conclusions-from-your-statistical-analyses-if-you-dont-think-carefully-about-measurement-leamer-1983-got-it/"/>
        <updated>2024-01-26T14:44:22.000Z</updated>
        <summary type="html"><![CDATA[Jacob Klerman writes: I have noted your recent emphasis on the importance of measurement (e.g., “Here are some ways to make your study replicable…”). For reasons not relevant here, I was rereading Leamer (1983), Let’s Take the Con Out of … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQL Simplified: Crafting Modular and Understandable Queries with CTEs]]></title>
        <id>https://www.kdnuggets.com/?p=163439</id>
        <link href="https://www.kdnuggets.com/sql-simplified-crafting-modular-and-understandable-queries-with-ctes?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=sql-simplified-crafting-modular-and-understandable-queries-with-ctes"/>
        <updated>2024-01-26T13:00:11.000Z</updated>
        <summary type="html"><![CDATA[Learn how to write SQL queries that are not just code but clear, modular, and reusable work.]]></summary>
        <author>
            <name>Josep Ferrer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSCN #286: Will AI Match Human Performance In 1-5 Years? What are the risks?]]></title>
        <id>https://kill-the-newsletter.com/alternates/7e3m2axeo0jykbov.html</id>
        <link href="https://kill-the-newsletter.com/alternates/7e3m2axeo0jykbov.html"/>
        <updated>2024-01-25T16:41:58.000Z</updated>
        <summary type="html"><![CDATA[96
            
        
        
        
        
        
        DSCN #286: Will AI Match Human Performance In 1-5 Years? What are the risks?

    
		p{
			margin:10px 0;
			padding:0;
		}
		table{
			border-collapse:collapse;
		}
		h1,h2,h3,h4,h5,h6{
			display:block;
			margin:0;
			padding:0;
		}
		img,a img{
			border:0;
			height:auto;
			outline:none;
			text-decoration:none;
		}
		body,#bodyTable,#bodyCell{
			height:100%;
			margin:0;
			padding:0;
			width:100%;
		}
		.mcnPreviewText{
			display:none !important;
		}
		#outlook a{
			padding:0;
		}
		img{
			-ms-interpolation-mode:bicubic;
		}
		table{
			mso-table-lspace:0pt;
			mso-table-rspace:0pt;
		}
		.ReadMsgBody{
			width:100%;
		}
		.ExternalClass{
			width:100%;
		}
		p,a,li,td,blockquote{
			mso-line-height-rule:exac…]]></summary>
        <author>
            <name>Data Science Community newsletter sign-up</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brute force cryptanalysis]]></title>
        <id>https://www.johndcook.com/blog/?p=239550</id>
        <link href="https://www.johndcook.com/blog/2024/01/25/brute-force-cryptanalysis/"/>
        <updated>2024-01-25T15:08:34.000Z</updated>
        <summary type="html"><![CDATA[A naive view of simple substitution ciphers is that they are secure because there are 26! ways to permute the English alphabet, and so an attacker would have to try 26! ≈ 4 × 1026 permutations. However, such brute force is not required. In practice, simple substitution ciphers are breakable by hand in a few […]
Brute force cryptanalysis first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging AI to Design Fair and Equitable EV Charging Grids]]></title>
        <id>https://www.kdnuggets.com/?p=163480</id>
        <link href="https://www.kdnuggets.com/leveraging-ai-to-design-fair-and-equitable-ev-charging-grids?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=leveraging-ai-to-design-fair-and-equitable-ev-charging-grids"/>
        <updated>2024-01-25T15:00:41.000Z</updated>
        <summary type="html"><![CDATA[Learn how AI can be used to design fair and efficient EV charging grids by optimizing their placement and pricing.]]></summary>
        <author>
            <name>Ankur Gupta</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mister P and Stan go to Bangladesh . . .]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=48560</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/25/mister-p-and-stan-go-to-bangladesh/"/>
        <updated>2024-01-25T14:22:41.000Z</updated>
        <summary type="html"><![CDATA[Prabhat Barnwal, Yuling Yao, Yiqian Wang, Nishat Akter Juy, Shabib Raihan, Mohammad Ashraful Haque, and Alexander van Geen ask, Is the low COVID-19–related mortality reported in Bangladesh for 2020 associated with massive undercounting? Here’s what they did: This repeated survey … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[10 Advanced Git Techniques]]></title>
        <id>https://www.kdnuggets.com/?p=163509</id>
        <link href="https://www.kdnuggets.com/10-advanced-git-techniques?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=10-advanced-git-techniques"/>
        <updated>2024-01-25T13:00:04.000Z</updated>
        <summary type="html"><![CDATA[Improve your version control skills to resolve issues and maintain a clean Git repository.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Straddling checkerboard encryption]]></title>
        <id>https://www.johndcook.com/blog/?p=239523</id>
        <link href="https://www.johndcook.com/blog/2024/01/25/straddling-checkerboard-encryption/"/>
        <updated>2024-01-25T12:10:40.000Z</updated>
        <summary type="html"><![CDATA[Introduction Computers fundamentally changed cryptography, opening up new possibilities for making and breaking codes. At first it may not have been clear which side benefited most, but now it’s clear that computers gave more power to code makers than code breakers. We now have cryptographic primitives that cannot be attacked more efficiently than by brute […]
Straddling checkerboard encryption first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding Data: An Introduction to Descriptive Statistics with the Ames Housing Dataset]]></title>
        <id>https://machinelearningmastery.com/?p=15543</id>
        <link href="https://machinelearningmastery.com/decoding-data-descriptive-statistics/"/>
        <updated>2024-01-25T06:28:01.000Z</updated>
        <summary type="html"><![CDATA[In this enlightening journey through the myriad lanes of Ames properties, we shine our spotlight on Descriptive Statistics, a cornerstone of Data Science. The study of the Ames properties dataset provides a rich landscape for implementing Descriptive Statistics to distill volumes of data into meaningful summaries. Descriptive statistics serve as the initial step in data […]
The post Decoding Data: An Introduction to Descriptive Statistics with the Ames Housing Dataset appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supercharge Your AI Journey! Join Uplimit’s Free Building AI Products using OpenAI Course]]></title>
        <id>https://www.kdnuggets.com/?p=163553</id>
        <link href="https://www.kdnuggets.com/2024/01/uplimit-supercharge-your-ai-journey-openai-course?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=supercharge-your-ai-journey-join-uplimits-free-building-ai-products-using-openai-course"/>
        <updated>2024-01-25T01:48:48.000Z</updated>
        <summary type="html"><![CDATA[Transform your AI aspirations into reality—join Uplimit's AI revolution!]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D rotations and spatial transformations made easy with RoMa]]></title>
        <id>https://medium.com/p/356a495a20c4</id>
        <link href="https://medium.com/pytorch/3d-rotations-and-spatial-transformations-made-easy-with-roma-356a495a20c4?source=rss----512b8efdf2e7---4"/>
        <updated>2024-01-25T00:02:54.000Z</updated>
        <summary type="html"><![CDATA[Struggling with quaternions, rotation vectors, right-hand rules and all these stuffs? Try RoMa: an easy-to-to-use, stable and efficient…]]></summary>
        <author>
            <name>Romain Brégier</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Generative AI Can Help You Improve Your Data Visualization Charts]]></title>
        <id>https://www.kdnuggets.com/?p=163457</id>
        <link href="https://www.kdnuggets.com/how-generative-ai-can-help-you-improve-your-data-visualization-charts?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-generative-ai-can-help-you-improve-your-data-visualization-charts"/>
        <updated>2024-01-24T15:00:50.000Z</updated>
        <summary type="html"><![CDATA[Using Generative AI to speed up and enhance data visualization.]]></summary>
        <author>
            <name>Angelica Lo Duca</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resources for teaching and learning survey sampling, from Scott Keeter at Pew Research]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49121</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/24/resources-for-teaching-and-learning-survey-sampling-from-scott-keeter-at-pew-research/"/>
        <updated>2024-01-24T14:46:47.000Z</updated>
        <summary type="html"><![CDATA[Art Owen informed me that he’ll be teaching sampling again at Stanford, and he was wondering about ideas for students gathering their own data. I replied that I like the idea of sampling from databases, biological sampling, etc. You can … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[8 Free Google Courses to Land Top Paying Jobs]]></title>
        <id>https://www.kdnuggets.com/?p=163358</id>
        <link href="https://www.kdnuggets.com/8-free-google-courses-to-land-top-paying-jobs?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=8-free-google-courses-to-land-top-paying-jobs"/>
        <updated>2024-01-24T13:00:41.000Z</updated>
        <summary type="html"><![CDATA[Learning a new skill just got better with these 10 free courses for lucrative careers.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DsDm: Model-Aware Dataset Selection with Datamodels]]></title>
        <id>https://gradientscience.org/dsdm/</id>
        <link href="https://gradientscience.org/dsdm/"/>
        <updated>2024-01-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Code



   Paper


tl;dr: When training large-scale models, standard practice is to select training data that is intuitively useful. However, it turns out that such data can actually hurt model performance. We instead design a framework that selects by modeling how models learn from data—and thereby greatly improve performance.
Suppose we want to train a large-scale ML model, like a language model or a diffusion model. How do we choose which data to train on? Standard methods tend to select data using human notions of data quality. For example, the GPT-3 training procedure selects training data that matches intuitively “high quality” data sources like Wikipedia. Filtering like this yields (qualitatively) clean data that feels like it should improve model performance. But does it actually i…]]></summary>
        <author>
            <name>gradient science</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progress in 2023, Leo edition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50027</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/23/progress-in-2023-leo-edition/"/>
        <updated>2024-01-23T23:00:39.000Z</updated>
        <summary type="html"><![CDATA[Following Andrew, Aki, Jessica, and Charles, and based on Andrew’s proposal, I list my research contributions for 2023. Published: Egidi, L. (2023). Seconder of the vote of thanks to Narayanan, Kosmidis, and Dellaportas and contribution to the Discussion of ‘Flexible … Continue reading →]]></summary>
        <author>
            <name>Leonardo Egidi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exphormer: Scaling transformers for graph-structured data]]></title>
        <id>http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html</id>
        <link href="http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html"/>
        <updated>2024-01-23T22:27:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google




Graphs, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. 




A common approach to learning on graphs are graph neural networks (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a message-passing framework,…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Here’s how to subscribe to our new weekly newsletter:]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50058</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/23/heres-how-to-get-our-new-weekly-newsletter/"/>
        <updated>2024-01-23T21:40:59.000Z</updated>
        <summary type="html"><![CDATA[Just a reminder: we have a new weekly newsletter. We posted on it a couple weeks ago; I’m just giving a reminder here because the goal of the newsletter is to reach people who wouldn’t otherwise go online to read … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KDnuggets News, January 24: 5 Free University Courses to Learn Data Science • Convert Unstructured Data into Structured Insights with LLMs]]></title>
        <id>https://www.kdnuggets.com/?p=163544</id>
        <link href="https://www.kdnuggets.com/newsletter-n03-2024-01-24?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=kdnuggets-news-january-24-5-free-university-courses-to-learn-data-science-convert-unstructured-data-into-structured-insights-with-llms"/>
        <updated>2024-01-23T19:26:45.000Z</updated>
        <summary type="html"><![CDATA[This week on KDnuggets: Here are five free university courses to help you get started in a data science career • Understand the unstructured data dilemma • And much, much more!]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powering Up with Predictive GenAI]]></title>
        <id>https://www.kdnuggets.com/?p=163532</id>
        <link href="https://www.kdnuggets.com/2024/01/pecan-powering-predictive-genai?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=powering-up-with-predictive-genai"/>
        <updated>2024-01-23T18:00:13.000Z</updated>
        <summary type="html"><![CDATA[Learn what Predictive GenAI does and how it can make predictive analytics far more accessible, efficient, and meaningful for your business.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[7 Steps to Landing Your First Data Science Job]]></title>
        <id>https://www.kdnuggets.com/?p=163418</id>
        <link href="https://www.kdnuggets.com/7-steps-to-landing-your-first-data-science-job?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=7-steps-to-landing-your-first-data-science-job"/>
        <updated>2024-01-23T17:00:30.000Z</updated>
        <summary type="html"><![CDATA[Want to make a successful career switch to data science? From learning data science concepts to cracking interviews, read this guide to move one step closer to your first data science job.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 16 Technical Data Sources for Advanced Data Science Projects]]></title>
        <id>https://www.kdnuggets.com/?p=163409</id>
        <link href="https://www.kdnuggets.com/top-16-technical-data-sources-for-advanced-data-science-projects?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=top-16-technical-data-sources-for-advanced-data-science-projects"/>
        <updated>2024-01-23T15:00:58.000Z</updated>
        <summary type="html"><![CDATA[Here are data repositories that will up your data science game and improve your data projects.]]></summary>
        <author>
            <name>Nate Rosidi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from mistakes (my online talk for the American Statistical Association, 2:30pm Tues 30 Jan 2024)]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50020</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/23/learning-from-mistakes-my-online-talk-for-the-american-statistical-association-230pm-tues-30-jan-2024/"/>
        <updated>2024-01-23T14:25:24.000Z</updated>
        <summary type="html"><![CDATA[Here’s the link: Learning from mistakes Andrew Gelman, Department of Statistics and Department of Political Science, Columbia University We learn so much from mistakes! How can we structure our workflow so that we can learn from mistakes more effectively? I … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Prompt Engineers are Making $300k/y]]></title>
        <id>https://www.kdnuggets.com/?p=163349</id>
        <link href="https://www.kdnuggets.com/ai-prompt-engineers-are-making-300ky?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ai-prompt-engineers-are-making-300k-y"/>
        <updated>2024-01-23T13:00:24.000Z</updated>
        <summary type="html"><![CDATA[Prompt engineering and generative AI are becoming hotter by the day. Be part of the heat!]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Email subscription changes]]></title>
        <id>https://www.johndcook.com/blog/?p=238592</id>
        <link href="https://www.johndcook.com/blog/2024/01/22/email-subscription-changes/"/>
        <updated>2024-01-23T02:20:14.000Z</updated>
        <summary type="html"><![CDATA[I will soon be discontinuing the email subscription option for this blog. I recommend that email subscribers switch over to subscribing to the RSS feed for the blog. If you’re unfamiliar with RSS, here is an article on how to get started. (I recommend RSS in general, and not just for subscribing to this blog. […]
Email subscription changes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[spacy-llm: From quick prototyping with LLMs to more reliable and efficient NLP solutions]]></title>
        <id>event:az-2024</id>
        <link href="https://speakerdeck.com/sofievl/2024-01-23-az"/>
        <updated>2024-01-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[LLMs are paving the way for fast prototyping of NLP applications. Here, Sofie showcases how to build a structured NLP pipeline to mine clinical trials, using spaCy and spacy-llm. Moving beyond a fast prototype, she offers pragmatic solutions to make the pipeline more reliable and cost efficient.]]></summary>
        <author>
            <name>Sofie Van Landeghem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intro to BridgeStan: The new in-memory interface for Stan]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50056</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/22/intro-to-bridgestan-the-new-in-memory-interface-for-stan/"/>
        <updated>2024-01-22T20:07:52.000Z</updated>
        <summary type="html"><![CDATA[This is Eric. Brian Parbhu took over the Bayesian Data Analysis meetup (formerly NYC Stan Users Group), and he is running an event in NYC this Friday, during which Brian Ward is going to discuss BridgeStan, the new in-memory interface … Continue reading →]]></summary>
        <author>
            <name>Eric Novik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3 Crucial Challenges in Conversational AI Development and How to Avoid Them]]></title>
        <id>https://www.kdnuggets.com/?p=163398</id>
        <link href="https://www.kdnuggets.com/3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them"/>
        <updated>2024-01-22T17:00:44.000Z</updated>
        <summary type="html"><![CDATA[Developing a conversational AI chatbot requires substantial effort. However, understanding and addressing key challenges in natural language understanding can streamline the development process.]]></summary>
        <author>
            <name>Suman Saurav</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3 Interesting Uses of Python’s Context Managers]]></title>
        <id>https://www.kdnuggets.com/?p=163388</id>
        <link href="https://www.kdnuggets.com/3-interesting-uses-of-python-context-managers?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=3-interesting-uses-of-pythons-context-managers"/>
        <updated>2024-01-22T15:00:07.000Z</updated>
        <summary type="html"><![CDATA[Context managers in Python help you handle resources efficiently. Let's learn how you can use them to manage database connections, subprocesses, and more.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“My view is that if I can show that a result was cooked and that doing it correctly does not yield the answer the authors claimed, then the result is discredited. . . . What I hear, instead, is the following . . .”]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49596</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/22/my-view-is-that-if-i-can-show-that-a-result-was-cooked-and-that-doing-it-correctly-does-not-yield-the-answer-the-authors-claimed-then-the-result-is-discredited-what-i-hear-instead-is-the-f/"/>
        <updated>2024-01-22T14:16:09.000Z</updated>
        <summary type="html"><![CDATA[Economic historian Tim Guinnane writes: I have a general question that I have not seen addressed on your blog. Often this question turns into a narrow question about retracting papers, but I think that short-circuits an important discussion. Like many … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Getting Started with Diffusers for Text-to-Image]]></title>
        <id>https://pyimagesearch.com/?p=42750</id>
        <link href="https://pyimagesearch.com/2024/01/22/getting-started-with-diffusers-for-text-to-image/"/>
        <updated>2024-01-22T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents Getting Started with Diffusers for Text-to-Image Introduction A Brief Primer on Diffusion Configuring Your Development Environment Need Help Configuring Your Development Environment? Setup and Imports Diffusers But What Is AutoPipeline? What Are Some Other Pipelines and Models?…
The post Getting Started with Diffusers for Text-to-Image appeared first on PyImageSearch.]]></summary>
        <author>
            <name>Aritra Roy Gosthipaty and Ritwik Raha</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language Model]]></title>
        <id>https://www.kdnuggets.com/?p=163368</id>
        <link href="https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model"/>
        <updated>2024-01-22T13:00:22.000Z</updated>
        <summary type="html"><![CDATA[Zephyr is a series of Large Language Models released by Hugging Face trained using distilled supervised fine-tuning (dSFT) on larger models with significantly improved task accuracy.]]></summary>
        <author>
            <name>Ahmad Anis</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revealing the Invisible: Visualizing Missing Values in Ames Housing]]></title>
        <id>https://machinelearningmastery.com/?p=15381</id>
        <link href="https://machinelearningmastery.com/revealing_the_invisible/"/>
        <updated>2024-01-22T11:22:26.000Z</updated>
        <summary type="html"><![CDATA[The digital age has ushered in an era where data-driven decision-making is pivotal in various domains, real estate being a prime example. Comprehensive datasets, like the one concerning properties in Ames, offer a treasure trove for data enthusiasts. Through meticulous exploration and analysis of such datasets, one can uncover patterns, gain insights, and make informed […]
The post Revealing the Invisible: Visualizing Missing Values in Ames Housing appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Vinod Chugani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Michael Wiebe has several new replications written up on his site.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50047</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/21/michael-wiebe-has-several-new-replications-written-up-on-his-site/"/>
        <updated>2024-01-22T02:02:08.000Z</updated>
        <summary type="html"><![CDATA[Michael Wiebe writes: I have several new replications written up on my site. Moretti (2021) studies whether larger cities drive more innovation, but I find that the event study and instrumental variable results are due to coding errors. This means … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbalanced Low-Rank Optimal Transport Solvers]]></title>
        <id>transport-solvers</id>
        <link href="https://machinelearning.apple.com/research/transport-solvers"/>
        <updated>2024-01-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two salient limitations have long hindered the relevance of optimal transport methods to machine learning. First, the  computational cost of standard sample-based solvers (when used on batches of  samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers. A flurry of recent works has addressed these computational and modeling limitations. Still it has resulted in two separate strains of methods: While the computational outlook was…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Model Learning for Cardiovascular Biomarkers Inference]]></title>
        <id>hybrid-model-learning</id>
        <link href="https://machinelearning.apple.com/research/hybrid-model-learning"/>
        <updated>2024-01-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This paper was accepted at the workshop Deep Generative Models for Health at NeurIPS 2023.
Cardiovascular diseases (CVDs) are a major global health concern, making the longitudinal monitoring of cardiovascular biomarkers vital for early diagnosis and intervention. A core challenge is the inference of cardiac pulse parameters from pulse waves, especially when acquired from wearable sensors at peripheral body locations. Traditional machine learning (ML) approaches face hurdles in this context due to the scarcity of labeled data, primarily sourced from clinical settings. Simultaneously, physical…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Wide Feedforward is All You Need]]></title>
        <id>one-wide-ffn</id>
        <link href="https://machinelearning.apple.com/research/one-wide-ffn"/>
        <updated>2024-01-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This paper was accepted at WMT conference at EMNLP.
The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work, we explore the role of FFN and find that despite, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline]]></title>
        <id>faster-nerf</id>
        <link href="https://machinelearning.apple.com/research/faster-nerf"/>
        <updated>2024-01-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, and/or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple NeRF+SR pipeline that directly combines existing modules, and we propose a lightweight augmentation…]]></summary>
        <author>
            <name>Apple Machine Learning Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User-friendly Introduction to PAC-Bayes Bounds]]></title>
        <id>http://www.nowpublishers.com/article/Details/MAL-100</id>
        <link href="http://www.nowpublishers.com/article/Details/MAL-100"/>
        <updated>2024-01-21T23:00:00.000Z</updated>
        <summary type="html"><![CDATA[Abstract
Aggregated predictors are obtained by making a set of basic
predictors vote according to some weights, that is, to some
probability distribution. Randomized predictors are obtained
by sampling in a set of basic predictors, according to some
prescribed probability distribution.
Thus, aggregated and randomized predictors have in common
that their definition rely on a probability distribution on
the set of predictors. In statistical learning theory, there is a
set of tools designed to understand the generalization ability
of such predictors: PAC-Bayesian or PAC-Bayes bounds.
Since the original PAC-Bayes bounds (Shawe-Taylor and
Williamson, 1997; McAllester, 1998), these tools have been
considerably improved in many directions. We will for example
describe a simplified version of the localization technique
(Catoni, 2003; Catoni, 2007) that was missed by the
community, and later rediscovered as “mutual information
bounds”. Very recently, PAC-Bayes bounds received a considerable
attention. There was workshop on PAC-Bayes at
NIPS 2017, (Almost) 50 Shades of Bayesian Learning: PACBayesian
trends and insights, organized by B. Guedj, F. Bach and P. Germain. 
One of the reasons of this recent interest
is the successful application of these bounds to neural
networks (Dziugaite and Roy, 2017). Since then, this is a
recurring topic of workshops in the major machine learning
conferences.
The objective of these notes is to provide an elementary
introduction to PAC-Bayes bounds.
Suggested Citation
Pierre Alquier (2024), "User-friendly Introduction to PAC-Bayes Bounds", Foundations and Trends® in Machine Learning: Vol. 17: No. 2, pp 174-303. http://dx.doi.org/10.1561/2200000100]]></summary>
        <author>
            <name>Articles for MAL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Machine Learning Engineer 🤖 #266 -     Forecasting & Causal Inference, OSS Value,
    TextToSpeech Whisper, ISO Global Standards. Meta Infra, MLOps Events, ML Frameworks + more 🚀]]></title>
        <id>https://kill-the-newsletter.com/alternates/t22ditc6m1ksloqq.html</id>
        <link href="https://kill-the-newsletter.com/alternates/t22ditc6m1ksloqq.html"/>
        <updated>2024-01-21T17:14:20.000Z</updated>
        <summary type="html"><![CDATA[The Machine Learning Engineer 🤖 #266#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…]]></summary>
        <author>
            <name>the ml engineer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What’s the problem, “math snobs” or rich dudes who take themselves too seriously and are enabled in that by the news media?]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49597</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/21/whats-the-problem-math-snobs-or-rich-dudes-who-take-themselves-too-seriously-and-are-enabled-in-that-by-the-news-media/"/>
        <updated>2024-01-21T14:00:46.000Z</updated>
        <summary type="html"><![CDATA[Chris Barker, the chair of the Statistical Consulting Section of the American Statistical Association, writes: I’m curious about your reaction/opinion to a Financial times article I read today about Sam Bankman-Fried (“SBF,” charged with fraud in the loss several billion … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regarding the use of “common sense” when evaluating research claims]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49235</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/20/regarding-the-use-of-common-sense-when-evaluating-research-claims/"/>
        <updated>2024-01-20T14:38:22.000Z</updated>
        <summary type="html"><![CDATA[I’ve often appealed to “common sense” or “face validity” when considering unusual research claims. For example, the statement that single women during certain times of the month were 20 percentage points more likely to support Barack Obama, or the claim … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beta inequality symmetries]]></title>
        <id>https://www.johndcook.com/blog/?p=237632</id>
        <link href="https://www.johndcook.com/blog/2024/01/20/beta-inequality-symmetries/"/>
        <updated>2024-01-20T13:45:08.000Z</updated>
        <summary type="html"><![CDATA[I was thinking about the work I did when I worked in biostatistics at MD Anderson. This work was practical rather than mathematically elegant, useful in its time but not of long-term interest. However, one result came out of this work that I would call elegant, and that was a symmetry I found. Let X […]
Beta inequality symmetries first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning in OpenCV (7-Day Mini-Course)]]></title>
        <id>https://machinelearningmastery.com/?p=16186</id>
        <link href="https://machinelearningmastery.com/machine-learning-in-opencv-7-day-mini-course/"/>
        <updated>2024-01-20T01:33:33.000Z</updated>
        <summary type="html"><![CDATA[Machine learning is an amazing tool for many tasks. OpenCV is a great library for manipulating images. It would be great if we can put them together. In this 7-part crash course, you will learn from examples how to make use of machine learning and the image processing API from OpenCV to accomplish some goals. […]
The post Machine Learning in OpenCV (7-Day Mini-Course) appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Adrian Tam</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progress in 2023, Charles edition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50030</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/19/progress-in-2023-charles-edition/"/>
        <updated>2024-01-19T20:00:15.000Z</updated>
        <summary type="html"><![CDATA[Following the examples of Andrew, Aki, and Jessica, and at Andrew’s request: Published: Variational Inference with Gaussian Score Matching. Neural Information Processing Systems. (Chirag Modi, CM, Yuling Yao, Robert Gower, David Blei and Lawrence Saul) The Shrinkage-Delinkage Trade-off: An Analysis … Continue reading →]]></summary>
        <author>
            <name>Charles Margossian</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enroll in a Data Science Undergraduate Program For Free]]></title>
        <id>https://www.kdnuggets.com/?p=163315</id>
        <link href="https://www.kdnuggets.com/enroll-in-a-data-science-undergraduate-program-for-free?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=enroll-in-a-data-science-undergraduate-program-for-free"/>
        <updated>2024-01-19T15:00:05.000Z</updated>
        <summary type="html"><![CDATA[Path to a Free Self-Taught Education in Data Science for Everyone.]]></summary>
        <author>
            <name>Abid Ali Awan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The free will to repost]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49593</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/19/the-free-will-to-repost/"/>
        <updated>2024-01-19T14:07:21.000Z</updated>
        <summary type="html"><![CDATA[Jonathan “no Trump” Falk points to this press release and writes: Scientist, after decades of study, concludes: We don’t have free will. Does that include the decision to write a book about free will? PS … A quick mention of … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Super Helpful SQL Cheat Sheets You Can’t Miss!]]></title>
        <id>https://www.kdnuggets.com/?p=163241</id>
        <link href="https://www.kdnuggets.com/5-super-helpful-sql-cheat-sheets-you-cant-miss?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-super-helpful-sql-cheat-sheets-you-cant-miss"/>
        <updated>2024-01-19T13:00:54.000Z</updated>
        <summary type="html"><![CDATA[Want to refresh your SQL skills? Bookmark these useful cheat sheets covering SQL basics, joins, window functions, and more.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🦙 spacy-llm v0.7.0]]></title>
        <id>release:spacy-llm_0.7.0</id>
        <link href="https://github.com/explosion/spacy-llm"/>
        <updated>2024-01-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Supporting arbitrarily long docs and various new tasks]]></summary>
        <author>
            <name>Explosion</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New decade, new tagline: Data & AI for Impact]]></title>
        <id>https://yanirseroussi.com/2024/01/19/new-decade-new-tagline-data-and-ai-for-impact/</id>
        <link href="https://yanirseroussi.com/2024/01/19/new-decade-new-tagline-data-and-ai-for-impact/"/>
        <updated>2024-01-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Shifting focus to ‘Data & AI for Impact’, with more startup-related content, increased posting frequency, and deeper audience engagement.]]></summary>
        <author>
            <name>Yanir Seroussi | Data &amp; AI for Impact</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Postdoc at Washington State University on law-enforcement statistics]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50024</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/18/postdoc-at-washington-state-university-on-law-enforcement-statistics/"/>
        <updated>2024-01-18T22:49:21.000Z</updated>
        <summary type="html"><![CDATA[This looks potentially important: The Center for Interdisciplinary Statistical Education and Research (CISER) at Washington State University (WSU) is excited to announce that it has an opening for a Post-Doctoral Research Associate (statistical scientist) supporting a new state-wide public data … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing ASPIRE for selective prediction in LLMs]]></title>
        <id>http://blog.research.google/2024/01/introducing-aspire-for-selective.html</id>
        <link href="http://blog.research.google/2024/01/introducing-aspire-for-selective.html"/>
        <updated>2024-01-18T18:03:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team





In the fast-evolving landscape of artificial intelligence, large language models (LLMs) have revolutionized the way we interact with machines, pushing the boundaries of natural language understanding and generation to unprecedented heights. Yet, the leap into high-stakes decision-making applications remains a chasm too wide, primarily due to the inherent uncertainty of model predictions. Traditional LLMs generate responses recursively, yet they lack an intrinsic mechanism to assign a confidence score to these responses. Although one can derive a confidence score by summing up the probabilities of individual tokens in the sequence, traditional approaches typically fall short in reliably dist…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Ways of Converting Unstructured Data into Structured Insights with LLMs]]></title>
        <id>https://www.kdnuggets.com/?p=163226</id>
        <link href="https://www.kdnuggets.com/5-ways-of-converting-unstructured-data-into-structured-insights-with-llms?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-ways-of-converting-unstructured-data-into-structured-insights-with-llms"/>
        <updated>2024-01-18T15:00:09.000Z</updated>
        <summary type="html"><![CDATA[From Chaos to Clarity: Understanding the Unstructured Data Dilemma.]]></summary>
        <author>
            <name>Josep Ferrer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Storytelling and Scientific Understanding (my talks with Thomas Basbøll at Johns Hopkins on 26 Apr)]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49977</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/18/storytelling-and-scientific-understanding-my-talks-with-thomas-basboll-at-johns-hopkins-on-26-apr/"/>
        <updated>2024-01-18T14:28:49.000Z</updated>
        <summary type="html"><![CDATA[Storytelling and Scientific Understanding Andrew Gelman and Thomas Basbøll Storytelling is central to science, not just as a tool for broadcasting scientific findings to the outside world, but also as a way that we as scientists understand and evaluate theories. … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When is a function of two variables separable?]]></title>
        <id>https://www.johndcook.com/blog/?p=236814</id>
        <link href="https://www.johndcook.com/blog/2024/01/18/separable-function/"/>
        <updated>2024-01-18T13:15:39.000Z</updated>
        <summary type="html"><![CDATA[Given a function f(x, y), how can you tell whether f can be factored into the product of a function g(x) of x alone and a function h(y) of y alone? Depending on how an expression for f is written, it may or may not be obvious whether f(x, y) can be separated into g(x) h(y). There […]
When is a function of two variables separable? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Free University Courses to Learn Data Science]]></title>
        <id>https://www.kdnuggets.com/?p=163263</id>
        <link href="https://www.kdnuggets.com/5-free-university-courses-to-learn-data-science?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-university-courses-to-learn-data-science"/>
        <updated>2024-01-18T13:00:50.000Z</updated>
        <summary type="html"><![CDATA[Looking to make a career in data science? Here are five free university courses to help you get started.]]></summary>
        <author>
            <name>Bala Priya C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progress in 2023, Aki’s software edition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50018</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/18/progress-in-2023-akis-software-edition/"/>
        <updated>2024-01-18T11:05:56.000Z</updated>
        <summary type="html"><![CDATA[Andrew, I, and Jessica (and I hope we get more) listed papers for progress in 2023, but many papers would be much less useful without software, so I list also software I’m contributing to with the most interesting improvements added … Continue reading →]]></summary>
        <author>
            <name>Aki Vehtari</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of Bernoulli differential equations]]></title>
        <id>https://www.johndcook.com/blog/?p=236796</id>
        <link href="https://www.johndcook.com/blog/2024/01/17/bernoulli-ode/"/>
        <updated>2024-01-18T02:45:24.000Z</updated>
        <summary type="html"><![CDATA[When a nonlinear first order ordinary differential equation has the form with n ≠ 1, the change of variables turns the equation into a linear equation in u. The equation is known as Bernoulli’s equation, though Leibniz came up with the same technique. Apparently the history is complicated [1]. It’s nice that Bernoulli’s equation can […]
Applications of Bernoulli differential equations first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[6 Reasons Why a Universal Semantic Layer is Beneficial to Your Data Stack]]></title>
        <id>https://www.kdnuggets.com/?p=163338</id>
        <link href="https://www.kdnuggets.com/2024/01/cube-6-reasons-why-a-universal-semantic-layer-is-beneficial?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=6-reasons-why-a-universal-semantic-layer-is-beneficial-to-your-data-stack"/>
        <updated>2024-01-17T18:00:36.000Z</updated>
        <summary type="html"><![CDATA[Looking to understand the universal semantic layer and how it can improve your data stack? This GigaOm Sonor report on Semantic Layers can help you delve deeper.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Semantic Vector Search Transforms Customer Support Interactions]]></title>
        <id>https://www.kdnuggets.com/?p=163270</id>
        <link href="https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-semantic-vector-search-transforms-customer-support-interactions"/>
        <updated>2024-01-17T15:00:59.000Z</updated>
        <summary type="html"><![CDATA[Semantic vector search is an advanced search technique revolutionizes how we interact with information by understanding the true meaning of words, thus leading to more relevant and insightful results.]]></summary>
        <author>
            <name>Taranjeet Singh</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad stuff going down at the American Sociological Association]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49464</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/17/bad-stuff-going-down-at-the-american-sociological-association/"/>
        <updated>2024-01-17T14:01:30.000Z</updated>
        <summary type="html"><![CDATA[I knew the Association for Psychological Science, the American Psychological Association, the American Political Science Association, the American Statistical Association, and the National Academy of Sciences had problems. It turns out the American Sociological Association does some bad things too. … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 FREE Courses on AI with Microsoft for 2024]]></title>
        <id>https://www.kdnuggets.com/?p=163219</id>
        <link href="https://www.kdnuggets.com/5-free-courses-on-ai-with-microsoft-for-2024?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=5-free-courses-on-ai-with-microsoft-for-2024"/>
        <updated>2024-01-17T13:00:33.000Z</updated>
        <summary type="html"><![CDATA[Kickstart your AI journey this new year with 5 FREE learning resources from Microsoft.]]></summary>
        <author>
            <name>Nisha Arya</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KDnuggets News, January 17: 4 Steps to Become a Generative AI Developer • Pandas vs. Polars: A Comparative Analysis]]></title>
        <id>https://www.kdnuggets.com/?p=163329</id>
        <link href="https://www.kdnuggets.com/newsletter-n02-2024-01-17?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=kdnuggets-news-january-17-4-steps-to-become-a-generative-ai-developer-pandas-vs-polars-a-comparative-analysis"/>
        <updated>2024-01-17T11:00:51.000Z</updated>
        <summary type="html"><![CDATA[This week on KDnuggets: We cover what a generative AI developer does, what tools you need to master, and how to get started • An in-depth analysis of Python DataFrame library syntax, speed, and usability... which one is best? • And much, much more!]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beware of misleading GPU vs CPU benchmarks]]></title>
        <id>https://pythonspeed.com/articles/gpu-vs-cpu/</id>
        <link href="https://pythonspeed.com/articles/gpu-vs-cpu/"/>
        <updated>2024-01-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Do you use NumPy, Pandas, or scikit-learn and want to get faster results?
Nvidia has created GPU-based replacements for each of these with the shared promise of extra speed.
For example, if you visit the front page of NVidia’s RAPIDS project, you’ll see benchmarks showing cuDF, a GPU-based Pandas replacement, is 15× to 80× faster than Pandas!
Unfortunately, while those speed-ups are impressive, they are also misleading.
GPU-based libraries might be the answer to your performance problems… or they might be an an unnecessary and expensive distraction.
Read more...]]></summary>
        <author>
            <name>Python⇒Speed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progress in 2023, Jessica Edition]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=50015</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/16/progress-in-2023-jessica-edition/"/>
        <updated>2024-01-16T19:10:39.000Z</updated>
        <summary type="html"><![CDATA[Since Aki and Andrew are doing it…  Published: Dongping Zhang, Jason Hartline, and Jessica Hullman (2024). Designing Shared Information Displays for Agents of Varying Strategic Sophistication. ACM Transactions on Computer-Supported Cooperative Work (CSCW). Yifan Wu, Ziyang Guo, Michalis Mamakos, Jason … Continue reading →]]></summary>
        <author>
            <name>Jessica Hullman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discover the World of Computer Vision: Introducing MLM’s Latest OpenCV Ebook]]></title>
        <id>https://www.kdnuggets.com/?p=163306</id>
        <link href="https://www.kdnuggets.com/2024/01/mlm-discover-the-world-of-computer-vision-ebook?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=discover-the-world-of-computer-vision-introducing-mlms-latest-opencv-ebook"/>
        <updated>2024-01-16T18:00:15.000Z</updated>
        <summary type="html"><![CDATA[Today, we're proud to announce a significant addition to our catalog at Machine Learning Mastery. Known for our detailed, code-centric guides, we're taking a leap further into the realms of Computer Vision with our latest offering.]]></summary>
        <author>
            <name>KDnuggets</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The IQ Test That AI Can’t Pass]]></title>
        <id>https://www.johndcook.com/blog/?p=233129</id>
        <link href="https://www.johndcook.com/blog/2024/01/16/the-iq-test-ai-cant-pass/"/>
        <updated>2024-01-16T17:59:42.000Z</updated>
        <summary type="html"><![CDATA[Large language models have recently achieved remarkable test scores on well-known academic and professional exams (see, e.g., [1], p. 6). On such tests, these models are at times said to reach human-level performance. However, there is one test that humans can pass but every AI method known to have been tried has abysmally failed. The […]
The IQ Test That AI Can’t Pass first appeared on John D. Cook.]]></summary>
        <author>
            <name>Wayne Joubert</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This post is not really about Aristotle.]]></title>
        <id>https://statmodeling.stat.columbia.edu/?p=49577</id>
        <link href="https://statmodeling.stat.columbia.edu/2024/01/16/this-post-is-not-really-about-aristotle/"/>
        <updated>2024-01-16T14:56:20.000Z</updated>
        <summary type="html"><![CDATA[I just read this magazine article by Nikhil Krishnan on the philosophy of Aristotle. As a former physics student, I’ve never had anything but disdain for that ancient philosopher, who’s famous for getting just about everything in physics wrong, as … Continue reading →]]></summary>
        <author>
            <name>Andrew</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Twitter account for cryptography]]></title>
        <id>https://www.johndcook.com/blog/?p=236099</id>
        <link href="https://www.johndcook.com/blog/2024/01/16/new-twitter-account-for-cryptography/"/>
        <updated>2024-01-16T13:39:30.000Z</updated>
        <summary type="html"><![CDATA[I’ve started a new Twitter account: @CryptographyTip. The icon for the account is the symbol for XOR, a common operation in encryption. I intend to post about cryptography theory as well as practical matters such as software and file formats. You can find a list of my other technical twitter accounts here. You can also […]
New Twitter account for cryptography first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Means of means bounding the logarithmic mean]]></title>
        <id>https://www.johndcook.com/blog/?p=236043</id>
        <link href="https://www.johndcook.com/blog/2024/01/16/bounding-logarithmic-mean/"/>
        <updated>2024-01-16T11:29:35.000Z</updated>
        <summary type="html"><![CDATA[The geometric, logarithmic, and arithmetic means of a and b are defined as follows. A few days ago I mentioned that G ≤ L ≤ A. The logarithmic mean slips between the geometric and arithmetic means. Or to put it another way, the logarithmic mean is bounded by the geometric and arithmetic means. You can […]
Means of means bounding the logarithmic mean first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Raspberry Pi Pico’s Memory Layout]]></title>
        <id>http://petewarden.com/?p=7906</id>
        <link href="https://petewarden.com/2024/01/16/understanding-the-raspberry-pi-picos-memory-layout/"/>
        <updated>2024-01-16T03:02:47.000Z</updated>
        <summary type="html"><![CDATA[A few months ago I started updating TensorFlow Lite Micro for the Raspberry Pi Pico board, which uses the RP2040 microcontroller. I ran into some baffling bugs that stopped me making progress, but eventually I tracked them down to my poor understanding of the memory layout. Since I had to do a deep dive, I […]]]></summary>
        <author>
            <name>Pete Warden</name>
        </author>
    </entry>
</feed>