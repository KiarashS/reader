{
  "sources": [
    {
      "title": "Release notes from osmosfeed",
      "feedUrl": "https://github.com/osmoscraft/osmosfeed/releases.atom",
      "siteUrl": "https://github.com/osmoscraft/osmosfeed/releases",
      "articles": []
    },
    {
      "title": "Towards Data Science - Medium",
      "feedUrl": "https://towardsdatascience.com/feed",
      "siteUrl": "https://towardsdatascience.com?source=rss----7f60cf5620c9---4",
      "articles": [
        {
          "id": "https://medium.com/p/3001f54c48e9",
          "author": "Yoann Mocquin",
          "description": "Always keep a dummy by your side.\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/the-dummy-models-of-scikit-learn-3001f54c48e9?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T09:06:17.000Z",
          "wordCount": null,
          "title": "The Dummy models of Scikit-learn",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/a54701d1b621",
          "author": "Andrew Skabar, PhD",
          "description": "",
          "link": "https://towardsdatascience.com/evaluating-synthetic-data-the-million-dollar-question-a54701d1b621?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T06:12:20.000Z",
          "wordCount": null,
          "title": "Evaluating Synthetic Data — The Million Dollar Question",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/360dce356df5",
          "author": "Mikhail Sarafanov",
          "description": "",
          "link": "https://towardsdatascience.com/stream-ordering-how-and-why-a-geo-scientist-sometimes-needed-to-rank-rivers-on-a-map-360dce356df5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T06:09:20.000Z",
          "wordCount": null,
          "title": "Stream Ordering: How And Why a Geo-scientist Sometimes Needed to Rank Rivers on a Map",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/41425a40d7d0",
          "author": "Vincent Koc",
          "description": "Architecture patterns and mental models for working with Large Language Models",
          "link": "https://towardsdatascience.com/generative-ai-design-patterns-a-comprehensive-guide-41425a40d7d0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T23:19:59.000Z",
          "wordCount": 3759,
          "title": "Generative AI Design Patterns: A Comprehensive Guide",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*2tBQkVvC-_sHACgSs0ouug.png"
        },
        {
          "id": "https://medium.com/p/ee11ab5f9f20",
          "author": "Sachin Date",
          "description": "The life and times of Abraham De Moivre, his famous theorem, and how it set the stage for the discovery of the Central Limit Theorem",
          "link": "https://towardsdatascience.com/abraham-de-moivre-his-famous-theorem-and-the-birth-of-the-normal-curve-ee11ab5f9f20?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T21:37:50.000Z",
          "wordCount": 8015,
          "title": "Abraham De Moivre, His Famous Theorem, and the Birth of the Normal Curve",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*SoJHemrSFz0wWPxZl3q4wg.jpeg"
        },
        {
          "id": "https://medium.com/p/cf3682f8563b",
          "author": "Robert A. Gonsalves",
          "description": "Transforming your ideas into tangible artifacts using Midjourney and open-source projects: Shap-E, MVDream, and threestudio\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/molding-the-imagination-using-ai-to-create-new-3d-printable-objects-cf3682f8563b?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T21:31:18.000Z",
          "wordCount": 1497,
          "title": "Molding the Imagination: Using AI to Create New 3D-Printable Objects",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*sao9nlpIadkA3dzQig2qHQ.jpeg"
        },
        {
          "id": "https://medium.com/p/e823535e0eb1",
          "author": "LucianoSphere (Luciano Abriata, PhD)",
          "description": "A careful selection looking for performance, flexibility, and richness of features.\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/the-most-advanced-libraries-for-data-visualization-and-analysis-on-the-web-e823535e0eb1?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T20:39:41.000Z",
          "wordCount": 1520,
          "title": "The Most Advanced Libraries for Data Visualization and Analysis on the Web",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*Cpgpk7Sl0hGofvcE7OEWMA.png"
        },
        {
          "id": "https://medium.com/p/cb1bd2f3f4bb",
          "author": "Yennie Jun",
          "description": "How well can AI models solve (and create) rebus puzzles?\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/measuring-ais-creativity-with-visual-word-puzzles-cb1bd2f3f4bb?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T20:10:28.000Z",
          "wordCount": null,
          "title": "Measuring AI’s Creativity with Visual Word Puzzles",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/0af4ea38f7b5",
          "author": "John Leung",
          "description": "Explore the potentials and constraints of LangChain for customer analytics, accompanied by practical implementation codes\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T19:34:16.000Z",
          "wordCount": null,
          "title": "Performing Customer Analytics with LangChain and LLMs",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/5c1db85984a1",
          "author": "Okan Bulut",
          "description": "",
          "link": "https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T17:50:27.000Z",
          "wordCount": null,
          "title": "Lexicon-Based Sentiment Analysis Using R",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/795582423666",
          "author": "Cristian Leo",
          "description": "",
          "link": "https://towardsdatascience.com/the-math-and-code-behind-k-means-clustering-795582423666?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T15:21:24.000Z",
          "wordCount": null,
          "title": "The Math and Code Behind K-Means Clustering",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/9d50b36bcbd3",
          "author": "Sofya Lipnitskaya",
          "description": "",
          "link": "https://towardsdatascience.com/bird-by-bird-using-finite-automata-9d50b36bcbd3?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T15:15:51.000Z",
          "wordCount": null,
          "title": "Finite Automata Simulation for Leveraging AI-Assisted Systems",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/9afdfaf2bd7c",
          "author": "Marco Peixeiro",
          "description": "Explore the architecture of Lag-Llama and learn to apply it in a forecasting project using Python\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/lag-llama-open-source-foundation-model-for-time-series-forecasting-9afdfaf2bd7c?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T14:52:49.000Z",
          "wordCount": null,
          "title": "Lag-Llama: Open-Source Foundation Model for Time Series Forecasting",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/e62531bca7a0",
          "author": "Rami Krispin",
          "description": "",
          "link": "https://towardsdatascience.com/setting-a-dockerized-python-environment-the-hard-way-e62531bca7a0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T14:45:42.000Z",
          "wordCount": null,
          "title": "Setting A Dockerized Python Environment — The Hard Way",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/afd97fce8fb5",
          "author": "Mariya Mansurova",
          "description": "",
          "link": "https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T08:03:13.000Z",
          "wordCount": null,
          "title": "Text Embeddings: Comprehensive Guide",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/71f714440923",
          "author": "Jarom Hulet",
          "description": "Understanding EMD through theory and from-scratch calculation\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/comparison-of-distributions-with-earth-movers-distance-71f714440923?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T07:55:44.000Z",
          "wordCount": null,
          "title": "Comparison of Distributions with Earth Mover’s Distance",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/465970a969e0",
          "author": "Ugur Yildirim",
          "description": "How to know the unknowable in observational studies",
          "link": "https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T00:27:50.000Z",
          "wordCount": 5372,
          "title": "Sensitivity Analysis for Unobserved Confounding",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*pw5JJ7_hGcYcTWNoyZsfzQ.png"
        },
        {
          "id": "https://medium.com/p/2902224aabf3",
          "author": "Stefano Bosisio",
          "description": "In this first article, we’re exploring Apache Beam, from a simple pipeline to a more complicated one, using GCP Dataflow. Let’s learn what…\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/apache-beam-data-processing-data-pipelines-dataflow-and-flex-templates-2902224aabf3?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:40:21.000Z",
          "wordCount": 1464,
          "title": "Apache Beam: Data Processing, Data Pipelines, Dataflow and Flex Templates",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*qDe_sNC7PPh3fU0KWs04Ug.jpeg"
        },
        {
          "id": "https://medium.com/p/a7dc47723788",
          "author": "Claudia Ng",
          "description": "Learn to build a Graph Convolutional Network that can handle heterogeneous graph data for link prediction\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/how-to-build-a-graph-based-neural-network-for-anomaly-detection-in-6-steps-a7dc47723788?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:22:38.000Z",
          "wordCount": 1414,
          "title": "How to Build a Graph-based Neural Network for Anomaly Detection in 6 Steps",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:640/1*Lb_4cCQckr6i7aDoJ2SN5w.png"
        },
        {
          "id": "https://medium.com/p/17f84378430b",
          "author": "Toon Beerten",
          "description": "A hands-on marketing use case\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/powerful-collaboration-of-ai-agents-with-crewai-17f84378430b?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:12:44.000Z",
          "wordCount": 1355,
          "title": "Powerful Collaboration of AI Agents with CrewAI",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*40kwxJ7oZ1kTAJsRt_2bow.png"
        },
        {
          "id": "https://medium.com/p/a4e876f9a532",
          "author": "Mohammed Mohammed",
          "description": "A constructive approach to measuring distribution differences.",
          "link": "https://towardsdatascience.com/understanding-kl-divergence-intuitively-a4e876f9a532?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:05:57.000Z",
          "wordCount": 3032,
          "title": "Understanding KL Divergence Intuitively",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*RlzqwDFtqqNYQjD2"
        },
        {
          "id": "https://medium.com/p/a57512cabd69",
          "author": "Mahyar Aboutalebi, Ph.D.",
          "description": "Learn how to work with Lexcube, a Python package for data visualization in the space-time domain!\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/3d-visualization-of-geospatial-big-data-by-lexcube-python-a57512cabd69?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T16:09:16.000Z",
          "wordCount": 1471,
          "title": "3D Visualization of Geospatial Big Data by Lexcube! (Python)",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*ThD3jbF6-ifOSyZVtzhdXg.png"
        },
        {
          "id": "https://medium.com/p/a5d7b15f3d1d",
          "author": "Jacky Kaub",
          "description": "Why distribution drifts can really hurt your models\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/the-biggest-weakness-of-boosting-trees-a5d7b15f3d1d?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T15:24:32.000Z",
          "wordCount": 1389,
          "title": "The Biggest Weakness Of Boosting Trees",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*bcxyWsq_OvXuZZkT"
        },
        {
          "id": "https://medium.com/p/ccb3a116f6eb",
          "author": "Conor O'Sullivan",
          "description": "Automation, machine learning and LLMs in the chip industry\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/7-lessons-from-an-ml-internship-at-intel-ccb3a116f6eb?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T14:37:22.000Z",
          "wordCount": 1403,
          "title": "7 Lessons from an ML Internship at Intel",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*D7KYKmCGk2NkPMdb6PFw0Q.png"
        },
        {
          "id": "https://medium.com/p/ea0f9ce61719",
          "author": "Christopher Tao",
          "description": "Get Holidays from Any Country, Any Year, Any date\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/python-could-know-your-holidays-no-matter-which-country-you-live-ea0f9ce61719?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T14:37:12.000Z",
          "wordCount": 1388,
          "title": "Python Could Know Your Holidays No Matter Which Country You Live",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*pk-kVwA2UxPMtRC_k1ED1Q.jpeg"
        },
        {
          "id": "https://medium.com/p/d3b3b28ba121",
          "author": "Marcin Kozak",
          "description": "The article shows simple examples of flat and nested recursion patterns in Python.\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/recursion-in-python-demystified-d3b3b28ba121?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T13:06:35.000Z",
          "wordCount": 1447,
          "title": "Recursion in Python Demystified",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*8EKWgHhKoScGLrZW"
        },
        {
          "id": "https://medium.com/p/769f43b46779",
          "author": "Márton Kardos",
          "description": "Understand Semantic Structures with Transformers and Topic Modeling",
          "link": "https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:55:45.000Z",
          "wordCount": 5294,
          "title": "Semantic Signal Separation",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*3_2zDivWp758eLvJPhp8Aw.png"
        },
        {
          "id": "https://medium.com/p/ee9b0b0ef082",
          "author": "Naomi Kriger",
          "description": "How to Create a Speech-to-Text-to-Speech Program",
          "link": "https://towardsdatascience.com/speech-to-text-to-speech-with-ai-using-python-a-how-to-guide-ee9b0b0ef082?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:47:58.000Z",
          "wordCount": 4225,
          "title": "Speech to Text to Speech with AI Using Python — a How-To Guide",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*Ai0UNZUQI8jW0nau"
        },
        {
          "id": "https://medium.com/p/6abe3e8b045c",
          "author": "Nate Cibik",
          "description": "Multilingual Learning with an Ollama-Python Walkie-Talkie",
          "link": "https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:17:09.000Z",
          "wordCount": 5607,
          "title": "LingoNaut Language Assistant",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1024/1*uxOckzPs6zbkuQZM80C7NA.jpeg"
        },
        {
          "id": "https://medium.com/p/e66b65ece369",
          "author": "Andy McDonald",
          "description": "Bringing Order to a Python Streamlit App Through an Organised Project Folder Structure\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/how-to-structure-and-organise-a-streamlit-app-e66b65ece369?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:04:44.000Z",
          "wordCount": 1504,
          "title": "How to Structure and Organise a Streamlit App",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*PWOSQYW83vqbv4c-"
        },
        {
          "id": "https://medium.com/p/a191965ac538",
          "author": "Mike Shakhomirov",
          "description": "Advanced techniques to process and load data efficiently\nContinue reading on Towards Data Science »",
          "link": "https://towardsdatascience.com/pandas-for-data-engineers-a191965ac538?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-10T16:50:07.000Z",
          "wordCount": 1515,
          "title": "Pandas for Data Engineers",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1024/1*v64M9uKn1Yqs_ly80ucq5Q.png"
        }
      ]
    },
    {
      "title": "Machine Learning Blog | ML@CMU | Carnegie Mellon University",
      "feedUrl": "https://blog.ml.cmu.edu/feed/",
      "siteUrl": "https://blog.ml.cmu.edu",
      "articles": []
    },
    {
      "title": "Apple Machine Learning Research",
      "feedUrl": "https://machinelearning.apple.com/rss.xml",
      "siteUrl": "https://machinelearning.apple.com",
      "articles": [
        {
          "id": "resource-constrained",
          "author": null,
          "description": "We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning…",
          "link": "https://machinelearning.apple.com/research/resource-constrained",
          "publishedOn": "2024-02-13T00:00:00.000Z",
          "wordCount": 2406,
          "title": "Resource-constrained Stereo Singing Voice Cancellation",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "autoregressive-image-models",
          "author": null,
          "description": "This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2…",
          "link": "https://machinelearning.apple.com/research/autoregressive-image-models",
          "publishedOn": "2024-02-01T00:00:00.000Z",
          "wordCount": 1259,
          "title": "Scalable Pre-training of Large Autoregressive Image Models",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "acoustic-model-fusion",
          "author": null,
          "description": "Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted its accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain…",
          "link": "https://machinelearning.apple.com/research/acoustic-model-fusion",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 3302,
          "title": "Acoustic Model Fusion for End-to-end Speech Recognition",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "large-scale-training",
          "author": null,
          "description": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite the widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for…",
          "link": "https://machinelearning.apple.com/research/large-scale-training",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1158,
          "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "flexible-keyword",
          "author": null,
          "description": "Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (for example, large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder, which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a…",
          "link": "https://machinelearning.apple.com/research/flexible-keyword",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1015,
          "title": "Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "investigating-salient-representation",
          "author": null,
          "description": "Representations from models such as Bidirectional Encoder Representations from Transformers (BERT) and Hidden units BERT (HuBERT) have helped to achieve state-of-the-art performance in dimensional speech emotion recognition. Both HuBERT, and BERT models generate fairly large dimensional representations, and such models were not trained with emotion recognition task in mind. Such large dimensional representations result in speech emotion models with large parameter size, resulting in both memory and computational cost complexities. In this work, we investigate the selection of representations…",
          "link": "https://machinelearning.apple.com/research/investigating-salient-representation",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1038,
          "title": "Investigating Salient Representations and Label Variance Modeling in Dimensional Speech Emotion Analysis",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "coml",
          "author": null,
          "description": "Machine learning (ML) models are fundamentally shaped by data, and building inclusive ML systems requires significant considerations around how to design representative datasets. Yet, few novice-oriented ML modeling tools are designed to foster hands-on learning of dataset design practices, including how to design for data diversity and inspect for data quality.\nTo this end, we outline a set of four data design practices (DDPs) for designing inclusive ML models and share how we designed a tablet-based application called Co-ML to foster the learning of DDPs through a collbaborative ML model…",
          "link": "https://machinelearning.apple.com/research/coml",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1957,
          "title": "Co-ML: Collaborative Machine Learning Model Building for Developing Dataset Design Practices",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "user-level-differentially",
          "author": null,
          "description": "We study differentially private stochastic convex optimization (DP-SCO) under user-level privacy where each user may hold multiple data items. Existing work for  user-level DP-SCO either requires super-polynomial runtime or requires number of users that grows polynomially with the dimensionality of the problem. We develop new algorithms for user-level DP-SCO that obtain optimal rates, run in polynomial time, and require a number of users that grow logarithmically in the dimension. Moreover, our algorithms are the first  to obtain optimal rates for non-smooth functions in polynomial time. These…",
          "link": "https://machinelearning.apple.com/research/user-level-differentially",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 3172,
          "title": "User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "transport-solvers",
          "author": null,
          "description": "Two salient limitations have long hindered the relevance of optimal transport methods to machine learning. First, the  computational cost of standard sample-based solvers (when used on batches of  samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \\textit{all} points from both measures, their output can be heavily influenced by outliers. A flurry of recent works has addressed these computational and modeling limitations. Still it has resulted in two separate strains of methods: While the computational outlook was…",
          "link": "https://machinelearning.apple.com/research/transport-solvers",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 2210,
          "title": "Unbalanced Low-Rank Optimal Transport Solvers",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "hybrid-model-learning",
          "author": null,
          "description": "This paper was accepted at the workshop Deep Generative Models for Health at NeurIPS 2023.\nCardiovascular diseases (CVDs) are a major global health concern, making the longitudinal monitoring of cardiovascular biomarkers vital for early diagnosis and intervention. A core challenge is the inference of cardiac pulse parameters from pulse waves, especially when acquired from wearable sensors at peripheral body locations. Traditional machine learning (ML) approaches face hurdles in this context due to the scarcity of labeled data, primarily sourced from clinical settings. Simultaneously, physical…",
          "link": "https://machinelearning.apple.com/research/hybrid-model-learning",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 4466,
          "title": "Hybrid Model Learning for Cardiovascular Biomarkers Inference",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "one-wide-ffn",
          "author": null,
          "description": "This paper was accepted at WMT conference at EMNLP.\nThe Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work, we explore the role of FFN and find that despite, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by…",
          "link": "https://machinelearning.apple.com/research/one-wide-ffn",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 5315,
          "title": "One Wide Feedforward is All You Need",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "faster-nerf",
          "author": null,
          "description": "Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, and/or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple NeRF+SR pipeline that directly combines existing modules, and we propose a lightweight augmentation…",
          "link": "https://machinelearning.apple.com/research/faster-nerf",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 1050,
          "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        }
      ]
    },
    {
      "title": "Google AI Blog",
      "feedUrl": "https://blog.research.google/atom.xml",
      "siteUrl": "http://blog.research.google/",
      "articles": [
        {
          "id": "http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html",
          "author": null,
          "description": "Posted by Mónica Ribero Díaz, Research Scientist, Google Research\n\n\n\n\n\nDifferential privacy (DP) is a property of randomized mechanisms that limit the influence of any individual user’s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies across industries and government applications (e.g., the US census) without compromising individual user identities.  As its adoption increases, it’s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, researchers compared six sparse vector technique (SVT) variations and found that only two…",
          "link": "http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html",
          "publishedOn": "2024-02-13T22:11:00.000Z",
          "wordCount": 27935,
          "title": "DP-Auditorium: A flexible library for auditing differential privacy",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/w1200-h630-p-k-no-nu/hero.jpg"
        },
        {
          "id": "http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html",
          "author": null,
          "description": "Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML\n\n\n\n\nObjects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks. Discrete mathematics and computer science have a long history of formalizing such networks as graphs, consisting of nodes connected by edges in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. \n\n\n\n\nGraph neural networks, or GNNs for short, have emerged…",
          "link": "http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html",
          "publishedOn": "2024-02-06T19:17:00.000Z",
          "wordCount": 27781,
          "title": "Graph neural networks in TensorFlow",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/w1200-h630-p-k-no-nu/TFGNN%20hero.gif"
        },
        {
          "id": "http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html",
          "author": null,
          "description": "Posted by Rajat Sen and Yichen Zhou, Google Research\n\n\n\n\n\nTime-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models performed well in the M5 competition).\n\n\n\nAt the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as translation, retrieval-augmented generation, and code completion. Thes…",
          "link": "http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html",
          "publishedOn": "2024-02-02T19:07:00.000Z",
          "wordCount": 27728,
          "title": "A decoder-only foundation model for time-series forecasting",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/w1200-h630-p-k-no-nu/hero.jpg"
        },
        {
          "id": "http://blog.research.google/2024/02/intervening-on-early-readouts-for.html",
          "author": null,
          "description": "Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research\n\n\n\n\nMachine learning models in the real world are often trained on limited data that may contain unintended statistical biases. For example, in the CELEBA celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as medical diagnosis. \n\n\n\n\n\nSurprisingly, recent work has also discovered an inherent tendency of deep networks to amplify such statistical biases, through the so-called simplicity bias of deep learning. T…",
          "link": "http://blog.research.google/2024/02/intervening-on-early-readouts-for.html",
          "publishedOn": "2024-02-02T17:49:00.000Z",
          "wordCount": 27761,
          "title": "Intervening on early readouts for mitigating spurious features and simplicity bias",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/w1200-h630-p-k-no-nu/SiFer%20Hero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html",
          "author": null,
          "description": "Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML\n\n\n\n\nText-to-image diffusion models have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., Stable Diffusion, DALL·E, and Imagen). While recent advancements in inference solutions on Android via MediaPipe and iOS via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.\n \n\nTo that end, in “MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices”, we introduce a novel approach with the potential for rapid text-to-image generation on…",
          "link": "http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html",
          "publishedOn": "2024-01-31T21:59:00.000Z",
          "wordCount": 27727,
          "title": "MobileDiffusion: Rapid text-to-image generation on-device",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/w1200-h630-p-k-no-nu/InstantTIGO%20hero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html",
          "author": null,
          "description": "Posted by Manish Gupta, Staff Software Engineer, Google Research\n\n\n\n\nAI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs).  LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver tens of exaflops of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively.\n\n\n\nThe bulk of an LLM’s memory and compute are consumed by weights in matrix multiplication operations. Using narrower data types reduces memory consumption. For example, storing weights in the 8-bit integer (i.e., U8 or S8)…",
          "link": "http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html",
          "publishedOn": "2024-01-26T19:56:00.000Z",
          "wordCount": 27947,
          "title": "Mixed-input matrix multiplication performance optimizations",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/w1200-h630-p-k-no-nu/matrixhero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html",
          "author": null,
          "description": "Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google\n\n\n\n\nGraphs, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. \n\n\n\n\nA common approach to learning on graphs are graph neural networks (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a message-passing framework,…",
          "link": "http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html",
          "publishedOn": "2024-01-23T22:27:00.000Z",
          "wordCount": 27895,
          "title": "Exphormer: Scaling transformers for graph-structured data",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/w1200-h630-p-k-no-nu/EXPHORMER%2005large.gif"
        },
        {
          "id": "http://blog.research.google/2024/01/introducing-aspire-for-selective.html",
          "author": null,
          "description": "Posted by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team\n\n\n\n\n\nIn the fast-evolving landscape of artificial intelligence, large language models (LLMs) have revolutionized the way we interact with machines, pushing the boundaries of natural language understanding and generation to unprecedented heights. Yet, the leap into high-stakes decision-making applications remains a chasm too wide, primarily due to the inherent uncertainty of model predictions. Traditional LLMs generate responses recursively, yet they lack an intrinsic mechanism to assign a confidence score to these responses. Although one can derive a confidence score by summing up the probabilities of individual tokens in the sequence, traditional approaches typically fall short in reliably dist…",
          "link": "http://blog.research.google/2024/01/introducing-aspire-for-selective.html",
          "publishedOn": "2024-01-18T18:03:00.000Z",
          "wordCount": 27602,
          "title": "Introducing ASPIRE for selective prediction in LLMs",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/w1200-h630-p-k-no-nu/ASPIRE%20hero.jpg"
        }
      ]
    },
    {
      "title": "FastML",
      "feedUrl": "https://fastml.com/atom.xml",
      "siteUrl": "https://fastml.com/atom.xml",
      "articles": []
    },
    {
      "title": "FastML",
      "feedUrl": "http://fastml.com/atom.xml",
      "siteUrl": "https://fastml.com/atom.xml",
      "articles": []
    },
    {
      "title": "AI and Social Science - Brendan O'Connor",
      "feedUrl": "http://brenocon.com/blog/feed/",
      "siteUrl": "http://brenocon.com/blog",
      "articles": []
    },
    {
      "title": "Probably Overthinking It",
      "feedUrl": "http://allendowney.blogspot.com/feeds/posts/default",
      "siteUrl": "http://allendowney.blogspot.com/",
      "articles": []
    },
    {
      "title": "The Spectator",
      "feedUrl": "http://blog.shakirm.com/feed/",
      "siteUrl": "https://blog.shakirm.com",
      "articles": []
    },
    {
      "title": "Edwin Chen's Blog",
      "feedUrl": "http://blog.echen.me/feeds/all.atom.xml",
      "siteUrl": "http://blog.echen.me/",
      "articles": []
    },
    {
      "title": "i am trask",
      "feedUrl": "https://iamtrask.github.io/feed.xml",
      "siteUrl": "https://iamtrask.github.io/",
      "articles": []
    },
    {
      "title": "Pete Warden's blog",
      "feedUrl": "https://petewarden.com/feed/",
      "siteUrl": "https://petewarden.com",
      "articles": [
        {
          "id": "http://petewarden.com/?p=7906",
          "author": "Pete Warden",
          "description": "A few months ago I started updating TensorFlow Lite Micro for the Raspberry Pi Pico board, which uses the RP2040 microcontroller. I ran into some baffling bugs that stopped me making progress, but eventually I tracked them down to my poor understanding of the memory layout. Since I had to do a deep dive, I […]",
          "link": "https://petewarden.com/2024/01/16/understanding-the-raspberry-pi-picos-memory-layout/",
          "publishedOn": "2024-01-16T03:02:47.000Z",
          "wordCount": 4732,
          "title": "Understanding the Raspberry Pi Pico’s Memory Layout",
          "imageUrl": "https://petewarden.files.wordpress.com/2024/02/pico-ram-layout-1.png"
        }
      ]
    },
    {
      "title": "Stories by Adam Geitgey on Medium",
      "feedUrl": "https://medium.com/feed/@ageitgey",
      "siteUrl": "https://medium.com/@ageitgey?source=rss-ba4c55e4aa3d------2",
      "articles": []
    },
    {
      "title": "Adit Deshpande",
      "feedUrl": "https://adeshpande3.github.io/feed.xml",
      "siteUrl": "https://adeshpande3.github.io",
      "articles": []
    },
    {
      "title": "Stories by Andrej Karpathy on Medium",
      "feedUrl": "https://medium.com/feed/@karpathy",
      "siteUrl": "https://medium.com/@karpathy?source=rss-ac9d9a35533e------2",
      "articles": []
    },
    {
      "title": "Comments on:",
      "feedUrl": "https://blog.wikimedia.org/c/research-2/wikimedia-research-newsletter/feed/",
      "siteUrl": "https://diff.wikimedia.org",
      "articles": []
    },
    {
      "title": "Explosion · RSS Feed",
      "feedUrl": "https://explosion.ai/feed.xml",
      "siteUrl": "https://explosion.ai",
      "articles": [
        {
          "id": "blog:nesta-skills",
          "author": "India Kerle",
          "description": "A case study on Nesta’s workflow for extracting 7 million job ads to better understand UK skill demand, using a custom mapping step to match skills to any government taxonomy.",
          "link": "https://explosion.ai/explosion.ai/blog/nesta-skills",
          "publishedOn": "2024-02-05T00:00:00.000Z",
          "wordCount": null,
          "title": "How Nesta uses NLP to process 7m job ads and shed light on the UK’s labor market",
          "imageUrl": null
        },
        {
          "id": "https://arxiv.org/abs/2402.01352",
          "author": "Explosion",
          "description": "We use the spaCy library for tokenization, part-of-speech tagging, and lemmatization of the words in the descriptions.",
          "link": "https://arxiv.org/abs/2402.01352",
          "publishedOn": "2024-02-02T00:00:00.000Z",
          "wordCount": 710,
          "title": "Describing Images Fast and Slow: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes",
          "imageUrl": "/static/browse/0.3.4/images/arxiv-logo-fb.png"
        },
        {
          "id": "event:az-2024",
          "author": "Sofie Van Landeghem",
          "description": "LLMs are paving the way for fast prototyping of NLP applications. Here, Sofie showcases how to build a structured NLP pipeline to mine clinical trials, using spaCy and spacy-llm. Moving beyond a fast prototype, she offers pragmatic solutions to make the pipeline more reliable and cost efficient.",
          "link": "https://speakerdeck.com/sofievl/2024-01-23-az",
          "publishedOn": "2024-01-23T00:00:00.000Z",
          "wordCount": 4797,
          "title": "spacy-llm: From quick prototyping with LLMs to more reliable and efficient NLP solutions",
          "imageUrl": "https://files.speakerdeck.com/presentations/ba2c9b51130349d48f52d3ee59a1cc6d/slide_0.jpg?28650740"
        },
        {
          "id": "release:spacy-llm_0.7.0",
          "author": "Explosion",
          "description": "Supporting arbitrarily long docs and various new tasks",
          "link": "https://github.com/explosion/spacy-llm",
          "publishedOn": "2024-01-19T00:00:00.000Z",
          "wordCount": 2561,
          "title": "🦙 spacy-llm v0.7.0",
          "imageUrl": "https://opengraph.githubassets.com/6868c6e5b1b3c72a7a3de2b61651dc408740e3eeb2c619044d3ac413edf2f97a/explosion/spacy-llm"
        }
      ]
    },
    {
      "title": "Justin Domke",
      "feedUrl": "https://justindomke.wordpress.com/feed/",
      "siteUrl": "https://justindomke.wordpress.com",
      "articles": []
    },
    {
      "title": "Blog - Jowanza Joseph",
      "feedUrl": "https://www.jowanza.com/blog?format=rss",
      "siteUrl": "https://www.jowanza.com/blog/",
      "articles": []
    },
    {
      "title": "Distill",
      "feedUrl": "https://distill.pub/rss.xml",
      "siteUrl": "https://distill.pub",
      "articles": []
    },
    {
      "title": "natural language processing blog",
      "feedUrl": "https://nlpers.blogspot.com/feeds/posts/default",
      "siteUrl": "https://nlpers.blogspot.com/",
      "articles": []
    },
    {
      "title": "MachineLearningMastery.com",
      "feedUrl": "https://feeds.feedburner.com/MachineLearningMastery",
      "siteUrl": "https://machinelearningmastery.com/",
      "articles": [
        {
          "id": "https://machinelearningmastery.com/?p=15635",
          "author": "Vinod Chugani",
          "description": "In the vast universe of data, it’s not always about what we can see but rather what we can infer. Confidence intervals, a cornerstone of inferential statistics, empower us to make educated guesses about a larger population based on our sample data. Using the Ames Housing dataset, let’s unravel the concept of confidence intervals and […]\nThe post Inferential Insights: How Confidence Intervals Illuminate the Ames Real Estate Market appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/inferential-insights-confidence-intervals/",
          "publishedOn": "2024-02-13T03:29:30.000Z",
          "wordCount": null,
          "title": "Inferential Insights: How Confidence Intervals Illuminate the Ames Real Estate Market",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15854",
          "author": "Vinod Chugani",
          "description": "Navigating the complex landscape of real estate analytics involves unraveling distinct narratives shaped by various property features within the housing market data. Our exploration today takes us into the realm of a potent yet frequently overlooked data visualization tool: the pair plot. This versatile graphic not only sheds light on the robustness and orientation of […]\nThe post Mastering Pair Plots for Visualization and Hypothesis Creation in the Ames Housing Market appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/pair-plots/",
          "publishedOn": "2024-02-08T02:11:59.000Z",
          "wordCount": null,
          "title": "Mastering Pair Plots for Visualization and Hypothesis Creation in the Ames Housing Market",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15614",
          "author": "Vinod Chugani",
          "description": "In the realm of real estate, understanding the intricacies of property features and their impact on sale prices is paramount. In this exploration, we’ll dive deep into the Ames Housing dataset, shedding light on the relationships between various features and their correlation with the sale price. Harnessing the power of data visualization, we’ll unveil patterns, […]\nThe post Feature Relationships 101: Lessons from the Ames Housing Data appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/feature-relationships-101/",
          "publishedOn": "2024-02-05T02:11:57.000Z",
          "wordCount": null,
          "title": "Feature Relationships 101: Lessons from the Ames Housing Data",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15473",
          "author": "Vinod Chugani",
          "description": "The real estate market is a complex ecosystem driven by numerous variables such as location, property features, market trends, and economic indicators. One dataset that offers a deep dive into this complexity is the Ames Housing dataset. Originating from Ames, Iowa, this dataset comprises various properties and their characteristics, ranging from the type of alley […]\nThe post Exploring Dictionaries, Classifying Variables, and Imputing Data in the Ames Dataset appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/classifying_variables/",
          "publishedOn": "2024-01-31T19:56:26.000Z",
          "wordCount": null,
          "title": "Exploring Dictionaries, Classifying Variables, and Imputing Data in the Ames Dataset",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15741",
          "author": "Vinod Chugani",
          "description": "Geospatial visualization has become an essential tool for understanding and representing data in a geographical context. It plays a pivotal role in various real-world applications, from urban planning and environmental studies to real estate and transportation. For instance, city planners might use geospatial data to optimize public transportation routes, while real estate professionals could leverage […]\nThe post From Data to Map: Visualizing Ames House Prices with Python appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/data-to-map-geospatial/",
          "publishedOn": "2024-01-29T22:07:30.000Z",
          "wordCount": null,
          "title": "From Data to Map: Visualizing Ames House Prices with Python",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15543",
          "author": "Vinod Chugani",
          "description": "In this enlightening journey through the myriad lanes of Ames properties, we shine our spotlight on Descriptive Statistics, a cornerstone of Data Science. The study of the Ames properties dataset provides a rich landscape for implementing Descriptive Statistics to distill volumes of data into meaningful summaries. Descriptive statistics serve as the initial step in data […]\nThe post Decoding Data: An Introduction to Descriptive Statistics with the Ames Housing Dataset appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/decoding-data-descriptive-statistics/",
          "publishedOn": "2024-01-25T06:28:01.000Z",
          "wordCount": null,
          "title": "Decoding Data: An Introduction to Descriptive Statistics with the Ames Housing Dataset",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=15381",
          "author": "Vinod Chugani",
          "description": "The digital age has ushered in an era where data-driven decision-making is pivotal in various domains, real estate being a prime example. Comprehensive datasets, like the one concerning properties in Ames, offer a treasure trove for data enthusiasts. Through meticulous exploration and analysis of such datasets, one can uncover patterns, gain insights, and make informed […]\nThe post Revealing the Invisible: Visualizing Missing Values in Ames Housing appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/revealing_the_invisible/",
          "publishedOn": "2024-01-22T11:22:26.000Z",
          "wordCount": null,
          "title": "Revealing the Invisible: Visualizing Missing Values in Ames Housing",
          "imageUrl": null
        },
        {
          "id": "https://machinelearningmastery.com/?p=16186",
          "author": "Adrian Tam",
          "description": "Machine learning is an amazing tool for many tasks. OpenCV is a great library for manipulating images. It would be great if we can put them together. In this 7-part crash course, you will learn from examples how to make use of machine learning and the image processing API from OpenCV to accomplish some goals. […]\nThe post Machine Learning in OpenCV (7-Day Mini-Course) appeared first on MachineLearningMastery.com.",
          "link": "https://machinelearningmastery.com/machine-learning-in-opencv-7-day-mini-course/",
          "publishedOn": "2024-01-20T01:33:33.000Z",
          "wordCount": null,
          "title": "Machine Learning in OpenCV (7-Day Mini-Course)",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "KDnuggets",
      "feedUrl": "https://www.kdnuggets.com/feed",
      "siteUrl": "https://www.kdnuggets.com",
      "articles": [
        {
          "id": "https://www.kdnuggets.com/?p=164048",
          "author": "KDnuggets",
          "description": "Stable Diffusion models are revolutionizing digital artistry, transforming mere text into stunning, lifelike images. Explore further here.",
          "link": "https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-text-to-image-stable-diffusion?utm_source=rss&utm_medium=rss&utm_campaign=generative-ai-playground-text-to-image-stable-diffusion-with-stability-ai-stable-diffusion-xl-and-compvis-on-the-latest-intel-gpu",
          "publishedOn": "2024-02-13T18:00:30.000Z",
          "wordCount": 4812,
          "title": "Generative AI Playground: Text-to-Image Stable Diffusion with Stability AI, Stable Diffusion XL, and CompVis on the Latest Intel® GPU",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/consolvo-text-to-image-fig1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=164018",
          "author": "Abid Ali Awan",
          "description": "This article has explored three promising prompting techniques that have been developed to reduce the occurrence of hallucinations in large language models.",
          "link": "https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization?utm_source=rss&utm_medium=rss&utm_campaign=3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization",
          "publishedOn": "2024-02-13T17:00:06.000Z",
          "wordCount": 5862,
          "title": "3 Research-Driven Advanced Prompting Techniques for LLM Efficiency and Speed Optimization",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/adeleye_3_researchdriven_advanced_prompting_techniques_llm_efficiency_speed_optimization_10.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163981",
          "author": "Nisha Arya",
          "description": "Want to prepare your tech career for 2024 and onwards? Have a look at O’Reilly’s FREE technology trends report.",
          "link": "https://www.kdnuggets.com/2024-tech-trends-ai-breakthroughs-development-insights-oreilly-free-report?utm_source=rss&utm_medium=rss&utm_campaign=2024-tech-trends-ai-breakthroughs-development-insights-from-oreillys-free-report",
          "publishedOn": "2024-02-13T15:00:51.000Z",
          "wordCount": 5639,
          "title": "2024 Tech Trends: AI Breakthroughs & Development Insights from O’Reilly’s Free Report",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/ahmed-free-technology-trends-report-2024-oreilly-1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163961",
          "author": "Cornellius Yudha Wijaya",
          "description": "Don’t overlook these essential aspects of programming activity.",
          "link": "https://www.kdnuggets.com/how-to-comment-your-python-code-as-a-data-scientist?utm_source=rss&utm_medium=rss&utm_campaign=how-to-comment-your-python-code-as-a-data-scientist",
          "publishedOn": "2024-02-13T13:00:18.000Z",
          "wordCount": 5813,
          "title": "How To Comment Your Python Code as a Data Scientist",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/wijaya_comment_python_code_data_scientist_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163953",
          "author": "Michael Galarnyk",
          "description": "You don't always have high-quality labeled datasets for supervised machine learning. Learn about why you should augment your real data with synthetic data as well as the ways to generate it.",
          "link": "https://www.kdnuggets.com/synthetic-data-for-machine-learning?utm_source=rss&utm_medium=rss&utm_campaign=synthetic-data-for-machine-learning",
          "publishedOn": "2024-02-12T17:00:56.000Z",
          "wordCount": 6001,
          "title": "Synthetic Data for Machine Learning",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/galarnyk_synthetic_data_machine_learning_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163946",
          "author": "Bala Priya C",
          "description": "Interested in data engineering but don't know where to start? Get up to speed in data engineering fundamentals with this free course.",
          "link": "https://www.kdnuggets.com/free-data-engineering-course-for-beginners?utm_source=rss&utm_medium=rss&utm_campaign=free-data-engineering-course-for-beginners",
          "publishedOn": "2024-02-12T15:00:58.000Z",
          "wordCount": 5588,
          "title": "Free Data Engineering Course for Beginners",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_free_data_engineering_course_beginners_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163939",
          "author": "Nisha Arya",
          "description": "This blog will go through platforms and courses you can take that will get you from 0-100 on your data science knowledge.",
          "link": "https://www.kdnuggets.com/learn-data-science-on-a-budget?utm_source=rss&utm_medium=rss&utm_campaign=learn-data-science-on-a-budget",
          "publishedOn": "2024-02-12T13:00:56.000Z",
          "wordCount": 5410,
          "title": "Learn Data Science on a Budget",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_learn_data_science_budget_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163920",
          "author": "KDnuggets",
          "description": "Don't miss this chance to chart your course toward a successful career in business analytics. Reserve your spot now and embark on a journey of knowledge and growth!",
          "link": "https://www.kdnuggets.com/2024/02/uc-business-analytics-summer-2024-information-session?utm_source=rss&utm_medium=rss&utm_campaign=university-of-cincinnati-ms-business-analytics-summer-2024-information-session",
          "publishedOn": "2024-02-09T18:10:21.000Z",
          "wordCount": 4872,
          "title": "University of Cincinnati MS Business Analytics Summer 2024 Information Session",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/uc-240209.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163889",
          "author": "Abid Ali Awan",
          "description": "Unlock the secrets to building, deploying, and monitoring models like a pro.",
          "link": "https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-mlops-engineer?utm_source=rss&utm_medium=rss&utm_campaign=the-only-free-course-you-need-to-become-a-mlops-engineer",
          "publishedOn": "2024-02-09T15:00:58.000Z",
          "wordCount": 5442,
          "title": "The Only Free Course You Need To Become a MLOps Engineer",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_free_course_need_become_mlops_engineer_2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163851",
          "author": "Cornellius Yudha Wijaya",
          "description": "Machine Learning is a skill that everyone should have, and these cheap books would facilitate that learning process.",
          "link": "https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning?utm_source=rss&utm_medium=rss&utm_campaign=5-cheap-books-to-master-machine-learning",
          "publishedOn": "2024-02-09T13:00:00.000Z",
          "wordCount": 5491,
          "title": "5 Cheap Books to Master Machine Learning",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/wijaya_5_cheap_books_master_machine_learning_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163907",
          "author": "KDnuggets",
          "description": "It’s more important than ever to think long-term about the analytics partnerships you forge. Are you choosing technologies that will stand the test of time? Are you choosing companies with proven track records?",
          "link": "https://www.kdnuggets.com/2024/02/altair-navigating-todays-data-ai-market-uncertainty?utm_source=rss&utm_medium=rss&utm_campaign=navigating-todays-data-and-ai-market-uncertainty",
          "publishedOn": "2024-02-08T18:00:47.000Z",
          "wordCount": 5531,
          "title": "Navigating Today’s Data and AI Market Uncertainty",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/data-center-with-server-racks-corridor-room-3d-render-digital-data-cloud-technology.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163869",
          "author": "Abid Ali Awan",
          "description": "Discover the top AI coding assistants that can 10X your productivity overnight - #5 has the best autocomplete feature, and #1 is the most advanced code assistant tool ever seen!",
          "link": "https://www.kdnuggets.com/top-5-ai-coding-assistants-you-must-try?utm_source=rss&utm_medium=rss&utm_campaign=top-5-ai-coding-assistants-you-must-try",
          "publishedOn": "2024-02-08T15:00:31.000Z",
          "wordCount": 5642,
          "title": "Top 5 AI Coding Assistants You Must Try",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_top_5_ai_coding_assistants_must_try_3.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163845",
          "author": "Kanwal Mehreen",
          "description": "Are you preparing for your dream data science job but feeling overwhelmed by the vast amount of online resources? Look no further than this free and easily accessible web-based book to help you brush up on your skills and feel confident for your upcoming interview.",
          "link": "https://www.kdnuggets.com/free-data-science-interview-book-to-land-your-dream-job?utm_source=rss&utm_medium=rss&utm_campaign=free-data-science-interview-book-to-land-your-dream-job",
          "publishedOn": "2024-02-08T13:00:45.000Z",
          "wordCount": 5693,
          "title": "Free Data Science Interview Book to Land Your Dream Job",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/mehreen_free_data_science_interview_book_land_dream_job_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163897",
          "author": "KDnuggets",
          "description": "Intel offers a thrilling glimpse into the next generation of AI, showcasing the power of Camel-5b and Open LLaMA 3B LLMs.",
          "link": "https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b?utm_source=rss&utm_medium=rss&utm_campaign=generative-ai-playground-llms-with-camel-5b-and-open-llama-3b-on-the-latest-intel-gpu",
          "publishedOn": "2024-02-07T18:00:37.000Z",
          "wordCount": 4647,
          "title": "Generative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on the Latest Intel® GPU",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/intel-230207.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163834",
          "author": "Nate Rosidi",
          "description": "This code based tutorial provides a brief introduction to Sentiment Analysis, a method used to predict emotions, similar to a digital psychologist.",
          "link": "https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words?utm_source=rss&utm_medium=rss&utm_campaign=sentiment-analysis-in-python-going-beyond-bag-of-words",
          "publishedOn": "2024-02-07T15:00:57.000Z",
          "wordCount": 6013,
          "title": "Sentiment Analysis in Python: Going Beyond Bag of Words",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/rosidi_sentiment_analysis_python_going_beyond_bag_words_2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163827",
          "author": "Bala Priya C",
          "description": "Want to learn Python to kickstart your career in data? Here are five free courses to help you master Python for data science.",
          "link": "https://www.kdnuggets.com/5-free-courses-to-master-python-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-to-master-python-for-data-science",
          "publishedOn": "2024-02-07T13:00:52.000Z",
          "wordCount": 5425,
          "title": "5 Free Courses to Master Python for Data Science",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_free_courses_master_python_data_science_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163817",
          "author": "John Hughes",
          "description": "This article introduced you to the world of ranking functions in SQL. We will cover the basics of how they work, how they're used, and how to avoid common pitfalls.",
          "link": "https://www.kdnuggets.com/breaking-down-denserank-a-step-by-step-guide-for-sql-enthusiasts?utm_source=rss&utm_medium=rss&utm_campaign=breaking-down-dense_rank-a-step-by-step-guide-for-sql-enthusiasts",
          "publishedOn": "2024-02-06T17:00:33.000Z",
          "wordCount": 7536,
          "title": "Breaking Down DENSE_RANK(): A Step-by-Step Guide for SQL Enthusiasts",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/hughes_breaking_denserank_stepbystep_guide_sql_enthusiasts_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163810",
          "author": "Nisha Arya",
          "description": "Want to learn more about AI and ChatGPT in 2024 for FREE? Keep reading.",
          "link": "https://www.kdnuggets.com/5-free-courses-on-ai-and-chatgpt-to-take-you-from-0-100?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-on-ai-and-chatgpt-to-take-you-from-0-100",
          "publishedOn": "2024-02-06T15:00:50.000Z",
          "wordCount": 5493,
          "title": "5 FREE Courses on AI and ChatGPT to Take You From 0-100",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_5_free_courses_ai_chatgpt_take_0100_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163799",
          "author": "Josep Ferrer",
          "description": "Discovering the Hidden Logic Behind SQL's Command Order.",
          "link": "https://www.kdnuggets.com/the-essential-guide-to-sql-execution-order?utm_source=rss&utm_medium=rss&utm_campaign=the-essential-guide-to-sqls-execution-order",
          "publishedOn": "2024-02-06T13:00:05.000Z",
          "wordCount": 5836,
          "title": "The Essential Guide to SQL’s Execution Order",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/ferrer_essential_guide_sql_execution_order_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163858",
          "author": "Nisha Arya",
          "description": "If you are new to generative AI or an expert who wants to learn more, O’Reilly offers a range of resources to kickstart your generative AI journey.",
          "link": "https://www.kdnuggets.com/books-courses-and-live-events-to-learn-generative-ai-with-oreilly?utm_source=rss&utm_medium=rss&utm_campaign=books-courses-and-live-events-to-learn-generative-ai-with-oreilly",
          "publishedOn": "2024-02-05T18:00:20.000Z",
          "wordCount": 5616,
          "title": "Books, Courses, and Live Events to Learn Generative AI with O’Reilly",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_books_courses_live_events_learn_generative_ai_oreilly_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163789",
          "author": "Oleg Royz",
          "description": "This article outlines strategies for overcoming data maturity challenges and accelerating AI adoption.",
          "link": "https://www.kdnuggets.com/data-maturity-the-cornerstone-of-ai-enabled-innovation?utm_source=rss&utm_medium=rss&utm_campaign=data-maturity-the-cornerstone-of-ai-enabled-innovation",
          "publishedOn": "2024-02-05T17:00:33.000Z",
          "wordCount": 6178,
          "title": "Data Maturity: The Cornerstone of AI-Enabled Innovation",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/royz_data_maturity_cornerstone_aienabled_innovation_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163782",
          "author": "Nisha Arya",
          "description": "The number of tech layoffs since 2022 is constantly rising. Why is this happening?",
          "link": "https://www.kdnuggets.com/the-surge-in-tech-layoffs-2024-who-to-blame?utm_source=rss&utm_medium=rss&utm_campaign=the-surge-in-tech-layoffs-2024-whos-to-blame",
          "publishedOn": "2024-02-05T15:00:05.000Z",
          "wordCount": 5681,
          "title": "The Surge in Tech Layoffs 2024: Who’s to Blame?",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_surge_tech_layoffs_2024_blame_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163774",
          "author": "Bala Priya C",
          "description": "How and why the data lake architecture often fails to meet its promises. And how better governance helps mitigate such challenges.",
          "link": "https://www.kdnuggets.com/a-data-lake-you-call-it-it-a-data-swamp?utm_source=rss&utm_medium=rss&utm_campaign=a-data-lake-you-call-it-its-a-data-swamp",
          "publishedOn": "2024-02-05T13:00:51.000Z",
          "wordCount": 6137,
          "title": "A Data Lake, You Call It? It’s a Data Swamp",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_data_lake_call_data_swamp_2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163748",
          "author": "Abid Ali Awan",
          "description": "Learn how to use OpenAI Python API for accessing language, embedding, audio, vision, and image generation models.",
          "link": "https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide?utm_source=rss&utm_medium=rss&utm_campaign=openai-api-for-beginners-your-easy-to-follow-starter-guide",
          "publishedOn": "2024-02-02T15:00:07.000Z",
          "wordCount": 7001,
          "title": "OpenAI API for Beginners: Your Easy-to-Follow Starter Guide",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_openai_api_beginners_easytofollow_starter_guide_11.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163710",
          "author": "Josep Ferrer",
          "description": "Navigating Complex Data Structures with Python's json_normalize.",
          "link": "https://www.kdnuggets.com/converting-jsons-to-pandas-dataframes-parsing-them-the-right-way?utm_source=rss&utm_medium=rss&utm_campaign=converting-jsons-to-pandas-dataframes-parsing-them-the-right-way",
          "publishedOn": "2024-02-02T13:00:10.000Z",
          "wordCount": 5750,
          "title": "Converting JSONs to Pandas DataFrames: Parsing Them the Right Way",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/ferrer_converting_jsons_pandas_dataframes_parsing_right_way_51.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163701",
          "author": "Abid Ali Awan",
          "description": "Learn about the most common questions asked during data science interviews. This blog covers non-technical, Python, SQL, statistics, data analysis, and machine learning questions.",
          "link": "https://www.kdnuggets.com/26-data-science-interview-questions-you-should-know?utm_source=rss&utm_medium=rss&utm_campaign=26-data-science-interview-questions-you-should-know",
          "publishedOn": "2024-02-01T15:00:59.000Z",
          "wordCount": 5516,
          "title": "26 Data Science Interview Questions You Should Know",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_26_data_science_interview_questions_know_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163682",
          "author": "Nisha Arya",
          "description": "Tune in to these 5 AI podcasts at the gym or on your commute to keep up to date with the world of AI.",
          "link": "https://www.kdnuggets.com/top-5-ai-podcasts-you-cant-miss-in-2024?utm_source=rss&utm_medium=rss&utm_campaign=top-5-ai-podcasts-you-cant-miss-in-2024",
          "publishedOn": "2024-02-01T13:00:42.000Z",
          "wordCount": 5464,
          "title": "Top 5 AI Podcasts You Can’t Miss in 2024",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_top_5_ai_podcasts_cant_miss_2024_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163691",
          "author": "Vijay Singh Khatri",
          "description": "This article has provided a brief overview of ChatGPT and its capabilities. It also discussed the importance of efficient data analysis and the benefits of integrating it into the analysis process.",
          "link": "https://www.kdnuggets.com/maximizing-efficiency-in-data-analysis-with-chatgpt?utm_source=rss&utm_medium=rss&utm_campaign=maximizing-efficiency-in-data-analysis-with-chatgpt",
          "publishedOn": "2024-01-31T15:00:26.000Z",
          "wordCount": 6266,
          "title": "Maximizing Efficiency in Data Analysis with ChatGPT",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/khatri_maximizing_efficiency_data_analysis_chatgpt_3.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163676",
          "author": "Bala Priya C",
          "description": "Looking to make a career in data analytics? Take the first steps today with these free courses.",
          "link": "https://www.kdnuggets.com/5-free-courses-to-break-into-data-analytics?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-to-break-into-data-analytics",
          "publishedOn": "2024-01-31T13:00:10.000Z",
          "wordCount": 5706,
          "title": "5 Free Courses to Break Into Data Analytics",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_free_courses_break_data_analytics_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163726",
          "author": "KDnuggets",
          "description": "LLMs aren't the right tool for most business applications. Find out why — and learn which AI techniques are a better match.",
          "link": "https://www.kdnuggets.com/2024/01/pecan-llms-used-alone-cant-address-companys-predictive-needs?utm_source=rss&utm_medium=rss&utm_campaign=why-llms-used-alone-cant-address-your-companys-predictive-needs",
          "publishedOn": "2024-01-30T18:00:01.000Z",
          "wordCount": 6172,
          "title": "Why LLMs Used Alone Can’t Address Your Company’s Predictive Needs",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/pecan-230130-2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163658",
          "author": "Nikola Greb",
          "description": "Case study of LLM's ability to learn, generalize, and be creative.",
          "link": "https://www.kdnuggets.com/does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster?utm_source=rss&utm_medium=rss&utm_campaign=does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster",
          "publishedOn": "2024-01-30T17:00:02.000Z",
          "wordCount": 6099,
          "title": "Does ChatGPT Have The Potential To Become A New Chess Super Grandmaster?",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster_011.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163651",
          "author": "Cornellius Yudha Wijaya",
          "description": "Complex queries seem intimidating, but this guide gives you insight into how to work more easily with SQL queries.",
          "link": "https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries?utm_source=rss&utm_medium=rss&utm_campaign=a-step-by-step-guide-to-reading-and-understanding-sql-queries",
          "publishedOn": "2024-01-30T15:00:42.000Z",
          "wordCount": 6028,
          "title": "A Step by Step Guide to Reading and Understanding SQL Queries",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/wijaya_step_step_guide_reading_understanding_sql_queries_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163644",
          "author": "Bala Priya C",
          "description": "For acing coding interviews, you need to have a rock solid foundation in data structures and algorithms. Check out these free university courses to help you in your journey.",
          "link": "https://www.kdnuggets.com/5-free-university-courses-to-ace-coding-interviews?utm_source=rss&utm_medium=rss&utm_campaign=5-free-university-courses-to-ace-coding-interviews",
          "publishedOn": "2024-01-30T13:00:57.000Z",
          "wordCount": 5612,
          "title": "5 Free University Courses to Ace Coding Interviews",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_free_university_courses_ace_coding_interviews_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163595",
          "author": "Olena Zherebetska",
          "description": "The post highlights real-world examples of NLP use cases across industries. It also covers NLP's objectives, challenges, and latest research developments.",
          "link": "https://www.kdnuggets.com/natural-language-processing-bridging-human-communication-with-ai?utm_source=rss&utm_medium=rss&utm_campaign=natural-language-processing-bridging-human-communication-with-ai",
          "publishedOn": "2024-01-29T17:00:16.000Z",
          "wordCount": 7952,
          "title": "Natural Language Processing: Bridging Human Communication with AI",
          "enclosure": {
            "url": "https://mastercard-a.akamaihd.net/global-risk/videos/DecisionIntelligenceExternalVideoGLOBALJul19.mp4",
            "length": "51628090",
            "type": "video/mp4"
          },
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/natural-language-processing-bridging-human-communication-with-ai_012.jpeg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163578",
          "author": "Nate Rosidi",
          "description": "ChatGPT can be a great tool for data scientists. Here’s what I learned about where it excels and where it is less so.",
          "link": "https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=what-i-learned-from-using-chatgpt-for-data-science",
          "publishedOn": "2024-01-29T15:00:51.000Z",
          "wordCount": 6184,
          "title": "What I Learned From Using ChatGPT for Data Science",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/rosidi_learned_chatgpt_data_science_2.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163571",
          "author": "Nisha Arya",
          "description": "Want to learn about AI? You can for FREE with LinkedIn.",
          "link": "https://www.kdnuggets.com/learn-with-linkedin-free-courses-about-ai?utm_source=rss&utm_medium=rss&utm_campaign=learn-with-linkedin-free-courses-about-ai",
          "publishedOn": "2024-01-29T13:00:14.000Z",
          "wordCount": 5405,
          "title": "Learn with LinkedIn: Free Courses About AI",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_learn_linkedin_free_courses_ai_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163520",
          "author": "Abid Ali Awan",
          "description": "Data Engineering ZoomCamp offers free access to reading materials, video tutorials, assignments, homeworks, projects, and workshops.",
          "link": "https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-professional-data-engineer?utm_source=rss&utm_medium=rss&utm_campaign=the-only-free-course-you-need-to-become-a-professional-data-engineer",
          "publishedOn": "2024-01-26T15:00:48.000Z",
          "wordCount": 5834,
          "title": "The Only Free Course You Need To Become a Professional Data Engineer",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_free_course_need_become_professional_data_engineer_2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163439",
          "author": "Josep Ferrer",
          "description": "Learn how to write SQL queries that are not just code but clear, modular, and reusable work.",
          "link": "https://www.kdnuggets.com/sql-simplified-crafting-modular-and-understandable-queries-with-ctes?utm_source=rss&utm_medium=rss&utm_campaign=sql-simplified-crafting-modular-and-understandable-queries-with-ctes",
          "publishedOn": "2024-01-26T13:00:11.000Z",
          "wordCount": 5537,
          "title": "SQL Simplified: Crafting Modular and Understandable Queries with CTEs",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/ferrer_sql_simplified_crafting_modular_understandable_queries_ctes_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163480",
          "author": "Ankur Gupta",
          "description": "Learn how AI can be used to design fair and efficient EV charging grids by optimizing their placement and pricing.",
          "link": "https://www.kdnuggets.com/leveraging-ai-to-design-fair-and-equitable-ev-charging-grids?utm_source=rss&utm_medium=rss&utm_campaign=leveraging-ai-to-design-fair-and-equitable-ev-charging-grids",
          "publishedOn": "2024-01-25T15:00:41.000Z",
          "wordCount": 7274,
          "title": "Leveraging AI to Design Fair and Equitable EV Charging Grids",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/leveraging-ai-to-design-fair-and-equitable-ev-charging-grids_08.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163509",
          "author": "Abid Ali Awan",
          "description": "Improve your version control skills to resolve issues and maintain a clean Git repository.",
          "link": "https://www.kdnuggets.com/10-advanced-git-techniques?utm_source=rss&utm_medium=rss&utm_campaign=10-advanced-git-techniques",
          "publishedOn": "2024-01-25T13:00:04.000Z",
          "wordCount": 5987,
          "title": "10 Advanced Git Techniques",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_10_advanced_git_techniques_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163553",
          "author": "KDnuggets",
          "description": "Transform your AI aspirations into reality—join Uplimit's AI revolution!",
          "link": "https://www.kdnuggets.com/2024/01/uplimit-supercharge-your-ai-journey-openai-course?utm_source=rss&utm_medium=rss&utm_campaign=supercharge-your-ai-journey-join-uplimits-free-building-ai-products-using-openai-course",
          "publishedOn": "2024-01-25T01:48:48.000Z",
          "wordCount": 4856,
          "title": "Supercharge Your AI Journey! Join Uplimit’s Free Building AI Products using OpenAI Course",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/uplimit-230124-1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163457",
          "author": "Angelica Lo Duca",
          "description": "Using Generative AI to speed up and enhance data visualization.",
          "link": "https://www.kdnuggets.com/how-generative-ai-can-help-you-improve-your-data-visualization-charts?utm_source=rss&utm_medium=rss&utm_campaign=how-generative-ai-can-help-you-improve-your-data-visualization-charts",
          "publishedOn": "2024-01-24T15:00:50.000Z",
          "wordCount": 6682,
          "title": "How Generative AI Can Help You Improve Your Data Visualization Charts",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/how-generative-ai-can-help-you-improve-your-data-visualization-charts_012.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163358",
          "author": "Nisha Arya",
          "description": "Learning a new skill just got better with these 10 free courses for lucrative careers.",
          "link": "https://www.kdnuggets.com/8-free-google-courses-to-land-top-paying-jobs?utm_source=rss&utm_medium=rss&utm_campaign=8-free-google-courses-to-land-top-paying-jobs",
          "publishedOn": "2024-01-24T13:00:41.000Z",
          "wordCount": 5467,
          "title": "8 Free Google Courses to Land Top Paying Jobs",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_8_free_google_courses_land_top_paying_jobs_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163544",
          "author": "KDnuggets",
          "description": "This week on KDnuggets: Here are five free university courses to help you get started in a data science career • Understand the unstructured data dilemma • And much, much more!",
          "link": "https://www.kdnuggets.com/newsletter-n03-2024-01-24?utm_source=rss&utm_medium=rss&utm_campaign=kdnuggets-news-january-24-5-free-university-courses-to-learn-data-science-convert-unstructured-data-into-structured-insights-with-llms",
          "publishedOn": "2024-01-23T19:26:45.000Z",
          "wordCount": 4856,
          "title": "KDnuggets News, January 24: 5 Free University Courses to Learn Data Science • Convert Unstructured Data into Structured Insights with LLMs",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_free_university_courses_learn_data_science_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163532",
          "author": "KDnuggets",
          "description": "Learn what Predictive GenAI does and how it can make predictive analytics far more accessible, efficient, and meaningful for your business.",
          "link": "https://www.kdnuggets.com/2024/01/pecan-powering-predictive-genai?utm_source=rss&utm_medium=rss&utm_campaign=powering-up-with-predictive-genai",
          "publishedOn": "2024-01-23T18:00:13.000Z",
          "wordCount": 5818,
          "title": "Powering Up with Predictive GenAI",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/pecan-240123-2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163418",
          "author": "Bala Priya C",
          "description": "Want to make a successful career switch to data science? From learning data science concepts to cracking interviews, read this guide to move one step closer to your first data science job.",
          "link": "https://www.kdnuggets.com/7-steps-to-landing-your-first-data-science-job?utm_source=rss&utm_medium=rss&utm_campaign=7-steps-to-landing-your-first-data-science-job",
          "publishedOn": "2024-01-23T17:00:30.000Z",
          "wordCount": 6390,
          "title": "7 Steps to Landing Your First Data Science Job",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_7_steps_landing_first_data_science_job_11.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163409",
          "author": "Nate Rosidi",
          "description": "Here are data repositories that will up your data science game and improve your data projects.",
          "link": "https://www.kdnuggets.com/top-16-technical-data-sources-for-advanced-data-science-projects?utm_source=rss&utm_medium=rss&utm_campaign=top-16-technical-data-sources-for-advanced-data-science-projects",
          "publishedOn": "2024-01-23T15:00:58.000Z",
          "wordCount": 6014,
          "title": "Top 16 Technical Data Sources for Advanced Data Science Projects",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/rosidi_top_16_technical_data_sources_advanced_data_science_projects_2.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163349",
          "author": "Nisha Arya",
          "description": "Prompt engineering and generative AI are becoming hotter by the day. Be part of the heat!",
          "link": "https://www.kdnuggets.com/ai-prompt-engineers-are-making-300ky?utm_source=rss&utm_medium=rss&utm_campaign=ai-prompt-engineers-are-making-300k-y",
          "publishedOn": "2024-01-23T13:00:24.000Z",
          "wordCount": 5380,
          "title": "AI Prompt Engineers are Making $300k/y",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_ai_prompt_engineers_making_300ky_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163398",
          "author": "Suman Saurav",
          "description": "Developing a conversational AI chatbot requires substantial effort. However, understanding and addressing key challenges in natural language understanding can streamline the development process.",
          "link": "https://www.kdnuggets.com/3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them?utm_source=rss&utm_medium=rss&utm_campaign=3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them",
          "publishedOn": "2024-01-22T17:00:44.000Z",
          "wordCount": 6221,
          "title": "3 Crucial Challenges in Conversational AI Development and How to Avoid Them",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/saurav_3_crucial_challenges_conversational_ai_development_avoid_1-scaled.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163388",
          "author": "Bala Priya C",
          "description": "Context managers in Python help you handle resources efficiently. Let's learn how you can use them to manage database connections, subprocesses, and more.",
          "link": "https://www.kdnuggets.com/3-interesting-uses-of-python-context-managers?utm_source=rss&utm_medium=rss&utm_campaign=3-interesting-uses-of-pythons-context-managers",
          "publishedOn": "2024-01-22T15:00:07.000Z",
          "wordCount": 5761,
          "title": "3 Interesting Uses of Python’s Context Managers",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_3_interesting_uses_python_context_managers_3.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163368",
          "author": "Ahmad Anis",
          "description": "Zephyr is a series of Large Language Models released by Hugging Face trained using distilled supervised fine-tuning (dSFT) on larger models with significantly improved task accuracy.",
          "link": "https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model?utm_source=rss&utm_medium=rss&utm_campaign=exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model",
          "publishedOn": "2024-01-22T13:00:22.000Z",
          "wordCount": 5876,
          "title": "Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language Model",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/anis_exploring_zephyr_7b_comprehensive_guide_latest_large_language_model_9-scaled.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163315",
          "author": "Abid Ali Awan",
          "description": "Path to a Free Self-Taught Education in Data Science for Everyone.",
          "link": "https://www.kdnuggets.com/enroll-in-a-data-science-undergraduate-program-for-free?utm_source=rss&utm_medium=rss&utm_campaign=enroll-in-a-data-science-undergraduate-program-for-free",
          "publishedOn": "2024-01-19T15:00:05.000Z",
          "wordCount": 5510,
          "title": "Enroll in a Data Science Undergraduate Program For Free",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/awan_enroll_data_science_undergraduate_program_free_3.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163241",
          "author": "Bala Priya C",
          "description": "Want to refresh your SQL skills? Bookmark these useful cheat sheets covering SQL basics, joins, window functions, and more.",
          "link": "https://www.kdnuggets.com/5-super-helpful-sql-cheat-sheets-you-cant-miss?utm_source=rss&utm_medium=rss&utm_campaign=5-super-helpful-sql-cheat-sheets-you-cant-miss",
          "publishedOn": "2024-01-19T13:00:54.000Z",
          "wordCount": 5428,
          "title": "5 Super Helpful SQL Cheat Sheets You Can’t Miss!",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_super_helpful_sql_cheat_sheets_cant_miss_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163226",
          "author": "Josep Ferrer",
          "description": "From Chaos to Clarity: Understanding the Unstructured Data Dilemma.",
          "link": "https://www.kdnuggets.com/5-ways-of-converting-unstructured-data-into-structured-insights-with-llms?utm_source=rss&utm_medium=rss&utm_campaign=5-ways-of-converting-unstructured-data-into-structured-insights-with-llms",
          "publishedOn": "2024-01-18T15:00:09.000Z",
          "wordCount": 5919,
          "title": "5 Ways of Converting Unstructured Data into Structured Insights with LLMs",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/ferrer_5_ways_converting_unstructured_data_structured_insights_llms_4.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163263",
          "author": "Bala Priya C",
          "description": "Looking to make a career in data science? Here are five free university courses to help you get started.",
          "link": "https://www.kdnuggets.com/5-free-university-courses-to-learn-data-science?utm_source=rss&utm_medium=rss&utm_campaign=5-free-university-courses-to-learn-data-science",
          "publishedOn": "2024-01-18T13:00:50.000Z",
          "wordCount": 5425,
          "title": "5 Free University Courses to Learn Data Science",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/c_5_free_university_courses_learn_data_science_1.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163338",
          "author": "KDnuggets",
          "description": "Looking to understand the universal semantic layer and how it can improve your data stack? This GigaOm Sonor report on Semantic Layers can help you delve deeper.",
          "link": "https://www.kdnuggets.com/2024/01/cube-6-reasons-why-a-universal-semantic-layer-is-beneficial?utm_source=rss&utm_medium=rss&utm_campaign=6-reasons-why-a-universal-semantic-layer-is-beneficial-to-your-data-stack",
          "publishedOn": "2024-01-17T18:00:36.000Z",
          "wordCount": 5156,
          "title": "6 Reasons Why a Universal Semantic Layer is Beneficial to Your Data Stack",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/cube-230117-1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163270",
          "author": "Taranjeet Singh",
          "description": "Semantic vector search is an advanced search technique revolutionizes how we interact with information by understanding the true meaning of words, thus leading to more relevant and insightful results.",
          "link": "https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions?utm_source=rss&utm_medium=rss&utm_campaign=how-semantic-vector-search-transforms-customer-support-interactions",
          "publishedOn": "2024-01-17T15:00:59.000Z",
          "wordCount": 5890,
          "title": "How Semantic Vector Search Transforms Customer Support Interactions",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/singh_semantic_vector_search_transforms_customer_support_interactions_4-scaled.jpg"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163219",
          "author": "Nisha Arya",
          "description": "Kickstart your AI journey this new year with 5 FREE learning resources from Microsoft.",
          "link": "https://www.kdnuggets.com/5-free-courses-on-ai-with-microsoft-for-2024?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-on-ai-with-microsoft-for-2024",
          "publishedOn": "2024-01-17T13:00:33.000Z",
          "wordCount": 5381,
          "title": "5 FREE Courses on AI with Microsoft for 2024",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/arya_5_free_courses_ai_microsoft_2024_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163329",
          "author": "KDnuggets",
          "description": "This week on KDnuggets: We cover what a generative AI developer does, what tools you need to master, and how to get started • An in-depth analysis of Python DataFrame library syntax, speed, and usability... which one is best? • And much, much more!",
          "link": "https://www.kdnuggets.com/newsletter-n02-2024-01-17?utm_source=rss&utm_medium=rss&utm_campaign=kdnuggets-news-january-17-4-steps-to-become-a-generative-ai-developer-pandas-vs-polars-a-comparative-analysis",
          "publishedOn": "2024-01-17T11:00:51.000Z",
          "wordCount": 4914,
          "title": "KDnuggets News, January 17: 4 Steps to Become a Generative AI Developer • Pandas vs. Polars: A Comparative Analysis",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/cotton_4_steps_become_generative_ai_developer_1.png"
        },
        {
          "id": "https://www.kdnuggets.com/?p=163306",
          "author": "KDnuggets",
          "description": "Today, we're proud to announce a significant addition to our catalog at Machine Learning Mastery. Known for our detailed, code-centric guides, we're taking a leap further into the realms of Computer Vision with our latest offering.",
          "link": "https://www.kdnuggets.com/2024/01/mlm-discover-the-world-of-computer-vision-ebook?utm_source=rss&utm_medium=rss&utm_campaign=discover-the-world-of-computer-vision-introducing-mlms-latest-opencv-ebook",
          "publishedOn": "2024-01-16T18:00:15.000Z",
          "wordCount": 5032,
          "title": "Discover the World of Computer Vision: Introducing MLM’s Latest OpenCV Ebook",
          "imageUrl": "https://www.kdnuggets.com/wp-content/uploads/mlm-computer-vision-opencv-ebook-e1705415250554.png"
        }
      ]
    },
    {
      "title": "Yanir Seroussi | Data & AI for Impact",
      "feedUrl": "https://yanirseroussi.com/feed/",
      "siteUrl": "https://yanirseroussi.com/",
      "articles": [
        {
          "id": "https://yanirseroussi.com/2024/02/12/nudging-chatgpt-to-invent-books-you-have-no-time-to-read/",
          "author": null,
          "description": "Getting ChatGPT Plus to elaborate on possible book content and produce a PDF cheatsheet, with the goal of learning about its capabilities.",
          "link": "https://yanirseroussi.com/2024/02/12/nudging-chatgpt-to-invent-books-you-have-no-time-to-read/",
          "publishedOn": "2024-02-12T05:00:00.000Z",
          "wordCount": 2105,
          "title": "Nudging ChatGPT to invent books you have no time to read",
          "imageUrl": "https://yanirseroussi.com/2024/02/12/nudging-chatgpt-to-invent-books-you-have-no-time-to-read/chatgpt-plus-road-trip.webp"
        },
        {
          "id": "https://yanirseroussi.com/til/2024/02/06/future-software-development-may-require-fewer-humans/",
          "author": null,
          "description": "Reflecting on an interview with Jason Warner, CEO of poolside.",
          "link": "https://yanirseroussi.com/til/2024/02/06/future-software-development-may-require-fewer-humans/",
          "publishedOn": "2024-02-06T06:15:00.000Z",
          "wordCount": 1399,
          "title": "Future software development may require fewer humans",
          "imageUrl": null
        },
        {
          "id": "https://yanirseroussi.com/2024/02/05/substance-over-titles-your-first-data-hire-may-be-a-data-scientist/",
          "author": null,
          "description": "Advice for hiring a startup’s first data person: match skills to business needs, consider contractors, and get help from data people.",
          "link": "https://yanirseroussi.com/2024/02/05/substance-over-titles-your-first-data-hire-may-be-a-data-scientist/",
          "publishedOn": "2024-02-05T02:45:00.000Z",
          "wordCount": 2406,
          "title": "Substance over titles: Your first data hire may be a data scientist",
          "imageUrl": "https://yanirseroussi.com/2024/02/05/substance-over-titles-your-first-data-hire-may-be-a-data-scientist/versatile-data-person.webp"
        },
        {
          "id": "https://yanirseroussi.com/2024/01/19/new-decade-new-tagline-data-and-ai-for-impact/",
          "author": null,
          "description": "Shifting focus to ‘Data & AI for Impact’, with more startup-related content, increased posting frequency, and deeper audience engagement.",
          "link": "https://yanirseroussi.com/2024/01/19/new-decade-new-tagline-data-and-ai-for-impact/",
          "publishedOn": "2024-01-19T00:00:00.000Z",
          "wordCount": 1218,
          "title": "New decade, new tagline: Data & AI for Impact",
          "imageUrl": "https://yanirseroussi.com/2024/01/19/new-decade-new-tagline-data-and-ai-for-impact/data-and-ai-for-impact-logo.png"
        }
      ]
    },
    {
      "title": "Articles for MAL",
      "feedUrl": "https://www.nowpublishers.com/feed/MAL",
      "siteUrl": "https://www.nowpublishers.com/feed/MAL",
      "articles": [
        {
          "id": "http://www.nowpublishers.com/article/Details/MAL-106",
          "author": null,
          "description": "Abstract\nDecision-making systems based on AI and machine learning\r\nhave been used throughout a wide range of real-world\r\nscenarios, including healthcare, law enforcement, education,\r\nand finance. It is no longer far-fetched to envision a future\r\nwhere autonomous systems will drive entire business decisions\r\nand, more broadly, support large-scale decision-making\r\ninfrastructure to solve society’s most challenging problems.\r\nIssues of unfairness and discrimination are pervasive when\r\ndecisions are being made by humans, and remain (or are potentially\r\namplified) when decisions are made using machines\r\nwith little transparency, accountability, and fairness. In this\r\nmonograph, we introduce a framework for causal fairness\r\nanalysis with the intent of filling in this gap, i.e., understanding,\r\nmodeling, and possibly solving issues of fairness\r\nin decision-making settings.\nThe main insight of our approach will be to link the quantification\r\nof the disparities present in the observed data\r\nwith the underlying, often unobserved, collection of causal\r\nmechanisms that generate the disparity in the first place, \r\na challenge we call the Fundamental Problem of Causal\r\nFairness Analysis (FPCFA). In order to solve the FPCFA,\r\nwe study the problem of decomposing variations and empirical\r\nmeasures of fairness that attribute such variations\r\nto structural mechanisms and different units of the population.\r\nOur effort culminates in the Fairness Map, the first\r\nsystematic attempt to organize and explain the relationship\r\nbetween various criteria found in the literature. Finally, we\r\nstudy which causal assumptions are minimally needed for\r\nperforming causal fairness analysis and propose the Fairness\r\nCookbook, which allows one to assess the existence of\r\ndisparate impact and disparate treatment.\nSuggested Citation\nDrago Plečko and Elias Bareinboim (2024), \"Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning\", Foundations and Trends® in Machine Learning: Vol. 17: No. 3, pp 304-589. http://dx.doi.org/10.1561/2200000106",
          "link": "http://www.nowpublishers.com/article/Details/MAL-106",
          "publishedOn": "2024-01-30T23:00:00.000Z",
          "wordCount": 811,
          "title": "Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning",
          "imageUrl": null
        },
        {
          "id": "http://www.nowpublishers.com/article/Details/MAL-100",
          "author": null,
          "description": "Abstract\nAggregated predictors are obtained by making a set of basic\r\npredictors vote according to some weights, that is, to some\r\nprobability distribution. Randomized predictors are obtained\r\nby sampling in a set of basic predictors, according to some\r\nprescribed probability distribution.\nThus, aggregated and randomized predictors have in common\r\nthat their definition rely on a probability distribution on\r\nthe set of predictors. In statistical learning theory, there is a\r\nset of tools designed to understand the generalization ability\r\nof such predictors: PAC-Bayesian or PAC-Bayes bounds.\nSince the original PAC-Bayes bounds (Shawe-Taylor and\r\nWilliamson, 1997; McAllester, 1998), these tools have been\r\nconsiderably improved in many directions. We will for example\r\ndescribe a simplified version of the localization technique\r\n(Catoni, 2003; Catoni, 2007) that was missed by the\r\ncommunity, and later rediscovered as “mutual information\r\nbounds”. Very recently, PAC-Bayes bounds received a considerable\r\nattention. There was workshop on PAC-Bayes at\r\nNIPS 2017, (Almost) 50 Shades of Bayesian Learning: PACBayesian\r\ntrends and insights, organized by B. Guedj, F. Bach and P. Germain. \r\nOne of the reasons of this recent interest\r\nis the successful application of these bounds to neural\r\nnetworks (Dziugaite and Roy, 2017). Since then, this is a\r\nrecurring topic of workshops in the major machine learning\r\nconferences.\nThe objective of these notes is to provide an elementary\r\nintroduction to PAC-Bayes bounds.\nSuggested Citation\nPierre Alquier (2024), \"User-friendly Introduction to PAC-Bayes Bounds\", Foundations and Trends® in Machine Learning: Vol. 17: No. 2, pp 174-303. http://dx.doi.org/10.1561/2200000100",
          "link": "http://www.nowpublishers.com/article/Details/MAL-100",
          "publishedOn": "2024-01-21T23:00:00.000Z",
          "wordCount": 657,
          "title": "User-friendly Introduction to PAC-Bayes Bounds",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Lil’Log",
      "feedUrl": "https://lilianweng.github.io/lil-log/feed.xml",
      "siteUrl": "https://lilianweng.github.io/lil-log/",
      "articles": []
    },
    {
      "title": "Natural Language Processing",
      "feedUrl": "https://www.reddit.com/r/LanguageTechnology.rss",
      "siteUrl": "https://www.reddit.com/r/LanguageTechnology",
      "articles": [
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aqk2h7/nltk_ssl_certificate_error_no_module_named_pip/",
          "author": null,
          "description": "I am having some serious issues with nltk.download() method. Specifically, I am struggling with a SSL certificate problem (solutions posted in other treads don't seem to work, probably due to some interferences from pip) and .zip format handling.\n Operating system: Sonoma 14.2.1. Ide: Pycharm. Interpreter: Pycharm/Projects/*Projectname*/.venv/bin/python (alternative: usr/local/bin/python3.12, but it doesn't matter which interpreter as same problems arise with both).\n 1- I have installed NLTK with pycharm in the virtualenv of the project. (Note: I also have NLTK downloaded also in the local library)\n 2- While working with the Python console, I can successfully import NLTK, as well as nltk.corpus. However, when it comes to nltk.download(*corpus*), it raises a well-known error (discussed in o…",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aqk2h7/nltk_ssl_certificate_error_no_module_named_pip/",
          "publishedOn": "2024-02-14T10:46:43.000Z",
          "wordCount": null,
          "title": "NLTK, SSL Certificate Error, No module named pip",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aqjeoe/graph_rag_for_wikipedia/",
          "author": null,
          "description": "submitted by    /u/davidmezzetti  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aqjeoe/graph_rag_for_wikipedia/",
          "publishedOn": "2024-02-14T10:01:48.000Z",
          "wordCount": null,
          "title": "Graph RAG for Wikipedia",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aqe74t/need_to_finetune_llms_on_chat_data/",
          "author": null,
          "description": "Hello, I've hundreds of transcripts having conversations between agent and customer. I've been working on finetuning mistral using QLoRa. The objective is to make a chatbot or virtual agent. However, after finetuning, instead of a single response, the model generates an entire conversation. My prompt looks something like - \n Conversation :\n {A Small Snippet from a Transcript}\n Agent :\n { }\n Should I change the prompt? Does anyone have any experience on finetuning on chat/transcript data? Any help would be highly appreciated.\n    submitted by    /u/Evermore2307  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aqe74t/need_to_finetune_llms_on_chat_data/",
          "publishedOn": "2024-02-14T04:25:23.000Z",
          "wordCount": null,
          "title": "Need to Finetune LLMs on Chat Data",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aq58wq/diffusion_of_thoughts_chainofthought_reasoning_in/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.07754\n Code: https://github.com/HKUNLP/diffusion-of-thoughts\n Abstract:\n  \nDiffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.\n  \n   submitted by    /u/FastestGPU  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aq58wq/diffusion_of_thoughts_chainofthought_reasoning_in/",
          "publishedOn": "2024-02-13T21:34:33.000Z",
          "wordCount": null,
          "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1apxedy/deploy_llama_2_mistral_and_mixtral_on_aws_ec2/",
          "author": null,
          "description": "Hi everyone,\n In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.\n vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).\n See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html\n In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.\n Please don't hesitate to share feedbacks about this article, it will be very much appreciated!\n Julien\n    submitted by    /u/juliensalinas  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1apxedy/deploy_llama_2_mistral_and_mixtral_on_aws_ec2/",
          "publishedOn": "2024-02-13T16:20:54.000Z",
          "wordCount": null,
          "title": "Deploy LLaMA 2, Mistral, and Mixtral, on AWS EC2 with vLLM",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aph3iw/training_transformer_based_language_model_from/",
          "author": null,
          "description": "Hi All,\n I tried to implement Attention is all you need paper to train language models based on the Transformer architecture, but failing to train these models. The loss or perplexity for that matter, reduces to some extent, but than it becomes stagnant. I am running out of ideas here, like what could be the reason for that. To access the scripts click here. Any help in this regard is appreciated.\n Thanks in advance.\n Cheers \n    submitted by    /u/gubberex  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aph3iw/training_transformer_based_language_model_from/",
          "publishedOn": "2024-02-13T01:28:36.000Z",
          "wordCount": null,
          "title": "Training Transformer based language model from scratch",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1apbxr2/help_my_gpt2_model_responds_to_prompts_with/",
          "author": null,
          "description": "I have been trying to feed my model with data related to chess rules, but the model responds nonsensically. \n The data I have used is a plain text file which is not very large. But what I don't know is whether the problem is that the data set is too small or that the information is not well structured. can anyone give me some guidance? \n How should a plain text dataset be structured? And if I convert it to csv what structure should it have? Also, if I want to create a much bigger dataset with the chess rules theme, how can I do it? Thanks anyway\n    submitted by    /u/Guilty0121  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1apbxr2/help_my_gpt2_model_responds_to_prompts_with/",
          "publishedOn": "2024-02-12T21:43:29.000Z",
          "wordCount": null,
          "title": "Help, my GPT2 model responds to prompts with nonsense answers.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ap9qdd/has_anyone_studied_computational_linguisticsnlp/",
          "author": null,
          "description": "I’d already written a post about my interest in choosing a master's degree in computational linguistics. I'm actually still undecided whether to pursue the academic world in traditional linguistics or to delve into NLP and computational linguistics. For the master's degree, I had already looked at the one in Language Technologies at Pisa and Konstanz ( Germany) and they both seemed really good until I saw the “Computational and Theoretical Modelling of Language and Cognition” course in Trento, which seems much, much better than the one in Pisa.\n I wanted to know in particular how is this MA, if it's really as great as it seems, and especially how affordable it is for a student who doesn't have a background in mathematics and computer science (I've only taken one statistics exam, but nothing too serious). I saw that the course doesn't require programming or math knowledge, however, there are mandatory exams in Mathematics and Machine Learning, and I wanted to know how feasible these courses actually are, whether they are very mathematical or more practical without requiring basic knowledge. Also, is it possible to find a job in the field after graduation, does the university provide connections with companies for its students?\n    submitted by    /u/aquilaa91  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ap9qdd/has_anyone_studied_computational_linguisticsnlp/",
          "publishedOn": "2024-02-12T20:15:43.000Z",
          "wordCount": null,
          "title": "Has anyone studied computational linguistics/NLP in TRENTO: “Computational and Theoretical Modelling of Language and Cognition”",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ap266r/exploring_adbased_revenue_options_for_gemini/",
          "author": null,
          "description": "Dear Google AI Team,\n As a supporter of Gemini Advanced, I understand the need for sustainable revenue models. Could a careful implementation of advertisements be a feasible solution? Here are some thoughts, emphasizing responsible integration:\n Contextual Emphasis: Focus on text-based ads directly related to conversation topics, ensuring relevancy for users. Limited and Unobtrusive: A small number of clearly marked ads in designated areas would minimize disruption to the user experience. Prioritizing User Needs: Gemini Advanced's core task must remain providing helpful and unbiased information. Sponsored results, if utilized, should only appear when highly relevant and should be clearly distinguished from organic answers. I believe a mindful approach to ads could potentially create a viable revenue stream while safeguarding Gemini Advanced's reputation as a reliable and trustworthy AI tool.\n Thank you for your consideration!\n Sincerely, Mehdi\n    submitted by    /u/General-Antelope-241  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ap266r/exploring_adbased_revenue_options_for_gemini/",
          "publishedOn": "2024-02-12T15:10:52.000Z",
          "wordCount": null,
          "title": "Exploring Ad-Based Revenue Options for Gemini Advanced",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ap1pbl/help_with_rasa_likert_survey_issue/",
          "author": null,
          "description": "Hey folks, I am currently developing a Rasa Chatbot to conduct a survey and I am having some issues.\n The survey in question uses a Likert scale thought, meaning that the answers to the vast majority of questions are \"strongly agree\", \"agree\", \"neutral\", \"disagree\" and \"strongly disagree\". I am using the \"forms\" with each of my questions having its own \"slot\". The issue that I am having is that because all of the questions have the same responses the chatbot doesn't know from content which answer applies to each question, I am unsure if my understanding of Rasa is flawed or if I am missing something but I don't know how I should go about fixing this.\n I'm aware this is a long shot but if anyone has any ideas or suggestions they would be much appreciated, I have added some of the code that …",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ap1pbl/help_with_rasa_likert_survey_issue/",
          "publishedOn": "2024-02-12T14:49:49.000Z",
          "wordCount": null,
          "title": "Help with Rasa Likert survey issue",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aoxls0/what_hardware_do_you_use_for_your_private_nlp/",
          "author": null,
          "description": "Since my laptop broke down and I am kicked out of my unis server after graduation, I am lost on how to provide the GPU Power for my NLP Projects now and what a new laptop should provide in RAM etc. and not get too expensive. What du you look for in your private hardware decisions when money is crucial? And do you rent yourself to a server elsewhere somehow, or do you get a device with the right GPU/CPU power?\n    submitted by    /u/nyliaw  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aoxls0/what_hardware_do_you_use_for_your_private_nlp/",
          "publishedOn": "2024-02-12T11:09:53.000Z",
          "wordCount": null,
          "title": "What Hardware do you use for your private NLP projects?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aovm9x/advanced_rag_techniques/",
          "author": null,
          "description": "Hi everyone,\n Here is an attempt to summarize different RAG Techniques for improved retrieval.\n The video goes through\n  \nLong Context re-ordering,\n Small-to-Big\n  \nAnd many more\n https://www.youtube.com/watch?v=YpcENPDn9u4&t=1s\n    submitted by    /u/Mosh_98  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aovm9x/advanced_rag_techniques/",
          "publishedOn": "2024-02-12T08:49:32.000Z",
          "wordCount": null,
          "title": "Advanced RAG Techniques",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aogmth/is_it_really_worth_a_ma_degree_in_computational/",
          "author": null,
          "description": "Can someone tell me their own experience, especially about finding a job after a degree in Language technology / computational linguistics ?\n I am about to graduate in a three-year course in foreign languages and literatures, I have taken various linguistics exams and am currently working on a thesis in historical linguistics. I have a great passion for linguistics and have become particularly interested in computational linguistics. At the moment, my professor of historical linguistics is very interested in me and has asked me to stay, not to leave, and always talks to me, even if not explicitly, about pursuing a Ph.D. in the future. Therefore, the choice was between staying at my university and continuing with the faculty of languages and cultures of Asia with a specialization in historical linguistics with the possibility of taking 3-4 courses in computational linguistics, or attending the university for NLP and computational linguistics in Germany at Konstanz or Tubingen, as they seem to be the only ones that accept students who do not have a background in computer science and do not require too much basic knowledge.\n I was wondering what the job prospects are after this degree, are there many job opportunities? Do these two programs (Konstanz / Tubingen) manage to easily place students in the workforce?\n Since I doubt that with just a few computational linguistics exams from my university I will really be able to find a job in these fields.\"\n    submitted by    /u/aquilaa91  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aogmth/is_it_really_worth_a_ma_degree_in_computational/",
          "publishedOn": "2024-02-11T19:47:09.000Z",
          "wordCount": null,
          "title": "Is it really worth a MA degree in Computational linguistics, especially in Gemrman ( Konstanz/ Tubingen) university? Are there really many job opportunities afterward ?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ao5h4a/developing_a_chrome_extension_to_filter_hateful/",
          "author": null,
          "description": "Hey Reddit community,\n ​\n My college project team consisting of two members is working on developing a Chrome extension aimed at filtering out hateful speech and negative comments encountered by users while surfing the web. Our goal is to create a tool that promotes a more positive online experience for users.\n Details:\n Functionality: Our extension will monitor the websites users visit in real-time. When it detects any content containing hateful speech or negative comments, it will automatically hide or filter out such content from the user's view.\n Feasibility: While we're excited about this project, we're also mindful of its feasibility. As two members, we want to ensure that the scope of the project is manageable within the given timeframe and resources. We're open to any suggestions or advice on how to approach the development process efficiently.\n Technical Considerations: We're particularly interested in insights on the technical aspects of building such an extension. Should we focus on specific programming languages or frameworks? Are there any existing libraries or APIs that could assist us in detecting and filtering out negative content effectively?\n Ethical Concerns: Additionally, we're aware of the ethical implications of content moderation. How can we strike a balance between filtering harmful content and respecting freedom of expression? Are there any best practices or guidelines we should adhere to in this regard?\n Any advice, tips, or resources you can provide would be greatly appreciated. We're eager to learn and make meaningful contributions with this project. Thanks in advance for your help!\n    submitted by    /u/jerry_10_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ao5h4a/developing_a_chrome_extension_to_filter_hateful/",
          "publishedOn": "2024-02-11T10:58:59.000Z",
          "wordCount": null,
          "title": "Developing a Chrome Extension to Filter Hateful Speech and Negative Comments - Advice Needed for College Project Team",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ao52n8/where_have_you_seen_rulebased_text_classification/",
          "author": null,
          "description": "I wrote a rule-based text classification app specialized in my language (Vietnamese) and would like to know where else this app is useful. For example if you input the prompt (which is use a bunch of keywords), e.g. fish 50k, then it will automatically label/classify the prompt like this: - Object: fish - Type of Object: food - Place of transaction: market - Type of place of transaction: offline - Consumer: myself - Type of consumer: myself - Price: 50000 VND\n The program can make this classification based on a config you declare, e.g.: yaml - Dimension name: Object Classification: - Food: fish, meat - Appliance: computer, speaker Default value: meat ... \n Which problems do you see this app will be useful? In general, where have you seen rule-based text classification being applied? Especially in the context of ChatGPT and its GPT store? I think rule-based classification is much cheaper and more accurate than statistical-based classification. Is that correct?\n    submitted by    /u/Ooker777  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ao52n8/where_have_you_seen_rulebased_text_classification/",
          "publishedOn": "2024-02-11T10:31:12.000Z",
          "wordCount": null,
          "title": "Where have you seen rule-based text classification being applied?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ao3sqg/analyzing_appraisel_feedback_for_real_estate/",
          "author": null,
          "description": "I'm excited to share that I'm diving into my first big natural language processing project focused on analyzing feedback related to the appraisal of real estate properties. I'll be working with Dutch text and aiming to classify various aspects of feedback to improve property evaluation processes.\n Here's a (translated) sample text I'll be working with:\n \"I believe the appraisal of my appartment is excessively high, particularly when compared to recent sales in our neighborhood. Additionally, the apartments you've selected as comparables for my unit are not suitable matches. I kindly request that you reassess the valuation.\"\n For this project, I've identified several key classifications (around 20) that the model needs to detect, in the above text it should for example find the following classifications.\n  \nAppraisal too high\n Comparison with neighbourhood\n Properties used for appraisal not similar\n  \nTo tackle this task, I'm considering starting with a pre-trained multilingual model like BERT-multilingual and fine tuning it with a dataset I've classified by hand. However, I'm open to suggestions and would love to hear your thoughts if you've worked on similar projects or have insights into the best approaches for classifying (Dutch) text in NLP. If you know of any similar projects online that would be very helpful as well! I'll be using Python for the implementation since I've been doing a lot of image classification tasks using Python and Tensorflow before.\n Any tips about wether to classify it sentence by sentence or the full text at once, how I could best go about this or any other helpful advice is very welcome!\n    submitted by    /u/AnterosNL  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ao3sqg/analyzing_appraisel_feedback_for_real_estate/",
          "publishedOn": "2024-02-11T09:01:49.000Z",
          "wordCount": null,
          "title": "Analyzing Appraisel Feedback for Real Estate",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1anee81/understanding_different_databases_choosing_the/",
          "author": null,
          "description": "submitted by    /u/Illustrious_Party330  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1anee81/understanding_different_databases_choosing_the/",
          "publishedOn": "2024-02-10T11:35:00.000Z",
          "wordCount": null,
          "title": "Understanding Different Databases | Choosing The Right Database For your...",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ancuj4/what_is_this_nlp_problem_called/",
          "author": null,
          "description": "Been searching the web for a few days now and have been unable to find the exact problem name and definition, so I’m turning to the experts. \n I have some documents of text and want to find the relevant sections of the text related to some key word or words. For example, say the document is about practical steps for how to take care of the pooping habits of common pets, and I search “dog” or “canine”. I’d like to get all sections of the text (there could be multiple) related to this discussion for dogs. Note that the text is fundamentally about taking care of animals pooping, not about animals themselves, so there could be parts of text relevant for dogs without the direct mention of them (likely around the sections where there is direct mention of them, but not always). Also note that by searching canine, I’d like to more or less get the same return as searching dog, even though the term is less commonly used. This second problem seems not nearly as hard, given both terms have very similar semantic meaning. The first is trickier given the model must understand relations and properties of the searched entity. \n Another example could be searching for “Donald Trump” in a news article about recent developments in republican candidates’ campaigns and how the party currently fairs against the democrats. It would be nice to get all discussions of Trump but also segments about how republicans are doing versus democrats since this is relevant for Trump. \n What is this problem called exactly? I’ve toyed around using a GPT and it yields reasonable results with the right prompts but perhaps there are more direct approaches.\n    submitted by    /u/AnonQuantGuy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ancuj4/what_is_this_nlp_problem_called/",
          "publishedOn": "2024-02-10T09:47:59.000Z",
          "wordCount": null,
          "title": "What is this NLP problem called?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amtdf0/sprachumfrage/",
          "author": null,
          "description": "Hallo,\n ich bin auf der Suche nach einem muttersprachlichen Urteil für französische Sprache. Für mein Examen im Fach Französisch führe ich eine Sprachuntersuchung durch zu prädikativen bzw. nicht prädikativen Adjektiven. Ich muss nun herausfinden, ob sich folgende Adjektive (in den Beispielen), von denen ich vermute, es seien ausschließlich nicht prädikative Adjektive, sich möglicherweise doch prädikativ verwenden lassen... Könnte ich hierzu ein muttersprachliches Urteil zur Korrektheit dieser Sätze bekommen? Die Sätze habe ich so geformt, dass eine prädikative Verwendung der Adjektive vorliegt.\n - l'autorité et la confiance sont possibles\n - l'effort et la lassitude sont physiques .\n - l'épouvante et la stupeur sont générales\n -l'équité et l'égalité sont absolues\n - le paca et le porc-épic sont alongées\n - leur coeur et leur cerveau sont pareils - le coeur est pareil?\n -la rancune et la haine sont éternelles\n -une puissance et une étendue sont égales - la puissance est égal?\n -la mortification et la fuite sont continuelles\n -l'oppression et la tristesse sont extrêmes\n -la religion et l' humanité sont inférieures (minderwertig)\n Funktionieren diese Sätze in dieser Schreibart wirklich?\n Über eine Rückmeldung wäre ich allen französischen Muttersprachlern sehr dankbar!!!\n    submitted by    /u/shiningstar12xh  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amtdf0/sprachumfrage/",
          "publishedOn": "2024-02-09T17:21:14.000Z",
          "wordCount": null,
          "title": "Sprachumfrage",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amt2d9/should_duolingo_create_an_18_language_course/",
          "author": null,
          "description": "submitted by    /u/Summer_19_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amt2d9/should_duolingo_create_an_18_language_course/",
          "publishedOn": "2024-02-09T17:08:59.000Z",
          "wordCount": null,
          "title": "Should Duolingo create an 18+ language course?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amlugx/d_unsupervised_log_anomaly_detection/",
          "author": null,
          "description": "Help!\n This is my first post on Reddit\n I am thinking about using the variational autoencoder model for anomaly detection . I have an Android Logs dataset. As the logs generated are a representative of time series type of data I thought about using the VAEs with an architecture involving LSTMs in the encoder and decoders. As far as what I know about the basics of the working of the autoencoder models , given a sequence size say n , during learning/training phase n-1 entries are used to generate a representation/prediction of the nth entry (log in this case) and then reconstruction error is minimized by the model.\n How could I approach ? Please suggest if there are mistakes in my approach. I was also thinking about converting the logs into word embeddings using Word2Vec and then using them as input for the VAE model.\n Please suggest latest techniques and tools that I must consider exploring and a sample approach to get started with for high accuracy in this situation.\n I'm seeking feedback on this approach. Do you spot any potential flaws or have suggestions for improvements? I'm open to any insights or experiences you can share!\n Looking forward to your input. Thanks!\n    submitted by    /u/CheesecakeNatural393  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amlugx/d_unsupervised_log_anomaly_detection/",
          "publishedOn": "2024-02-09T11:14:42.000Z",
          "wordCount": null,
          "title": "[D] Unsupervised Log Anomaly Detection",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amlb4r/analyzing_sales_transcripts/",
          "author": null,
          "description": "I'm a digital marketer and our company uses Chorus.ai to record/transcribe sales calls. What I'd like to do is essentially export as many sales calls as possible and identify pain points, problems, objections, etc. to better inform our marketing material. The ultimate goal is to quantify some of the qualitative data we have. \n I'm thinking about doing this with a combination of keywords (that we've identified and that possibly an AI tool could identify for us) + other AI tools out there to scrape/analyze 100+ calls. As I’m looking into this more I think ‘coding’ for themes is what I likely am trying to do.\n Any resources/tools for how to start doing this? The call transcripts are private company data and so using something like Chat GPT won't cut it due to privacy concerns.\n    submitted by    /u/driedupkelp  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amlb4r/analyzing_sales_transcripts/",
          "publishedOn": "2024-02-09T10:37:54.000Z",
          "wordCount": null,
          "title": "Analyzing sales transcripts",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amjpqg/named_entity_recognition_when_adjusting_labels/",
          "author": null,
          "description": "For example, I have a word in a text sequence that is \"patterns\" as in \"The patterns were very pretty.\" In the original character-wise label sequence, only \"pattern\" is labeled and the \"s\" isn't. When we tokenize \"patterns\" using a pre-trained BERT tokenizer, the result is pat, ##tern, ##s.\n I'm trying to adjust the labels using the offsets_mapping that is returned by the tokenizer, but that leads to the problem that only pat and ##tern are labeled and ##s isn't.\n My question is, if we have labels that don't span the entire word and stop mid-subword, does that lead to performance problems?\n    submitted by    /u/Seankala  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amjpqg/named_entity_recognition_when_adjusting_labels/",
          "publishedOn": "2024-02-09T08:40:43.000Z",
          "wordCount": null,
          "title": "[Named Entity Recognition] When adjusting labels after tokenization, do we include all subwords for BIO tags or just the ones that are originally labeled?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1amag5r/nlp_whitepaper_comparison_modelling_ideas/",
          "author": null,
          "description": "I'm looking to compare how government strategy and attitudes towards ageing poulations has evolved over time, and how they differ across a handful of European countries. \n The idea would be to take a handful of government policy whitepapers that address the ageing population challenge; and then extract the most common 'words' or 'themes' for each document, and then cluster together policies and strategy based on similarity scores. \n First thoughts around this would be to use TFIDF to identify important keywords for each specific whitepaper. I'd then use a word embeddings model like BERT to calculate measures of pairwise cosine similarity between documents, enabling me to cluster them in to groups. This would help to generate insight into which governments have similar strategies. \n To look at how this attitudes and strategy have evolved over time, i'd then probably look to build separate models each year. \n I wondered if anybody had any other ideas around this? Would be interesting to see if its possible to use a topic modelling approach like LDA or BERTopic to extract latent themes in each whitepaper instead. I wasnt sure if its possible to do this using only a few documents. In this particular case i'd only have a few whitepapers to work with (one per country per year) \n Open to any creative ideas!\n    submitted by    /u/LDM-88  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1amag5r/nlp_whitepaper_comparison_modelling_ideas/",
          "publishedOn": "2024-02-09T00:08:18.000Z",
          "wordCount": null,
          "title": "NLP whitepaper comparison modelling ideas",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1also9r/opinions_on_nanodl_a_new_library_for_developing/",
          "author": null,
          "description": "Hey guys, I just published the developer version of NanoDL, a library for developing transformer models within the Jax/Flax ecosystem and would love your feedback!\n Key Features of NanoDL include:\n A wide array of blocks and layers, facilitating the creation of customised transformer models from scratch.\n An extensive selection of models like LlaMa2, Mistral, Mixtral, GPT3, GPT4 (inferred), T5, Whisper, ViT, Mixers, GAT, CLIP, and more, catering to a variety of tasks and applications.\n Data-parallel distributed trainers so developers can efficiently train large-scale models on multiple GPUs or TPUs, without the need for manual training loops.\n Dataloaders, making the process of data handling for Jax/Flax more straightforward and effective.\n Custom layers not found in Flax/Jax, such as RoPE, GQA, MQA, and SWin attention, allowing for more flexible model development.\n GPU/TPU-accelerated classical ML models like PCA, KMeans, Regression, Gaussian Processes etc., akin to SciKit Learn on GPU.\n Modular design so users can blend elements from various models, such as GPT, Mixtral, and LlaMa2, to craft unique hybrid transformer models.\n A range of advanced algorithms for NLP and computer vision tasks, such as Gaussian Blur, BLEU etc.\n Each model is contained in a single file with no external dependencies, so the source code can also be easily used.\n Checkout the repository for sample usage and more details: https://github.com/HMUNACHI/nanodl\n Ultimately, I want as many opinions as possible, next steps to consider, issues, even contributions.\n Note: I am working on the readme docs. For now, in the source codes, I include a comprehensive example on top of each model file in comments.\n    submitted by    /u/Henrie_the_dreamer  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1also9r/opinions_on_nanodl_a_new_library_for_developing/",
          "publishedOn": "2024-02-08T10:25:02.000Z",
          "wordCount": null,
          "title": "Opinions On NanoDL, A New Library For Developing Transformers From Scratch.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1alqwpb/what_specific_problems_in_what_domains_and_fields/",
          "author": null,
          "description": "In my understanding, there are two types of approaches in NLP: rule-based and statistic-based. Rule-based approach is simple, understandable and need not training, while statistic-based is better if the rules are complex and you have good training data.\n What domains, fields or industries have the need to use rule-based approach classification problems? I think there should be a review on how this technique is applied in various field, but I can't find one.\n My goal: I develop a rule-based classification tool for my language and now I'm looking for potential users. I want to know their needs and the insights of their fields.\n    submitted by    /u/Ooker777  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1alqwpb/what_specific_problems_in_what_domains_and_fields/",
          "publishedOn": "2024-02-08T08:17:19.000Z",
          "wordCount": null,
          "title": "What specific problems in what domains and fields have the need to use rule-based text classification?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1al8elm/how_to_detect_bad_data_in_your_instruction_tuning/",
          "author": null,
          "description": "Hello Redditors!\n I've spent some time looking at instruction-tuning (aka LLM Alignment / Fine-Tuning) datasets and I've found that they inevitably have bad data lurking within them. This is often what’s preventing LLMs to go from demo to production, not more parameters/GPUs… However, bad instruction-response data is hard to detect manually.\n Applying our techniques below to the famous dolly-15k dataset immediately reveals all sorts of issues in this dataset (even though it was carefully curated by over 5000 employees): responses that are inaccurate, unhelpful, or poorly written, incomplete/vague instructions, and other sorts of bad language (toxic, PII, …)\n Data auto-detected to be bad can be filtered from the dataset or manually corrected. This is the fastest way to improve the quality of your existing instruction tuning data and your LLMs!\n Feel free to check out the code on Github to reproduce these findings or read more details here in our article which demonstrates automated techniques to catch low-quality data in any instruction tuning dataset.\n    submitted by    /u/cmauck10  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1al8elm/how_to_detect_bad_data_in_your_instruction_tuning/",
          "publishedOn": "2024-02-07T17:29:05.000Z",
          "wordCount": null,
          "title": "How to detect bad data in your instruction tuning dataset (for better LLM fine-tuning)",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1aki4o8/natural_fluency_polyglot_survey/",
          "author": null,
          "description": "submitted by    /u/SlavXyy  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1aki4o8/natural_fluency_polyglot_survey/",
          "publishedOn": "2024-02-06T19:25:35.000Z",
          "wordCount": null,
          "title": "Natural Fluency Polyglot Survey",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/LanguageTechnology/comments/1ajqvdd/most_impressive_nlp_minds/",
          "author": null,
          "description": "Hi! I'm sorry if this has already been asked, and if it has kindly point me in the direction of that post and I would be most appreciative.\n ​\n My question: as people who know a lot more about the NLP landscape than I do, who are the people you consider to be the most impressive minds in the field and why?\n ​\n Any opinions would be amazing. Thank you!\n    submitted by    /u/meltymeems  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/LanguageTechnology/comments/1ajqvdd/most_impressive_nlp_minds/",
          "publishedOn": "2024-02-05T20:37:25.000Z",
          "wordCount": null,
          "title": "Most Impressive NLP Minds",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Statistical Modeling, Causal Inference, and Social Science",
      "feedUrl": "https://statmodeling.stat.columbia.edu/feed/",
      "siteUrl": "https://statmodeling.stat.columbia.edu",
      "articles": [
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49548",
          "author": "Andrew",
          "description": "Econ Journal Watch asked me and some others to contribute to an article, “What are your most underappreciated works?,” where each of us wrote 200 words or less about an article of ours that had received few citations. Here’s what … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/13/i-love-this-paper-but-its-barely-been-noticed/",
          "publishedOn": "2024-02-13T14:57:14.000Z",
          "wordCount": 5578,
          "title": "I love this paper but it’s barely been noticed.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50145",
          "author": "Andrew",
          "description": "The above figures come from this article which is listed on this Orcid page (with further background here): Horrifying as all this is, at least from the standpoint of students and faculty at the University of Nevada, not to mention … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/12/torment-executioners-in-reno-nevada/",
          "publishedOn": "2024-02-12T14:55:05.000Z",
          "wordCount": null,
          "title": "Torment executioners in Reno, Nevada, keep tormenting us with their publications.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48813",
          "author": "Andrew",
          "description": "Mark Palko points us to a recent update by Robert Yeh et al. of the famous randomized parachute-jumping trial: Palko writes: I also love the way they dot all the i’s and cross all the t’s. The whole thing is … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/11/clinical-trials-that-are-designed-to-fail/",
          "publishedOn": "2024-02-11T14:38:34.000Z",
          "wordCount": 5157,
          "title": "Clinical trials that are designed to fail",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48826",
          "author": "Andrew",
          "description": "A question came up about the effects of school funding and student performance, and we were referred to this review article from a few years ago by Larry Hedges, Terri Pigott, Joshua Polanin, Ann Marie Ryan, Charles Tocci, and Ryan … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/10/if-school-funding-doesnt-really-matter-why-does-everyone-want-their-kid-to-go-to-a-well-funded-school/",
          "publishedOn": "2024-02-10T14:59:08.000Z",
          "wordCount": null,
          "title": "If school funding doesn’t really matter, why do people want their kid’s school to be well funded?",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50122",
          "author": "Andrew",
          "description": "Remember that absolutely ridiculous claim that scientific citations are worth $100,000 each? It appears that someone is taking this literally. Or, nearly so. Nick Wise has the story: A couple of months ago a professor received the following email, which … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/09/hey-heres-some-free-money-for-you-just-lend-your-name-to-these-universities-and-theyll-pay-you-1000-for-every-article-you-publish/",
          "publishedOn": "2024-02-09T14:37:05.000Z",
          "wordCount": 4842,
          "title": "Hey, here’s some free money for you!  Just lend your name to this university and they’ll pay you $1000 for every article you publish!",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50141",
          "author": "Jessica Hullman",
          "description": "This is Jessica. Speaking of data sonification (or sensification), Hyeok, Yea Seul Kim, and I write:  Data sonification-mapping data variables to auditory variables, such as pitch or volume-is used for data accessibility, scientific exploration, and data-driven art (e.g., museum exhibitions) … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/08/listen-to-those-residuals/",
          "publishedOn": "2024-02-08T17:55:20.000Z",
          "wordCount": null,
          "title": "Listen to those residuals",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50112",
          "author": "Andrew",
          "description": "Monday, February 12, 2024, 12:00pm to 1:15pm Social penumbras predict political attitudes The political influence of a group is typically explained in terms of its size, geographic concentration, or the wealth and power of the group’s members. This article introduces … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/08/social-penumbras-predict-political-attitudes-my-talk-at-harvard-on-monday-feb-12-at-noon/",
          "publishedOn": "2024-02-08T14:56:13.000Z",
          "wordCount": null,
          "title": "Social penumbras predict political attitudes (my talk at Harvard on Monday Feb 12 at noon)",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50137",
          "author": "Aki Vehtari",
          "description": "Osvaldo Martin writes: The third edition of Bayesian Analysis with Python serves as an introduction to the basic concepts of applied Bayesian modeling. It adopts a hands-on approach, guiding you through the process of building, exploring and expanding models using … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/08/bayesian-analysis-with-python/",
          "publishedOn": "2024-02-08T12:46:55.000Z",
          "wordCount": null,
          "title": "Bayesian Analysis with Python",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50131",
          "author": "Bob Carpenter",
          "description": "Another way of saying this is that you should treat inline code comments as a last resort when there is no other way to make your intentions clear. I used to teach a session of Andrew’s statistical communication class once … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/07/when-all-else-fails-add-a-code-comment/",
          "publishedOn": "2024-02-07T20:00:32.000Z",
          "wordCount": null,
          "title": "When all else fails, add a code comment",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50106",
          "author": "Andrew",
          "description": "This one’s important. Matt Lerner points us to this report by Rosie Bettle, Replicability & Generalisability: A Guide to CEA discounts. “CEA” is cost-effectiveness analysis, and by “discounts” they mean what we’ve called the Edlin factor—“discount” is a better name … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/07/replicability-generalisability-applying-a-discount-factor-to-cost-effectiveness-estimates/",
          "publishedOn": "2024-02-07T14:54:22.000Z",
          "wordCount": null,
          "title": "“Replicability & Generalisability”:  Applying a discount factor to cost-effectiveness estimates.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50133",
          "author": "Lizzie",
          "description": "… Or not, according to what language is allowed. At the start of the year I mentioned that I am on a bad roll with AI just now, and the start of that roll began in late November when I … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/06/ive-been-mistaken-for-a-chatbot/",
          "publishedOn": "2024-02-07T04:07:06.000Z",
          "wordCount": 9031,
          "title": "I’ve been mistaken for a chatbot",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50113",
          "author": "Andrew",
          "description": "“As we look to sleep and neuroscience for answers we can study flies specifically the Drosophila melanogaster we highlight in our research.” 1. The story Someone writes: I recently read a paper of yours in the Chronicle about how academic … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/06/its-bezzle-time-the-dean-of-engineering-at-the-university-of-nevada-gets-paid-372127-a-year-and-wrote-a-paper-thats-so-bad-you-cant-believe-it-i-mean-really-you-have-to-take-a-look-at-t/",
          "publishedOn": "2024-02-06T14:12:08.000Z",
          "wordCount": 25615,
          "title": "It’s bezzle time:  The Dean of Engineering at the University of Nevada gets paid $372,127 a year and wrote a paper that’s so bad, you can’t believe it.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50128",
          "author": "Lizzie",
          "description": "It’s back! As regular readers know, the Cherry Blossom Prediction Competition will run throughout February 2024. We challenge you to predict the bloom date of cherry trees at five locations throughout the world and win prizes. We’ve been promoting the … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/05/cherry-blossoms-not-just-another-prediction-competition/",
          "publishedOn": "2024-02-05T22:49:20.000Z",
          "wordCount": 3582,
          "title": "Cherry blossoms—not just another prediction competition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50126",
          "author": "Andrew",
          "description": "The other day we linked to a study whose purpose was to “investigate challenges faced by curators of data visualizations for blind and low-vision individuals.” JooYoung Seo, the organizer of that project, provides further background: With the exception of a … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/05/using-the-term-visualization-for-non-visual-representation-of-data/",
          "publishedOn": "2024-02-05T22:06:44.000Z",
          "wordCount": null,
          "title": "Using the term “visualization” for non-visual representation of data",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49037",
          "author": "Andrew",
          "description": "Toby Ord writes: I think you will like this short proof that puts a lower bound on the probability that one’s vote is decisive. It requires just one assumption (that the probability distribution over vote share is unimodal) and takes … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/05/a-new-argument-estimating-the-probability-that-your-vote-will-be-decisive/",
          "publishedOn": "2024-02-05T14:40:52.000Z",
          "wordCount": null,
          "title": "A new argument for estimating the probability that your vote will be decisive",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50120",
          "author": "Bob Carpenter",
          "description": "I started thinking a bit more about what we’re doing when we use something like posteriordb (from Stan) or Inference Gym (from TensorFlow Probability) to evaluate a sampler. posteriordb gives you Stan programs and 10,000 reference draws from their posterior. … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/04/evaluating-samplers-with-reference-draws/",
          "publishedOn": "2024-02-04T20:00:37.000Z",
          "wordCount": null,
          "title": "Evaluating samplers with reference draws",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49712",
          "author": "Andrew",
          "description": "I was dealing with some trolling in blog comments awhile ago and someone sent me an email of support, appreciating my patience in my responses to the troll. The standard advice is “Don’t feed the trolls,” but usually here it … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/04/dont-feed-the-trolls-and-the-troll-semi-bluff/",
          "publishedOn": "2024-02-04T14:17:51.000Z",
          "wordCount": 7445,
          "title": "“Don’t feed the trolls” and the troll semi-bluff",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48802",
          "author": "Andrew",
          "description": "Andy Solow writes: I wonder if you can help me with a question that has been bugging me for a while? I have been thinking about Lindley’s supra Bayesian method for expert probability assessment. Briefly, the model is that, conditional … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/03/a-question-about-lindleys-supra-bayesian-method-for-expert-probability-assessment/",
          "publishedOn": "2024-02-03T14:27:53.000Z",
          "wordCount": null,
          "title": "A question about Lindley’s supra Bayesian method for expert probability assessment",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49268",
          "author": "Andrew",
          "description": "Jonathan Falk came across this article and writes: Is there any possible weaker conclusion than “providing caloric information may help some adults with food decisions”? Is there any possible dataset which would contradict that conclusion? On one hand, gotta give … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/02/participants-reported-being-hungrier-when-they-walked-into-the-cafe-mean-7-38-sd-2-20-than-when-they-walked-out-mean-1-53-sd-2-70-f1-75-107-68-p-0-001/",
          "publishedOn": "2024-02-02T14:46:19.000Z",
          "wordCount": null,
          "title": "“Participants reported being hungrier when they walked into the café (mean = 7.38, SD = 2.20) than when they walked out [mean = 1.53, SD = 2.70, F(1, 75) = 107.68, P < 0.001].\"",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50107",
          "author": "Andrew",
          "description": "Here’s the survey, and here’s what it says: The purpose of this study is to investigate challenges faced by curators of data visualizations for blind and low-vision individuals. This includes, but is not limited to, graphs, charts, plots, diagrams, and … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/01/click-here-to-help-this-researcher-gather-different-takes-on-making-data-visualizations-for-blind-people/",
          "publishedOn": "2024-02-02T01:37:16.000Z",
          "wordCount": 3119,
          "title": "Click here to help this researcher gather different takes on making data visualizations for blind people",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49651",
          "author": "Andrew",
          "description": "Retraction Watch points to this fun article by Ashley Rindsberg, “The Lancet was made for political activism,” subtitled, For 200 years, it has thrived on melodrama and scandal. And they didn’t even mention Surgisphere (for more detail, see here) or … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/02/01/lancet-bashing/",
          "publishedOn": "2024-02-01T14:20:53.000Z",
          "wordCount": null,
          "title": "Lancet-bashing!",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49647",
          "author": "Andrew",
          "description": "Following the recommendation of Elin in comments, I checked out the podcast, If Books Could Kill. It seemed like the kinda thing I might like: 2 guys going back and forth taking apart Gladwell, Freakonomics, David Brooks, Nudge, and other … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/31/opposition/",
          "publishedOn": "2024-01-31T14:53:52.000Z",
          "wordCount": 8780,
          "title": "Opposition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50096",
          "author": "Jessica Hullman",
          "description": "This is Jessica. A while back on the blog I shared some opinions about studies of human-decision making, such as to understand how visualizations or displays of model predictions and explanations impact people’s behavior. My view is essentially that a … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/30/minimum-criteria-for-studies-evaluating-human-decision-making/",
          "publishedOn": "2024-01-30T18:45:59.000Z",
          "wordCount": null,
          "title": "Minimum criteria for studies evaluating human decision-making",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50065",
          "author": "Andrew",
          "description": "A message came in my inbox from “The RetractoBot Team, University of Oxford,” with subject line, “RetractoBot: You cited a retracted paper”: That’s funny! When we cited that paper by Lacour and Green, we already knew it was no good. … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/30/retractobot-you-cited-a-retracted-paper/",
          "publishedOn": "2024-01-30T14:42:52.000Z",
          "wordCount": null,
          "title": "Hey, I got tagged by RetractoBot!",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50094",
          "author": "Bob Carpenter",
          "description": "Rasmuth Bååth reports the following fun story in a blog post, The source of the cake dataset (it’s a hierarchical modeling example included with the R package lme4). Rasmuth writes, While looking for a dataset to illustrate a simple hierarchical … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/29/fun-with-daata-reference-librarian/",
          "publishedOn": "2024-01-29T19:56:26.000Z",
          "wordCount": null,
          "title": "Fun with Dååta: Reference librarian edition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48180",
          "author": "Andrew",
          "description": "Alexey Guzey asks: How much have you thought about AI and when will AI be able to do scientific research both cheaper and better than us, thus effectively obsoleting humans? My first reply: I guess that AI can already do … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/29/when-will-ai-be-able-to-do-scientific-research-both-cheaper-and-better-than-us-thus-effectively-obsoleting-humans/",
          "publishedOn": "2024-01-29T14:59:38.000Z",
          "wordCount": null,
          "title": "“When will AI be able to do scientific research both cheaper and better than us, thus effectively obsoleting humans?”",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48933",
          "author": "Andrew",
          "description": "A few months ago I wrote about some disturbing stuff I’d been hearing about from Harvard Law School professors Cass Sunstein and Adrian Vermeuele. The two of them wrote an article back in 2005 writing, “a refusal to impose [the … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/28/sy/",
          "publishedOn": "2024-01-28T14:04:58.000Z",
          "wordCount": null,
          "title": "Sympathy for the Nudgelords:  Vermeule endorsing stupid and dangerous election-fraud claims and Levitt promoting climate change denial are like cool dudes in the 60s wearing Che T-shirts and thinking Chairman Mao was cool—we think they’re playing with fire, they think they’re cute contrarians pointing out contradictions in the system.  For a certain kind of person, it’s fun to be a rogue.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49625",
          "author": "Andrew",
          "description": "Benjamin Kircup writes: I think you will be very interested to see this preprint that is making the rounds: Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology (ecoevorxiv.org) I see several … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/27/pt/",
          "publishedOn": "2024-01-27T14:04:58.000Z",
          "wordCount": null,
          "title": "The paradox of replication studies:  A good analyst has special data analysis and interpretation skills.  But it’s considered a bad or surprising thing that if you give the same data to different analysts, they come to different conclusions.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50072",
          "author": "Jessica Hullman",
          "description": "Eytan Adar pointed me to a video, Reasoning Under Uncertainty, from the historical ACM SIGCHI (computer-human interaction) Video Project. Beyond fashionable hairstyles, it demos interfaces from a software curriculum to teach high school students statistical reasoning in 1989. They’ve got … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/26/they-solved-the-human-statistical-reasoning-interface-back-in-the-80s/",
          "publishedOn": "2024-01-26T18:59:05.000Z",
          "wordCount": 3467,
          "title": "They solved the human-statistical reasoning interface back in the 80s",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49259",
          "author": "Andrew",
          "description": "Jacob Klerman writes: I have noted your recent emphasis on the importance of measurement (e.g., “Here are some ways to make your study replicable…”). For reasons not relevant here, I was rereading Leamer (1983), Let’s Take the Con Out of … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/26/the-importance-of-measurement-and-how-you-can-draw-ridiculous-conclusions-from-your-statistical-analyses-if-you-dont-think-carefully-about-measurement-leamer-1983-got-it/",
          "publishedOn": "2024-01-26T14:44:22.000Z",
          "wordCount": null,
          "title": "The importance of measurement, and how you can draw ridiculous conclusions from your statistical analyses if you don’t think carefully about measurement . . . Leamer (1983) got it.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=48560",
          "author": "Andrew",
          "description": "Prabhat Barnwal, Yuling Yao, Yiqian Wang, Nishat Akter Juy, Shabib Raihan, Mohammad Ashraful Haque, and Alexander van Geen ask, Is the low COVID-19–related mortality reported in Bangladesh for 2020 associated with massive undercounting? Here’s what they did: This repeated survey … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/25/mister-p-and-stan-go-to-bangladesh/",
          "publishedOn": "2024-01-25T14:22:41.000Z",
          "wordCount": 10569,
          "title": "Mister P and Stan go to Bangladesh . . .",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49121",
          "author": "Andrew",
          "description": "Art Owen informed me that he’ll be teaching sampling again at Stanford, and he was wondering about ideas for students gathering their own data. I replied that I like the idea of sampling from databases, biological sampling, etc. You can … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/24/resources-for-teaching-and-learning-survey-sampling-from-scott-keeter-at-pew-research/",
          "publishedOn": "2024-01-24T14:46:47.000Z",
          "wordCount": 6166,
          "title": "Resources for teaching and learning survey sampling, from Scott Keeter at Pew Research",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50027",
          "author": "Leonardo Egidi",
          "description": "Following Andrew, Aki, Jessica, and Charles, and based on Andrew’s proposal, I list my research contributions for 2023. Published: Egidi, L. (2023). Seconder of the vote of thanks to Narayanan, Kosmidis, and Dellaportas and contribution to the Discussion of ‘Flexible … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/23/progress-in-2023-leo-edition/",
          "publishedOn": "2024-01-23T23:00:39.000Z",
          "wordCount": null,
          "title": "Progress in 2023, Leo edition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50058",
          "author": "Andrew",
          "description": "Just a reminder: we have a new weekly newsletter. We posted on it a couple weeks ago; I’m just giving a reminder here because the goal of the newsletter is to reach people who wouldn’t otherwise go online to read … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/23/heres-how-to-get-our-new-weekly-newsletter/",
          "publishedOn": "2024-01-23T21:40:59.000Z",
          "wordCount": 2847,
          "title": "Here’s how to subscribe to our new weekly newsletter:",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50020",
          "author": "Andrew",
          "description": "Here’s the link: Learning from mistakes Andrew Gelman, Department of Statistics and Department of Political Science, Columbia University We learn so much from mistakes! How can we structure our workflow so that we can learn from mistakes more effectively? I … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/23/learning-from-mistakes-my-online-talk-for-the-american-statistical-association-230pm-tues-30-jan-2024/",
          "publishedOn": "2024-01-23T14:25:24.000Z",
          "wordCount": 4765,
          "title": "Learning from mistakes (my online talk for the American Statistical Association, 2:30pm Tues 30 Jan 2024)",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50056",
          "author": "Eric Novik",
          "description": "This is Eric. Brian Parbhu took over the Bayesian Data Analysis meetup (formerly NYC Stan Users Group), and he is running an event in NYC this Friday, during which Brian Ward is going to discuss BridgeStan, the new in-memory interface … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/22/intro-to-bridgestan-the-new-in-memory-interface-for-stan/",
          "publishedOn": "2024-01-22T20:07:52.000Z",
          "wordCount": null,
          "title": "Intro to BridgeStan: The new in-memory interface for Stan",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49596",
          "author": "Andrew",
          "description": "Economic historian Tim Guinnane writes: I have a general question that I have not seen addressed on your blog. Often this question turns into a narrow question about retracting papers, but I think that short-circuits an important discussion. Like many … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/22/my-view-is-that-if-i-can-show-that-a-result-was-cooked-and-that-doing-it-correctly-does-not-yield-the-answer-the-authors-claimed-then-the-result-is-discredited-what-i-hear-instead-is-the-f/",
          "publishedOn": "2024-01-22T14:16:09.000Z",
          "wordCount": 5191,
          "title": "“My view is that if I can show that a result was cooked and that doing it correctly does not yield the answer the authors claimed, then the result is discredited. . . . What I hear, instead, is the following . . .”",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50047",
          "author": "Andrew",
          "description": "Michael Wiebe writes: I have several new replications written up on my site. Moretti (2021) studies whether larger cities drive more innovation, but I find that the event study and instrumental variable results are due to coding errors. This means … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/21/michael-wiebe-has-several-new-replications-written-up-on-his-site/",
          "publishedOn": "2024-01-22T02:02:08.000Z",
          "wordCount": 3508,
          "title": "Michael Wiebe has several new replications written up on his site.",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49597",
          "author": "Andrew",
          "description": "Chris Barker, the chair of the Statistical Consulting Section of the American Statistical Association, writes: I’m curious about your reaction/opinion to a Financial times article I read today about Sam Bankman-Fried (“SBF,” charged with fraud in the loss several billion … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/21/whats-the-problem-math-snobs-or-rich-dudes-who-take-themselves-too-seriously-and-are-enabled-in-that-by-the-news-media/",
          "publishedOn": "2024-01-21T14:00:46.000Z",
          "wordCount": 9568,
          "title": "What’s the problem, “math snobs” or rich dudes who take themselves too seriously and are enabled in that by the news media?",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49235",
          "author": "Andrew",
          "description": "I’ve often appealed to “common sense” or “face validity” when considering unusual research claims. For example, the statement that single women during certain times of the month were 20 percentage points more likely to support Barack Obama, or the claim … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/20/regarding-the-use-of-common-sense-when-evaluating-research-claims/",
          "publishedOn": "2024-01-20T14:38:22.000Z",
          "wordCount": null,
          "title": "Regarding the use of “common sense” when evaluating research claims",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50030",
          "author": "Charles Margossian",
          "description": "Following the examples of Andrew, Aki, and Jessica, and at Andrew’s request: Published: Variational Inference with Gaussian Score Matching. Neural Information Processing Systems. (Chirag Modi, CM, Yuling Yao, Robert Gower, David Blei and Lawrence Saul) The Shrinkage-Delinkage Trade-off: An Analysis … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/19/progress-in-2023-charles-edition/",
          "publishedOn": "2024-01-19T20:00:15.000Z",
          "wordCount": 3038,
          "title": "Progress in 2023, Charles edition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49593",
          "author": "Andrew",
          "description": "Jonathan “no Trump” Falk points to this press release and writes: Scientist, after decades of study, concludes: We don’t have free will. Does that include the decision to write a book about free will? PS … A quick mention of … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/19/the-free-will-to-repost/",
          "publishedOn": "2024-01-19T14:07:21.000Z",
          "wordCount": null,
          "title": "The free will to repost",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50024",
          "author": "Andrew",
          "description": "This looks potentially important: The Center for Interdisciplinary Statistical Education and Research (CISER) at Washington State University (WSU) is excited to announce that it has an opening for a Post-Doctoral Research Associate (statistical scientist) supporting a new state-wide public data … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/18/postdoc-at-washington-state-university-on-law-enforcement-statistics/",
          "publishedOn": "2024-01-18T22:49:21.000Z",
          "wordCount": 2926,
          "title": "Postdoc at Washington State University on law-enforcement statistics",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49977",
          "author": "Andrew",
          "description": "Storytelling and Scientific Understanding Andrew Gelman and Thomas Basbøll Storytelling is central to science, not just as a tool for broadcasting scientific findings to the outside world, but also as a way that we as scientists understand and evaluate theories. … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/18/storytelling-and-scientific-understanding-my-talks-with-thomas-basboll-at-johns-hopkins-on-26-apr/",
          "publishedOn": "2024-01-18T14:28:49.000Z",
          "wordCount": 3920,
          "title": "Storytelling and Scientific Understanding (my talks with Thomas Basbøll at Johns Hopkins on 26 Apr)",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50018",
          "author": "Aki Vehtari",
          "description": "Andrew, I, and Jessica (and I hope we get more) listed papers for progress in 2023, but many papers would be much less useful without software, so I list also software I’m contributing to with the most interesting improvements added … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/18/progress-in-2023-akis-software-edition/",
          "publishedOn": "2024-01-18T11:05:56.000Z",
          "wordCount": null,
          "title": "Progress in 2023, Aki’s software edition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49464",
          "author": "Andrew",
          "description": "I knew the Association for Psychological Science, the American Psychological Association, the American Political Science Association, the American Statistical Association, and the National Academy of Sciences had problems. It turns out the American Sociological Association does some bad things too. … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/17/bad-stuff-going-down-at-the-american-sociological-association/",
          "publishedOn": "2024-01-17T14:01:30.000Z",
          "wordCount": null,
          "title": "Bad stuff going down at the American Sociological Association",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=50015",
          "author": "Jessica Hullman",
          "description": "Since Aki and Andrew are doing it…  Published: Dongping Zhang, Jason Hartline, and Jessica Hullman (2024). Designing Shared Information Displays for Agents of Varying Strategic Sophistication. ACM Transactions on Computer-Supported Cooperative Work (CSCW). Yifan Wu, Ziyang Guo, Michalis Mamakos, Jason … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/16/progress-in-2023-jessica-edition/",
          "publishedOn": "2024-01-16T19:10:39.000Z",
          "wordCount": 6192,
          "title": "Progress in 2023, Jessica Edition",
          "imageUrl": null
        },
        {
          "id": "https://statmodeling.stat.columbia.edu/?p=49577",
          "author": "Andrew",
          "description": "I just read this magazine article by Nikhil Krishnan on the philosophy of Aristotle. As a former physics student, I’ve never had anything but disdain for that ancient philosopher, who’s famous for getting just about everything in physics wrong, as … Continue reading →",
          "link": "https://statmodeling.stat.columbia.edu/2024/01/16/this-post-is-not-really-about-aristotle/",
          "publishedOn": "2024-01-16T14:56:20.000Z",
          "wordCount": null,
          "title": "This post is not really about Aristotle.",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "John D. Cook",
      "feedUrl": "https://www.johndcook.com/blog/feed/",
      "siteUrl": "https://www.johndcook.com/blog",
      "articles": [
        {
          "id": "https://www.johndcook.com/blog/?p=242879",
          "author": "John",
          "description": "A few days ago I wrote about the privacy implications of metadata in a PDF. This post will do the same for photos. You can see the metadata in a photo using exiftool. By default cameras include time and location data. I ran this tool on a photo I took in Seattle a few years […]\nHow much metadata is in a photo? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/13/photo-metadata/",
          "publishedOn": "2024-02-13T15:44:47.000Z",
          "wordCount": 1560,
          "title": "How much metadata is in a photo?",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242881",
          "author": "John",
          "description": "The Borwein integrals introduced in [1] are a famous example of how proof-by-example can go wrong. Define sinc(x) as sin(x)/x. Then the following equations hold. However where δ ≈ 2.3 × 10−11. This is where many presentations end, concluding with the moral that a pattern can hold for a while and then stop. But I’d […]\nThe Borwein integrals first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/13/borwein-integrals/",
          "publishedOn": "2024-02-13T12:56:42.000Z",
          "wordCount": 1450,
          "title": "The Borwein integrals",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242801",
          "author": "Wayne Joubert",
          "description": "Suppose you have two Linux processes trying to modify a file at the same time and you don’t want them stepping on each other’s work and making a mess.  A common solution is to use a “lock” mechanism (a.k.a. “mutex”). One process “locks the lock” and by this action has sole ownership of a […]\nAvoiding Multiprocessing Errors in Bash Shell first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/12/avoiding-multiprocessing-errors-in-bash-shell/",
          "publishedOn": "2024-02-12T13:03:53.000Z",
          "wordCount": 2124,
          "title": "Avoiding Multiprocessing Errors in Bash Shell",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242847",
          "author": "John",
          "description": "I was looking today at a cardboard box that had the “this way up” symbol on it and wondered whether there is a Unicode value for it. Apparently not. But there is an ISO code for it: ISO 7000 symbol 0623. It’s an international standard symbol for indicating how to orient a package. The name […]\nThis-way-up and Knuth arrows first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/10/this-way-up-and-knuth-arrows/",
          "publishedOn": "2024-02-10T22:32:03.000Z",
          "wordCount": 1571,
          "title": "This-way-up and Knuth arrows",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242839",
          "author": "John",
          "description": "Fermat’s little theorem says that if p is a prime number, then for any positive integer b < p we hve bp-1 = 1 (mod p). This theorem gives a necessary but not sufficient condition for a number to be prime. Fermat’s primality test The converse of Fermat’s little theorem is not always true, but […]\nFactoring pseudoprimes first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/10/factoring-pseudoprimes/",
          "publishedOn": "2024-02-10T15:45:25.000Z",
          "wordCount": 1987,
          "title": "Factoring pseudoprimes",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242837",
          "author": "John",
          "description": "When you add a comment to a LaTeX file, it makes no visible change to the output. The comment is ignored as far as the appearance of the file. But is that comment somehow included in the file anyway? If you compile a LaTeX file to PDF, then edit it by throwing in a comment, […]\nDo comments in a LaTeX file change the output? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/08/pdflatex-comment/",
          "publishedOn": "2024-02-08T18:31:24.000Z",
          "wordCount": 1669,
          "title": "Do comments in a LaTeX file change the output?",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242803",
          "author": "John",
          "description": "When you create a PDF file, what you see is not all you get. There is metadata embedded in the file that might be useful. It also might reveal information you’d rather not reveal. The previous post looked at just the time stamp on a file. This post will look at more metadata, focusing on […]\nYour PDF may reveal more than you intend first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/08/pdf-forensics/",
          "publishedOn": "2024-02-08T16:53:32.000Z",
          "wordCount": 2085,
          "title": "Your PDF may reveal more than you intend",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242777",
          "author": "John",
          "description": "If you save a file as a PDF twice, you won’t get exactly the same file both times. To illustrate this, I created an LibreOffice document containing “Hello world.” and saved it twice, first as humpty.pdf then as dumpty.pdf. Then I compared the two files. % diff humpty.pdf dumpty.pdf Binary files humpty.pdf and dumpty.pdf differ […]\nIf you save a file as PDF twice, you get two different files first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/08/pdf-metadata/",
          "publishedOn": "2024-02-08T13:27:28.000Z",
          "wordCount": 1683,
          "title": "If you save a file as PDF twice, you get two different files",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242706",
          "author": "Wayne Joubert",
          "description": "The popularity of low precision arithmetic for computing has exploded since the 2017 release of the Nvidia Volta GPU. The half precision tensor cores of Volta offered a massive 16X performance gain over double precision for key operations. The “race to the bottom” for lower precision computations continues: some have even solved significant problems using […]\nIs Low Precision Arithmetic Safe? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/06/is-low-precision-arithmetic-safe/",
          "publishedOn": "2024-02-06T15:20:34.000Z",
          "wordCount": 2149,
          "title": "Is Low Precision Arithmetic Safe?",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242737",
          "author": "John",
          "description": "There are many answers to the question in the title: How likely is a random variable to be far from its center? The answers depend on how much you’re willing to assume about your random variable. The more you can assume, the stronger your conclusion. The answers also depend on what you mean by “center,” […]\nHow likely is a random variable to be far from its center? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/06/chevyshev-variations/",
          "publishedOn": "2024-02-06T14:33:55.000Z",
          "wordCount": 1587,
          "title": "How likely is a random variable to be far from its center?",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242694",
          "author": "John",
          "description": "Some readers will look at the title of this post and think “Ah yes, the FFT. I use it all the time. But what is this quadratic reciprocity?” Others will look at the same title and think “Gauss called the quadratic reciprocity theorem the jewel in the crown of mathematics. But what is this FFT […]\nConnecting the FFT and quadratic reciprocity first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/03/fft-reciprocity/",
          "publishedOn": "2024-02-03T19:59:27.000Z",
          "wordCount": 1752,
          "title": "Connecting the FFT and quadratic reciprocity",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242690",
          "author": "John",
          "description": "It’s common to truncate US zip codes to the first three digits for privacy reasons. Truncating to the first two digits is less common, but occurs in some data sets. HIPAA Safe Harbor requires sparse 3-digit zip codes to be suppressed; even when rolled up to three digits some regions are still sparsely populated. How […]\nTwo-digit zip codes first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/02/01/two-digit-zip-codes/",
          "publishedOn": "2024-02-02T02:13:39.000Z",
          "wordCount": 1474,
          "title": "Two-digit zip codes",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=242204",
          "author": "John",
          "description": "Bessel functions are to polar coordinates what sines and cosines are to rectangular coordinates. This is why Bessel function often arise in applications with radial symmetry. The locations of the zeros of Bessel functions are important in application, and so you can find software for computing these zeros in mathematical libraries. In days gone by […]\nBessel zero spacing first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/31/bessel-zero-spacing/",
          "publishedOn": "2024-01-31T16:44:36.000Z",
          "wordCount": 1540,
          "title": "Bessel zero spacing",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=241794",
          "author": "John",
          "description": "Suppose we have an n × n chessboard. The case n = 8 is of course most common, but we consider all positive integer values of n. The graph of a chess piece has an edge between two squares if and only if the piece can legally move between the two squares. Now suppose we […]\nColoring the queen’s graph first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/30/coloring-the-queens-graph/",
          "publishedOn": "2024-01-30T15:04:57.000Z",
          "wordCount": 1722,
          "title": "Coloring the queen’s graph",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=241456",
          "author": "John",
          "description": "A SWIFT-BIC number identifies a bank, not a particular bank account. The BIC part stands for Bank Identifier Code. I had to look up the structure of SWIFT-BIC codes recently, and here it is: Four letters to identify the bank Two letters to identify the country Two letters or digits to identify the location Optionally, […]\nRegex to match SWIFT-BIC codes first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/29/swift/",
          "publishedOn": "2024-01-29T19:47:52.000Z",
          "wordCount": 1938,
          "title": "Regex to match SWIFT-BIC codes",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=240612",
          "author": "John",
          "description": "I just finished reading The Three Body Problem. At the end of the book is a preview of Cixin Liu’s book Supernova Era. A bit of dialog in that preview stood out to me because it is touches on themes I’ve written about before. “I’ve heard about that. When a butterfly flaps its wings, there’s […]\nBad takes on chaos theory first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/27/butterflies-dont-work-that-way/",
          "publishedOn": "2024-01-27T16:42:00.000Z",
          "wordCount": 1598,
          "title": "Bad takes on chaos theory",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=236641",
          "author": "Wayne Joubert",
          "description": "The news from Meta last week is a vivid reminder of the importance of making code run faster and more power-efficiently. Meta intends to purchase 350,000 Nvidia H100 GPUs this year [1]. Assuming 350W TDP [2] and $0.1621 per kW-h [3] average US energy cost, one expects a figure of $174 million per year in […]\nNew Ways To Make Code Run Faster first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/26/making-code-run-faster/",
          "publishedOn": "2024-01-26T18:25:11.000Z",
          "wordCount": 1793,
          "title": "New Ways To Make Code Run Faster",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=239550",
          "author": "John",
          "description": "A naive view of simple substitution ciphers is that they are secure because there are 26! ways to permute the English alphabet, and so an attacker would have to try 26! ≈ 4 × 1026 permutations. However, such brute force is not required. In practice, simple substitution ciphers are breakable by hand in a few […]\nBrute force cryptanalysis first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/25/brute-force-cryptanalysis/",
          "publishedOn": "2024-01-25T15:08:34.000Z",
          "wordCount": 1754,
          "title": "Brute force cryptanalysis",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=239523",
          "author": "John",
          "description": "Introduction Computers fundamentally changed cryptography, opening up new possibilities for making and breaking codes. At first it may not have been clear which side benefited most, but now it’s clear that computers gave more power to code makers than code breakers. We now have cryptographic primitives that cannot be attacked more efficiently than by brute […]\nStraddling checkerboard encryption first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/25/straddling-checkerboard-encryption/",
          "publishedOn": "2024-01-25T12:10:40.000Z",
          "wordCount": 1914,
          "title": "Straddling checkerboard encryption",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=238592",
          "author": "John",
          "description": "I will soon be discontinuing the email subscription option for this blog. I recommend that email subscribers switch over to subscribing to the RSS feed for the blog. If you’re unfamiliar with RSS, here is an article on how to get started. (I recommend RSS in general, and not just for subscribing to this blog. […]\nEmail subscription changes first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/22/email-subscription-changes/",
          "publishedOn": "2024-01-23T02:20:14.000Z",
          "wordCount": 1367,
          "title": "Email subscription changes",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=237632",
          "author": "John",
          "description": "I was thinking about the work I did when I worked in biostatistics at MD Anderson. This work was practical rather than mathematically elegant, useful in its time but not of long-term interest. However, one result came out of this work that I would call elegant, and that was a symmetry I found. Let X […]\nBeta inequality symmetries first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/20/beta-inequality-symmetries/",
          "publishedOn": "2024-01-20T13:45:08.000Z",
          "wordCount": 1531,
          "title": "Beta inequality symmetries",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=236814",
          "author": "John",
          "description": "Given a function f(x, y), how can you tell whether f can be factored into the product of a function g(x) of x alone and a function h(y) of y alone? Depending on how an expression for f is written, it may or may not be obvious whether f(x, y) can be separated into g(x) h(y). There […]\nWhen is a function of two variables separable? first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/18/separable-function/",
          "publishedOn": "2024-01-18T13:15:39.000Z",
          "wordCount": 1521,
          "title": "When is a function of two variables separable?",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=236796",
          "author": "John",
          "description": "When a nonlinear first order ordinary differential equation has the form with n ≠ 1, the change of variables turns the equation into a linear equation in u. The equation is known as Bernoulli’s equation, though Leibniz came up with the same technique. Apparently the history is complicated [1]. It’s nice that Bernoulli’s equation can […]\nApplications of Bernoulli differential equations first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/17/bernoulli-ode/",
          "publishedOn": "2024-01-18T02:45:24.000Z",
          "wordCount": 1442,
          "title": "Applications of Bernoulli differential equations",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=233129",
          "author": "Wayne Joubert",
          "description": "Large language models have recently achieved remarkable test scores on well-known academic and professional exams (see, e.g., [1], p. 6). On such tests, these models are at times said to reach human-level performance. However, there is one test that humans can pass but every AI method known to have been tried has abysmally failed. The […]\nThe IQ Test That AI Can’t Pass first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/16/the-iq-test-ai-cant-pass/",
          "publishedOn": "2024-01-16T17:59:42.000Z",
          "wordCount": 1615,
          "title": "The IQ Test That AI Can’t Pass",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=236099",
          "author": "John",
          "description": "I’ve started a new Twitter account: @CryptographyTip. The icon for the account is the symbol for XOR, a common operation in encryption. I intend to post about cryptography theory as well as practical matters such as software and file formats. You can find a list of my other technical twitter accounts here. You can also […]\nNew Twitter account for cryptography first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/16/new-twitter-account-for-cryptography/",
          "publishedOn": "2024-01-16T13:39:30.000Z",
          "wordCount": 1294,
          "title": "New Twitter account for cryptography",
          "imageUrl": null
        },
        {
          "id": "https://www.johndcook.com/blog/?p=236043",
          "author": "John",
          "description": "The geometric, logarithmic, and arithmetic means of a and b are defined as follows. A few days ago I mentioned that G ≤ L ≤ A. The logarithmic mean slips between the geometric and arithmetic means. Or to put it another way, the logarithmic mean is bounded by the geometric and arithmetic means. You can […]\nMeans of means bounding the logarithmic mean first appeared on John D. Cook.",
          "link": "https://www.johndcook.com/blog/2024/01/16/bounding-logarithmic-mean/",
          "publishedOn": "2024-01-16T11:29:35.000Z",
          "wordCount": 1490,
          "title": "Means of means bounding the logarithmic mean",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Python⇒Speed",
      "feedUrl": "https://pythonspeed.com/atom.xml",
      "siteUrl": "https://pythonspeed.com/atom.xml",
      "articles": [
        {
          "id": "https://pythonspeed.com/articles/gpu-without-cuda/",
          "author": null,
          "description": "If you’re doing computations on a GPU, NVIDIA is the default, alongside its CUDA libraries.\nSome libraries like PyTorch support do support AMD GPUs and Macs.\nBut from the re-implementations of NumPy, SciPy, and Pandas in the RAPIDS project, to Numba’s GPU support, NVIDIA has best software support in the Python world.\nSticking to NVIDIA-specific software has some downsides, however:\nIt won’t run on modern Mac laptops.\nTesting in CI is more difficult: you need custom runners that have NVIDIA GPUs.\nYou can’t use any other GPUs you might have access to, like AMD GPUs.\nWhat can you do if you want to use GPUs in a portable manner?\nIn this article we’ll cover one option, the wgpu-py library.\nRead more...",
          "link": "https://pythonspeed.com/articles/gpu-without-cuda/",
          "publishedOn": "2024-02-13T00:00:00.000Z",
          "wordCount": 1140,
          "title": "Not just NVIDIA: GPU programming that runs everywhere",
          "imageUrl": "https://pythonspeed.com/assets/titles/gpu-without-cuda.png"
        },
        {
          "id": "https://pythonspeed.com/articles/numba-profiling/",
          "author": null,
          "description": "pre {\n    font-size: 90% !important;\n}\n\n\nIf you’re writing numeric Python code, Numba can be a great way to speed up your program.\nBy compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python.\nIn other words, it’s similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python.\nNumba code isn’t always as fast as it could be, however.\nThis is where profiling is useful: it can find at least some of the bottlenecks in your code.\nIn this article we’ll cover:\nProfila, a new profiler I’ve released that is specifically designed for Numba code.\nThe limits of profiling.\nThere are many potential performance enhancements that a profiler can’t and won’t help you discover.\nRead more...",
          "link": "https://pythonspeed.com/articles/numba-profiling/",
          "publishedOn": "2024-01-30T00:00:00.000Z",
          "wordCount": 2095,
          "title": "Profiling your Numba code",
          "imageUrl": "https://pythonspeed.com/assets/titles/numba-profiling.png"
        },
        {
          "id": "https://pythonspeed.com/articles/gpu-vs-cpu/",
          "author": null,
          "description": "Do you use NumPy, Pandas, or scikit-learn and want to get faster results?\nNvidia has created GPU-based replacements for each of these with the shared promise of extra speed.\nFor example, if you visit the front page of NVidia’s RAPIDS project, you’ll see benchmarks showing cuDF, a GPU-based Pandas replacement, is 15× to 80× faster than Pandas!\nUnfortunately, while those speed-ups are impressive, they are also misleading.\nGPU-based libraries might be the answer to your performance problems… or they might be an an unnecessary and expensive distraction.\nRead more...",
          "link": "https://pythonspeed.com/articles/gpu-vs-cpu/",
          "publishedOn": "2024-01-17T00:00:00.000Z",
          "wordCount": 923,
          "title": "Beware of misleading GPU vs CPU benchmarks",
          "imageUrl": "https://pythonspeed.com/assets/titles/gpu-vs-cpu.png"
        }
      ]
    },
    {
      "title": "Tim Dettmers",
      "feedUrl": "https://timdettmers.com/feed/",
      "siteUrl": "https://timdettmers.com/",
      "articles": []
    },
    {
      "title": "Learn Machine Learning",
      "feedUrl": "https://www.reddit.com/r/learnmachinelearning.rss",
      "siteUrl": "https://www.reddit.com/r/learnmachinelearning",
      "articles": [
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqjk16/mldata_analytics_tutors/",
          "author": null,
          "description": "Hey. I'm a Supply Chain Analytics masters student, and have begun using Python for data analysis. It's all new to me. Are there any tutoring platforms for computer science that people use? Similar to Italki, Preply etc.? One-to-one learning!\n Thanks for any tips!\n    submitted by    /u/Last-Joke-8961  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqjk16/mldata_analytics_tutors/",
          "publishedOn": "2024-02-14T10:12:02.000Z",
          "wordCount": null,
          "title": "ML/Data Analytics Tutors?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqjhoh/azure_functions_building_datadriven_solutions/",
          "author": null,
          "description": "submitted by    /u/Kairo1004  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqjhoh/azure_functions_building_datadriven_solutions/",
          "publishedOn": "2024-02-14T10:07:30.000Z",
          "wordCount": null,
          "title": "Azure Functions: Building Data-Driven Solutions With Python",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqj5uf/best_free_machine_learning_courses_online_might/",
          "author": null,
          "description": "submitted by    /u/Sreeravan  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqj5uf/best_free_machine_learning_courses_online_might/",
          "publishedOn": "2024-02-14T09:44:12.000Z",
          "wordCount": null,
          "title": "Best Free Machine Learning Courses Online might know",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqiuhe/paper_reproduction/",
          "author": null,
          "description": "Hello, I'm a graduated computer science engineer and i have a solid background with ML and my graduation project was a reasearch on adversarial Machine Learning in NLP. But since the graduation i didn't work on any ML related topics, so i kinda forgot many things, i want to reproduce a paper to refresh my memory and also get more hands-on work because i want to do work to be included in my resume as well. My question is, is reproducing a paper the right choice for me right now? If so then what papers could i start with (l'm open for any thing in ML either CV, NLP Multimodal, generative models,..etc literally anything is fine with me as long as i will gain more knowledge and experience). And if not then what are your suggestions for what i should do instead?\n    submitted by    /u/Ineffable-1  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqiuhe/paper_reproduction/",
          "publishedOn": "2024-02-14T09:20:40.000Z",
          "wordCount": null,
          "title": "Paper reproduction",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqhbxp/12_best_online_courses_for_machine_learning_with/",
          "author": null,
          "description": "submitted by    /u/Aqsa81  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqhbxp/12_best_online_courses_for_machine_learning_with/",
          "publishedOn": "2024-02-14T07:31:55.000Z",
          "wordCount": null,
          "title": "12 Best Online Courses for Machine Learning with Python- 2024",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqgob7/learn_ai/",
          "author": null,
          "description": "I have taken an introductory course in ML covering basic ML and few things about neural nets. I am fascinated by tools like GPT and stable diffusion. I would like to know how they work. Can you guys recommend me how should I start my journey?\n Thanks\n    submitted by    /u/Traditional-Olive194  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqgob7/learn_ai/",
          "publishedOn": "2024-02-14T06:48:48.000Z",
          "wordCount": null,
          "title": "Learn AI",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqfh73/how_to_think_about_tensorsvector_dimensions/",
          "author": null,
          "description": "I have recently started to get in to ML/DL. I can understand concepts and ideas. But whenever i am going through a repository I have issues following the data flows (trying to understand how data is moving around the model). Especially data transformations/ reshaping that keep happening. How does one think about these more intuitively? How did you overcome this struggle in your initial days. What was your journey with these problems?\n    submitted by    /u/jdjddhdjdjdj  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqfh73/how_to_think_about_tensorsvector_dimensions/",
          "publishedOn": "2024-02-14T05:36:06.000Z",
          "wordCount": null,
          "title": "How to think about Tensors/Vector dimensions intuitively?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aqd902/faq_chatbot/",
          "author": null,
          "description": "I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.\n    submitted by    /u/Intelligent_Usual392  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aqd902/faq_chatbot/",
          "publishedOn": "2024-02-14T03:36:23.000Z",
          "wordCount": null,
          "title": "Faq chatbot",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq9lh6/overfitting/",
          "author": null,
          "description": "Hello everyone,\n I've been trying to understand how to detect an overfitting model, but the explanations I've come across are often overly complex. I know that it's easy to spot an underfitting model when metrics like accuracy, recall, and F1 score are low, for example, around 0.5 in a classification model. But how do you detect overfitting using this approach? Or perhaps you use another method?\n    submitted by    /u/Esmasta97  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq9lh6/overfitting/",
          "publishedOn": "2024-02-14T00:38:09.000Z",
          "wordCount": null,
          "title": "Overfitting",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq8nu6/practical_tips_or_learned_experiences_fine_tuning/",
          "author": null,
          "description": "Hi,\n I'm currently working with a pre-trained ViT (ViT_B_16) for downstream tasks. I'm in the process of fine-tuning it with approximately 13,000 images distributed across 100 classes. My approach involves following PyTorch official tutorials and conducting training on a Cloud Tesla T4 (g4dn.8xlarge). Each epoch takes around 14 minutes, and for the initial run, I've arbitrarily set 20 epochs.\n For additional context, my model specifications include a batch size of 32, and I'm utilizing an Adam optimizer with a learning rate set to 1e-3. I have a few questions:\n  \nIs a 14-minute duration per epoch reasonable?\n Are there ways to decrease the training duration? Would you suggest a linear learning rate scheduler, increasing the batch size, or adjusting the DataLoader number of workers?\n Did I overlook anything by not incorporating weight decay, epsilon, or betas?\n  \nI'm feeling a bit anxious about the setup and would greatly appreciate any advice.\n Many thanks for your time.\n    submitted by    /u/Numerous_Speed_9107  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq8nu6/practical_tips_or_learned_experiences_fine_tuning/",
          "publishedOn": "2024-02-13T23:57:05.000Z",
          "wordCount": null,
          "title": "Practical tips or learned experiences fine tuning a ViT for a classification exercise",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq8lm6/alternative_for_a_securities_price_prediction/",
          "author": null,
          "description": "I really enjoy learning about finance and I've been wanting to apply my machine learning on a project in that field. I understand that time-series forecasting of stocks and such has been done billions of times, and that there are algorithms which simplify that task a lot, so for both of those reasons my project would be pointless. I really wanted to do something related to this segment, but any way i turn i end up bumping my head into \"too simple\" or \"not a good concept\" for ML.\n Any ideas? I'm really all out of luck.\n    submitted by    /u/avaqueue  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq8lm6/alternative_for_a_securities_price_prediction/",
          "publishedOn": "2024-02-13T23:54:25.000Z",
          "wordCount": null,
          "title": "Alternative for a securities price prediction project?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq7gh4/i_have_an_unorthodox_genetic_algorithm_and_im/",
          "author": null,
          "description": "I developed a genetic algorithm that, in my opinion, has some unorthodox characteristics and I would like to find out if there is something similar already published in the literature. I don't have much experience with genetic algorithms so I don't really know where to start searching, so I'm wondering if anyone here has heard or seen of an algorithm with characteristics similar to the following:\n  \nThe population of size M is divided into two disjoint groups; the top K individuals (\"top\" in terms of their fitness value) constitute the \"upper bin\", and the rest M - K individuals constitute the \"lower bin\".\n During parent selection (before performing crossover), a pair of individuals is chosen in one of three possible ways: both individuals are randomly chosen from the upper bin, both indiv…",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq7gh4/i_have_an_unorthodox_genetic_algorithm_and_im/",
          "publishedOn": "2024-02-13T23:05:55.000Z",
          "wordCount": null,
          "title": "I have an unorthodox genetic algorithm and I'm wondering if there is already something published in the literature that explains a similar algorithm",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq516w/vectorbt_for_quantitative_analysis_in_python/",
          "author": null,
          "description": "submitted by    /u/fancypigollo  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq516w/vectorbt_for_quantitative_analysis_in_python/",
          "publishedOn": "2024-02-13T21:25:47.000Z",
          "wordCount": null,
          "title": "VectorBT for Quantitative Analysis in Python",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq2dya/predicted_output_after_decoding_is_always_empty/",
          "author": null,
          "description": "Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.\n I created a new project to debug with, my real one is much more complicated, but this code is just to show my issue:\n ```import tensorflow as tf\n import numpy as np\n from keras.callbacks import EarlyStopping\n from keras.layers import Bidirectional, Dropout, BatchNormalization, Embedding, LSTM, Dense\n from keras.optimizers import Adam\n from keras.regularizers import l2\n from keras.models import Sequential\n from keras.preprocessing.sequence import pad_sequences\n from sklearn.model_selection import train_test_split\n user_prompts = np.array([\n \"What's your favorite animal?\",\n \"What's your favorite movie?\",\n \"What's your favorite book?\",\n \"What's your favorite season?\",\n \"What's your favorit…",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq2dya/predicted_output_after_decoding_is_always_empty/",
          "publishedOn": "2024-02-13T19:38:44.000Z",
          "wordCount": null,
          "title": "Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq282o/how_long_does_it_take_to_finish_all_kaggle_learn/",
          "author": null,
          "description": "Will 5 days with 5 hours each day be enough?\n    submitted by    /u/Icy_Broccoli_4162  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq282o/how_long_does_it_take_to_finish_all_kaggle_learn/",
          "publishedOn": "2024-02-13T19:31:55.000Z",
          "wordCount": null,
          "title": "How long does it take to finish all Kaggle Learn courses?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq1zsr/faq_chatbot/",
          "author": null,
          "description": "I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.\n    submitted by    /u/Intelligent_Usual392  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq1zsr/faq_chatbot/",
          "publishedOn": "2024-02-13T19:22:35.000Z",
          "wordCount": null,
          "title": "Faq chatbot",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aq0vyo/what_is_the_best_way_to_download_multiple_json/",
          "author": null,
          "description": "submitted by    /u/alenathomasfc  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aq0vyo/what_is_the_best_way_to_download_multiple_json/",
          "publishedOn": "2024-02-13T18:39:17.000Z",
          "wordCount": null,
          "title": "What is the best way to download multiple JSON files from the same domain?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apzpon/books_on_linear_algebra_for_machine_learning/",
          "author": null,
          "description": "I’m doing a project which involves creating a neural network from the ground up. There’s plenty of resources that can tell me how to do that, but I’ve struggled to find any that go into depth on the linear algebra behind ML and neural networks. Are there any papers, books or other resources anyone could direct me to that provide a rigorous understanding of the MATH behind ML?\n    submitted by    /u/Endeavor09  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apzpon/books_on_linear_algebra_for_machine_learning/",
          "publishedOn": "2024-02-13T17:52:39.000Z",
          "wordCount": null,
          "title": "Books on Linear Algebra for Machine Learning?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apzizc/%F0%9D%90%8C%F0%9D%90%9A%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%9E_%F0%9D%90%8B%F0%9D%90%9E%F0%9D%90%9A%F0%9D%90%AB%F0%9D%90%A7%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%A0_%F0%9D%90%92%F0%9D%90%AE%F0%9D%90%A9%F0%9D%90%9E%F0%9D%90%AB%F0%9D%90%AF%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%9E%F0%9D%90%9D_%F0%9D%90%92%F0%9D%90%AD%F0%9D%90%9A%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%9C%F0%9D%90%9A%F0%9D%90%A5_%F0%9D%90%8C%F0%9D%90%A8%F0%9D%90%9D%F0%9D%90%9E%F0%9D%90%A5%F0%9D%90%AC/",
          "author": null,
          "description": "submitted by    /u/victoriosus  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apzizc/%F0%9D%90%8C%F0%9D%90%9A%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%9E_%F0%9D%90%8B%F0%9D%90%9E%F0%9D%90%9A%F0%9D%90%AB%F0%9D%90%A7%F0%9D%90%A2%F0%9D%90%A7%F0%9D%90%A0_%F0%9D%90%92%F0%9D%90%AE%F0%9D%90%A9%F0%9D%90%9E%F0%9D%90%AB%F0%9D%90%AF%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%9E%F0%9D%90%9D_%F0%9D%90%92%F0%9D%90%AD%F0%9D%90%9A%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%AC%F0%9D%90%AD%F0%9D%90%A2%F0%9D%90%9C%F0%9D%90%9A%F0%9D%90%A5_%F0%9D%90%8C%F0%9D%90%A8%F0%9D%90%9D%F0%9D%90%9E%F0%9D%90%A5%F0%9D%90%AC/",
          "publishedOn": "2024-02-13T17:45:22.000Z",
          "wordCount": null,
          "title": "𝐌𝐚𝐜𝐡𝐢𝐧𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠: 𝐒𝐮𝐩𝐞𝐫𝐯𝐢𝐬𝐞𝐝 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐌𝐨𝐝𝐞𝐥𝐬 🤖",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apyyrd/unlock_ais_power_top_free_courses_on_chatgpt/",
          "author": null,
          "description": "submitted by    /u/UseCreative4765  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apyyrd/unlock_ais_power_top_free_courses_on_chatgpt/",
          "publishedOn": "2024-02-13T17:23:06.000Z",
          "wordCount": null,
          "title": "Unlock AI's Power: Top Free Courses on ChatGPT & Large Language Models| DeepLearning.AI",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apyvgp/training_tesseract_on_images_of_text_lines/",
          "author": null,
          "description": "Hello,\n I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.\n Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.\n Any help is highly appreciated!\n    submitted by    /u/SirVampyr  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apyvgp/training_tesseract_on_images_of_text_lines/",
          "publishedOn": "2024-02-13T17:19:30.000Z",
          "wordCount": null,
          "title": "Training Tesseract on images of text lines + transcriptions?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apy9uf/multihead_in_transformers/",
          "author": null,
          "description": "I'm a newcomer to transformers and have a question about multi-head usage. I've gathered from various blogs that it helps capture word importance in different contexts. For instance, with 2 heads and a token size of 516, the embeddings split into two parts processed separately. If a word like 'king' represents gender and royalty in different dimensions, having them in separate halves enables the model to learn both aspects. However, if gender and royalty are in the same half, can the model still discern their importance individually, or is it limited to focusing on one aspect at a time?\n    submitted by    /u/thestorytellerixvii  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apy9uf/multihead_in_transformers/",
          "publishedOn": "2024-02-13T16:55:47.000Z",
          "wordCount": null,
          "title": "Multi-head in transformers",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apxv7d/which_model_benefited_the_most_from/",
          "author": null,
          "description": "Is it possible to see which model benefited the most from hyperparameter tuning? I think we can say it's not the random forest classifier since its results are pretty good and even.\n https://preview.redd.it/y1c8q252sdic1.png?width=1766&format=png&auto=webp&s=4c681106c16a537fc54eec5d63e42c3784d1aa45\n    submitted by    /u/nipaldi  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apxv7d/which_model_benefited_the_most_from/",
          "publishedOn": "2024-02-13T16:39:43.000Z",
          "wordCount": null,
          "title": "Which model benefited the most from hyperparameter tuning?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apxjn6/deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/",
          "author": null,
          "description": "Hi everyone,\n In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.\n vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).\n See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html\n In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.\n Please don't hesitate to share feedbacks about this article, it will be very much appreciated!\n Julien\n    submitted by    /u/juliensalinas  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apxjn6/deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/",
          "publishedOn": "2024-02-13T16:26:39.000Z",
          "wordCount": null,
          "title": "Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apvhbe/i_am_confused_about_what_i_should_do/",
          "author": null,
          "description": "I learn the basic classification and regression model(ex: SVR,decision tree,random forest etc).What should I do next???\n ​\n    submitted by    /u/Personal-Novel-7171  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apvhbe/i_am_confused_about_what_i_should_do/",
          "publishedOn": "2024-02-13T15:01:42.000Z",
          "wordCount": null,
          "title": "I am confused about what I should do",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apve3o/my_code_run_without_error_but_the_resulted_model/",
          "author": null,
          "description": "Hello everyone. I'm still a beginner. I want to ask what's wrong in my code that the resulted model didn't classify the apple leaf diseases correctly? the code run without error when I run the code, but the disease clasification is false. I trained this on 7k images apple leaves diseases dataset with 4 classes, each class has images around 1.8k. The total images of each class is not equal, is this affect the model clasification? or is there something wrong in my code below?\n ```python from google.colab import drive drive.mount('/content/gdrive')\n import zipfile zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/dataset/data9k.zip', 'r') zip_ref.extractall(\"/content/dataset\") zip_ref.close()\n import tensorflow as tf from tensorflow.keras.applications.imagenet_utils import preprocess_input i…",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apve3o/my_code_run_without_error_but_the_resulted_model/",
          "publishedOn": "2024-02-13T14:57:55.000Z",
          "wordCount": null,
          "title": "My Code Run Without Error But The Resulted Model Clasified The Apple Leafes Disesases Incorrectly",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apuw1a/is_it_possible_to_train_a_os_llm_with_my_own_data/",
          "author": null,
          "description": "Hello, I am working on an internal GPT of some sorts and I dont get the best results with RAG. My question is there a way to download an os llm from huggingface and to continue the training with the internal data. ? \n    submitted by    /u/Longjumping_Fruit843  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apuw1a/is_it_possible_to_train_a_os_llm_with_my_own_data/",
          "publishedOn": "2024-02-13T14:34:44.000Z",
          "wordCount": null,
          "title": "Is it possible to train a os llm with my own data?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aptd69/books_and_reference_sites_newbie_here/",
          "author": null,
          "description": "guys can you suggest a reference or tutorial series or a book for machine learning ? my college professors sometimes don't explain what they are writing, like I understood the k-means algo , all the theory they taught but I am confused about polynomial regression and what is regularized linear regression\n a book or a reference site maybe helpful\n thanks for the upcoming answers !\n    submitted by    /u/kichiDsimp  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aptd69/books_and_reference_sites_newbie_here/",
          "publishedOn": "2024-02-13T13:23:26.000Z",
          "wordCount": null,
          "title": "Books and reference sites, newbie here",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apsdu9/having_a_hard_time_with_theory_heavy_courses/",
          "author": null,
          "description": "I went through some courses where there's only theory and not practical work, like CS229 and Deepmind UCL Intro to RL, and I feel like I haven't learnt a thing. I tkae notes of everything shown in slides, written on boards but don't understand them at all. I miss what they are talking about/expaining and my notes makes no sense to me.\n I feel like I'd have learnt more if I just sat down and listened to what they said while looking at their displayed slides. I don't actually need these notes since I don't review them, for an exam or something else unlike I would in irl courses.\n    submitted by    /u/open_23  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apsdu9/having_a_hard_time_with_theory_heavy_courses/",
          "publishedOn": "2024-02-13T12:33:27.000Z",
          "wordCount": null,
          "title": "Having a hard time with theory heavy courses",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aprjg8/ai_in_sports_slowmotion_reviews_project/",
          "author": null,
          "description": "A interesting approach using FILM: Frame Interpolation for Large Motion to further smooth down slow motion videos which are quite hard to watch and indecisive. \n Video demo here\n Any suggestions or further improvements?\n    submitted by    /u/ade17_in  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aprjg8/ai_in_sports_slowmotion_reviews_project/",
          "publishedOn": "2024-02-13T11:45:15.000Z",
          "wordCount": null,
          "title": "AI in sports! slow-motion reviews [Project]",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apqki2/new_to_ai_join_our_beginner_meetup/",
          "author": null,
          "description": "Hi Everyone! \n Just wanted to post to see if anyone is interested in AI but not sure where to start? I'm hosting a no-pressure, beginner-friendly AI meetup on 21st February at 5pm AEST if anyone is interested. It's a great chance to:\n  \nTry hands-on AI activities.\n Ask questions in a supportive environment.\n Meet fellow AI newbies.\n  \nDetails:\n  \nWhen: 21st February, 5pm AEST\n Where: Online\n RSVP: https://www.meetup.com/meetup-group-ynmkmrlc/events/299167242/?utm\\_medium=referral&utm\\_campaign=yourEvent\\_savedevents\\_share\\_modal&utm\\_source=link\n  \nPlease feel to reach out with any questions and I hope to see you there!\n    submitted by    /u/KeyMiddle32  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apqki2/new_to_ai_join_our_beginner_meetup/",
          "publishedOn": "2024-02-13T10:42:03.000Z",
          "wordCount": null,
          "title": "New to AI? Join Our Beginner Meetup!",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1appqes/simplified_transformers/",
          "author": null,
          "description": "I understand how revolutionary the transformer architecture is. However, while studying it, it's crazy how much details this thing contains, which makes it harder for everyone to understand/enhance.\n My question is, have there been any attempts to build a simplified transformer architecture? something akin to how GRUs are kind of a simplified version of LSTMs. \n Would really appreciate it if you could point me to any papers that have tried to accomplish that, or maybe find any unnecessary/less necessary elements in the original 2017 paper.\n    submitted by    /u/Salloum-A  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1appqes/simplified_transformers/",
          "publishedOn": "2024-02-13T09:43:38.000Z",
          "wordCount": null,
          "title": "Simplified Transformers?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1appe1h/best_books_to_learn_tensorflow_in_2024_for/",
          "author": null,
          "description": "submitted by    /u/Sreeravan  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1appe1h/best_books_to_learn_tensorflow_in_2024_for/",
          "publishedOn": "2024-02-13T09:18:20.000Z",
          "wordCount": null,
          "title": "Best Books to Learn Tensorflow in 2024 for beginners & Advanced -",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apoveo/i_need_to_deploy_an_ai_model_wrapped_by_a_rest/",
          "author": null,
          "description": "From what I understand, after a data scientist hands me the models binaries, I should load it into Pytorch. Then I create endpoints our client can call. The input of the client is placed through transformers, then fed into the model which spits out a prediction. I then return the prediction in the HTTP response.\n Is this done in any regular ol' CPU cloud VM like a Digital Ocean droplet? Or should I use deploy the model separately in a Paperspace GPU and call it from a Flask server in a droplet? The details are quite fuzzy for me on this.\n    submitted by    /u/Radiant-Message9493  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apoveo/i_need_to_deploy_an_ai_model_wrapped_by_a_rest/",
          "publishedOn": "2024-02-13T08:40:44.000Z",
          "wordCount": null,
          "title": "I need to deploy an AI model wrapped by a REST API to the cloud. Which cloud provider should I use?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apnf51/solving_linear_equations_for_ml/",
          "author": null,
          "description": "I’m reading the book “Math for Machine Learning” and there’s a Linear Algebra section that focuses on solving linear equations (reduced row echelon form, elementary transformations, calculating the inverse). How applicable is this to learning ML. Should I actually solve these linear equations by hand to prepare to understand ML?\n    submitted by    /u/Droski_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apnf51/solving_linear_equations_for_ml/",
          "publishedOn": "2024-02-13T06:58:44.000Z",
          "wordCount": null,
          "title": "Solving Linear Equations for ML?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aplhu5/my_company_gave_me_a_500dollar_education_budget/",
          "author": null,
          "description": "Any recommendations on courses, books, or other materials that are worth paying for? TIA.\n    submitted by    /u/myotheraccount7071  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aplhu5/my_company_gave_me_a_500dollar_education_budget/",
          "publishedOn": "2024-02-13T05:05:59.000Z",
          "wordCount": null,
          "title": "My company gave me a 500-dollar education budget. Looking for recommendations",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1aphj8p/announcing_bitesized_coding_problems_for_ml/",
          "author": null,
          "description": "Hey guys,\n u/NeetCode and I are excited to announce coding problems for AI/ML that you can solve in your browser and run against test cases. They assume no prior background knowledge in AI/ML. They work up from linear regression to coding and training a GPT chat model from scratch!\n For each problem, I also created a 5-10 minute background video covering the concepts needed to solve the problem (or quiz, for the topics that have multiple choice quizzes to go along with them) as well as a solution video.\n All the videos for the problems, and 2x a week concept overviews on different ML topics (suggestions welcome!) can be found on my channel: https://www.youtube.com/@GPTandChill\n The problem list can be found here on NeetCode's site https://neetcode.io/practice?subpage=practice&tab=coreSkills&topic=Machine%20Learning OR here on my site https://www.gptandchill.ai/leetcode-for-ml\n And here are Navi's posts for some additional context:\n https://x.com/neetcode1/status/1756997643556041191?s=20\n https://www.linkedin.com/posts/activity-7162822685037674496-i0Yo?utm_source=share&utm_medium=member_desktop\n Let us know if you like this kind of educational content or have any feedback!\n    submitted by    /u/GPTandChill  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1aphj8p/announcing_bitesized_coding_problems_for_ml/",
          "publishedOn": "2024-02-13T01:49:20.000Z",
          "wordCount": null,
          "title": "Announcing Bite-Sized Coding Problems for ML",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/learnmachinelearning/comments/1apgtuh/do_machine_learning_engineers_work_on_robotics/",
          "author": null,
          "description": "I'm very interested in both Robotics and AI, and I was wondering whether or not I could be a Machine Learning Engineer in robotics. \n    submitted by    /u/Daniu_13  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/learnmachinelearning/comments/1apgtuh/do_machine_learning_engineers_work_on_robotics/",
          "publishedOn": "2024-02-13T01:15:41.000Z",
          "wordCount": null,
          "title": "Do Machine Learning Engineers work on robotics?",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "matthewmcateer.me",
      "feedUrl": "https://matthewmcateer.me/rss.xml",
      "siteUrl": "https://matthewmcateer.me",
      "articles": []
    },
    {
      "title": "Count Bayesie - A Probability Blog",
      "feedUrl": "https://www.countbayesie.com/blog?format=rss",
      "siteUrl": "http://www.countbayesie.com/",
      "articles": []
    },
    {
      "title": "Machine Learning (Theory)",
      "feedUrl": "https://hunch.net/?feed=rss2",
      "siteUrl": "https://hunch.net",
      "articles": []
    },
    {
      "title": "Statistical Thinking",
      "feedUrl": "https://www.fharrell.com/index.xml",
      "siteUrl": "https://fharrell.com/",
      "articles": [
        {
          "id": "https://fharrell.com/talk/cos/",
          "author": "Frank Harrell",
          "description": "Slides\nElaborations\nVideo",
          "link": "https://fharrell.com/talk/cos/",
          "publishedOn": "2024-01-30T06:00:00.000Z",
          "wordCount": 2062,
          "title": "Overview of Composite Outcome Scales & Statistical Approaches for Analyzing Them",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Machine Learning",
      "feedUrl": "https://old.reddit.com/r/MachineLearning/.rss",
      "siteUrl": "https://old.reddit.com/r/MachineLearning/",
      "articles": [
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqlp23/p_sign_language_recognition_slr_how_good_is_it/",
          "author": null,
          "description": "I've seen a lot of beginner tutorials to implement video stream based sign language recognition but they all seem to have some issues in a real world situation (Let's say a TV recording of a press conference).\n A client asked if we can do this, so I started wondering:\n  \nHow good is the current state of the are for SLR really? Is it being used in practice?\n Are there existing models or even services that can just be used?\n Do these exist or can be adopted for less popular sign language dialects?\n  \n   submitted by    /u/Enum1  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqlp23/p_sign_language_recognition_slr_how_good_is_it/",
          "publishedOn": "2024-02-14T12:26:17.000Z",
          "wordCount": 1696,
          "title": "[P] Sign Language Recognition (SLR): How good is it really and can I make it work for a less popular sign language?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqlkra/r_can_you_create_a_saving_space_link_between/",
          "author": null,
          "description": "Good day gents, I just discovered Comfy not a long a go, might be transitioning from A1111. So it gotta be a method to save space by sharing the models files between these two apps\n    submitted by    /u/qualaric  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqlkra/r_can_you_create_a_saving_space_link_between/",
          "publishedOn": "2024-02-14T12:19:38.000Z",
          "wordCount": 1619,
          "title": "[R] Can you create a saving space link between WebUI and ComfyUI?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqljss/d_ml_course_recommendation/",
          "author": null,
          "description": "Hello, I am especially interested in learning more about the field of data science. This includes learning about all the fundamentals from algorithms and underlying mathmatical theories to types of machine learning. I want to use python since i find it easy to use and i will be using it at work a lot soon. What i am missing right now is the ability to think of problems i can solve. By that i mean small easy projects to learn the basics. I feel like i need to be guided through this learning process just like i am being guided in class at my university. Which courses can you recommend? What did they teach you? What would you describe the learning process to be like?\n    submitted by    /u/rThilo  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqljss/d_ml_course_recommendation/",
          "publishedOn": "2024-02-14T12:18:05.000Z",
          "wordCount": 1693,
          "title": "[D] ML Course Recommendation",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqlgck/d_qa_role_in_machine_learning_projects/",
          "author": null,
          "description": "In my work experience I've met a situation, where we have to understand the role of QA in ML projects.\n I know that it's an extremely specific situation, just because ML testing is the job that Engineer does with the training and after.\n So my question is: do you have any experience with QA application directly in ML projects, or related literature to start?\n    submitted by    /u/thattallsoldier  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqlgck/d_qa_role_in_machine_learning_projects/",
          "publishedOn": "2024-02-14T12:12:32.000Z",
          "wordCount": 1638,
          "title": "[D] QA role in Machine Learning projects",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqjrc8/r_world_model_on_millionlength_video_and_language/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.08268 \n Github: https://github.com/LargeWorldModel/LWM \n Models: https://huggingface.co/LargeWorldModel !\n Abstract:\n  \nCurrent language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we …",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqjrc8/r_world_model_on_millionlength_video_and_language/",
          "publishedOn": "2024-02-14T10:26:03.000Z",
          "wordCount": 1915,
          "title": "[R] World Model on Million-Length Video And Language With RingAttention - UC Berkeley 2024 - Is able to describe a clip in an over an hour long video with over 500 clips with near perfect accuracy! - Is open source!",
          "imageUrl": "https://external-preview.redd.it/89xpv0ix1jic1.jpg?overlay-align=bottom,left&crop=1177:616.230366492,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1177&height=616.230366492&auto=webp&s=43ab4022189d13369b816c9b1e5100ab7f83c15d"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqjp5d/p_making_my_bookshelves_clickable_with_computer/",
          "author": null,
          "description": "I built a system that lets you take a photo of a bookshelf and create an interactive HTML web page where you can click on books in an image to learn more about each one.\n The tech stack for this project is:\n  \nGrounded SAM to retrieve polygons for books.\n OpenCV + supervision transformations to prepare books for OCR.\n GPT-4 with Vision for OCR\n Google Books API to get book metadata.\n HTML + SVG generation to create the final web page.\n  \nI wrote about how I built this project on my blog.\n Try the demo.\n I'd love feedback on how I can improve the book detection rate for better performance. Training a custom segmentation model on book spines might work, but I am cognizant about how much data I might need for that.\n The red polygons below indicate segmented books that, in the demo, are clickable:\n https://preview.redd.it/p9w4rgsn1jic1.png?width=1260&format=png&auto=webp&s=35116c7eb9d1f5dab2b11375be9e2ff0e7163b78\n    submitted by    /u/zerojames_  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqjp5d/p_making_my_bookshelves_clickable_with_computer/",
          "publishedOn": "2024-02-14T10:21:46.000Z",
          "wordCount": 1722,
          "title": "[P] Making my bookshelves clickable with computer vision",
          "imageUrl": "https://external-preview.redd.it/p9w4rgsn1jic1.png?overlay-align=bottom,left&crop=1200:628.272251309,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1200&height=628.272251309&auto=webp&s=1aa6a1438cd0ffab882a22f5a415c73c58d28ba0"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqjib3/p_graph_rag_for_wikipedia/",
          "author": null,
          "description": "​\n https://preview.redd.it/362o1bscziic1.jpg?width=2048&format=pjpg&auto=webp&s=ab28e5921bb06d3a1aec881323b48bfadaa835bc\n RAG in it's simplest form is a vector search as context with a LLM prompt.\n With the next version of txtai, we'll have a series of new graph-based RAG techniques. Think of this like a road trip with a number of pit stops.\n Say you're researching the early medieval history of England. Sure we can run a vector search for that. But what if we can instruct a query to traverse a number of concepts we're interested in?\n Let's take the example above. This is a network of Wikipedia articles (via txtai-wikipedia). The query traverses paths of history between the Roman Empire, Anglo-Saxon period, Viking period and ends with the Norman conquest. This rich dataset is then available as a library of context to downstream LLM prompts.\n Graph databases aren't new. The difference here is that txtai builds a vector store and uses that to automatically build a graph network weighted by vector similarity.\n Read this article for more: https://neuml.hashnode.dev/generate-knowledge-with-semantic-graphs-and-rag\n    submitted by    /u/davidmezzetti  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqjib3/p_graph_rag_for_wikipedia/",
          "publishedOn": "2024-02-14T10:08:47.000Z",
          "wordCount": 1732,
          "title": "[P] Graph RAG for Wikipedia",
          "imageUrl": "https://external-preview.redd.it/362o1bscziic1.jpg?overlay-align=bottom,left&crop=1200:628.272251309,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1200&height=628.272251309&auto=webp&s=89b58a6012d5c6d1eb96eb617183e45fefe70096"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqj2i0/finding_a_lowdim_subspace_where_points_in_the/",
          "author": null,
          "description": "I have an embeddings dataset consisting of some clusters of data points (I chose a priori which points belong to the same cluster). Currently, points belonging to the same cluster aren't necessarily close to each other in embedding space. I want to find a low-dimensional subspace such that if I project these embeddings onto that subspace, points belonging the same cluster will be close to each other. Different clusters don't necessarily need to be far apart in the low-dim space. I thought of an optimization problem to solve using SVD and the k-means cost function, but not sure if I can actually solve it. Was curious if anyone else has ideas/thoughts!\n    submitted by    /u/oomydoomy  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqj2i0/finding_a_lowdim_subspace_where_points_in_the/",
          "publishedOn": "2024-02-14T09:37:21.000Z",
          "wordCount": 1742,
          "title": "Finding a low-dim subspace where points in the same cluster are close to each other [R]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqiyt3/d_p_few_questions_about_tortoise_tts_and_rvc/",
          "author": null,
          "description": "I have 3 hours high quality dataset with and the model will be used to do the same thing the dataset is from. My question is How many epoches should i let it run for and if i should cut down the dataset. \n Also what is the difference between using 10, 10 second audio to make a voice and making my own entire model.\n i will be using the same dataset to train an RVC model, my plan is to run the tortoise output through the same model in RVC toupscale the quality and later further enhance it with adobe enhance.\n I would be grateful if someone knowledgeable on this topic helped me out because i am completey new to this, Thank you.\n    submitted by    /u/Opurbobin  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqiyt3/d_p_few_questions_about_tortoise_tts_and_rvc/",
          "publishedOn": "2024-02-14T09:29:54.000Z",
          "wordCount": 1666,
          "title": "[D] [P] Few questions about tortoise TTS and RVC.",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqiw8m/paper_reproduction_d/",
          "author": null,
          "description": "Hello, I'm a graduated computer science engineer, and i have a solid background with ML . My graduation project was research on adversarial machine learning in NLP. But since the graduation i didn't work on any ML related topics, so i kinda forgot many things, i want to reproduce a paper to refresh my memory and also get more hands-on work because i want to do thing to be included in my resume as well. My question is, is reproducing a paper the right choice for me right now? If so then what papers could i start with (I'm open for any thing in ML either CV, NLP, Multimodal, generative models, ..etc literally anything is fine with me as long as i will gain more knowledge and experience). And if not, then what are your suggestions for what i should do instead?\n    submitted by    /u/Ineffable-1  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqiw8m/paper_reproduction_d/",
          "publishedOn": "2024-02-14T09:24:17.000Z",
          "wordCount": 1692,
          "title": "Paper reproduction [D]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqidg6/d_best_rnns_deep_dive_videoarticlepost/",
          "author": null,
          "description": "I'm learning RNN's, and every article and video I came across so far misses some details, no one is explaining everything in great details. I prefer reading or watching a material, where author is writing code not using ready RNN functions and is explaining everything in details like Andrej Karpathy would. (I read Karpathy's blog post on RNN, but there was no much code related to RNNs and article was mostly about LSTMs) \n    submitted by    /u/your_dream724  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqidg6/d_best_rnns_deep_dive_videoarticlepost/",
          "publishedOn": "2024-02-14T08:45:17.000Z",
          "wordCount": 1679,
          "title": "[D] Best RNNs deep dive video/article/post ?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqh7e7/will_there_be_ai_winter_30_d/",
          "author": null,
          "description": "Where do you think this trend is going?\n ​\n https://preview.redd.it/cr0wroqo3iic1.png?width=899&format=png&auto=webp&s=79b9b9ecd16702eeb70344972fa650e735f27569\n ​\n Image source: https://www.nature.com/articles/s42256-023-00735-0\n View Poll\n    submitted by    /u/we_are_mammals  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqh7e7/will_there_be_ai_winter_30_d/",
          "publishedOn": "2024-02-14T07:23:14.000Z",
          "wordCount": 1918,
          "title": "Will there be AI Winter 3.0? [D]",
          "imageUrl": "https://external-preview.redd.it/cr0wroqo3iic1.png?overlay-align=bottom,left&crop=899:470.680628272,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=899&height=470.680628272&auto=webp&s=9418f2e7a2a1bc1d0075509ab1d843000efe0754"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqfcts/p_open_images_dataset_v7/",
          "author": null,
          "description": "Hey guys, I’m not sure this is the right space to ask this type of question. I’m kinda new on this so sorry if it’s not.\n Yesterday I started trying to create a script that detects some sort of category in some provided images. However in order to train the model I saw that I could download a lot of photos, with the detections already done from Open Images Dataset. However I don’t seem to find any way to download all the photos (or at least a good amount) from a specific category (airplanes for example). \n Does anyone who knows how to work with this type of project can help me? Thank you a lot.\n    submitted by    /u/goncalosm01  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqfcts/p_open_images_dataset_v7/",
          "publishedOn": "2024-02-14T05:29:20.000Z",
          "wordCount": 1690,
          "title": "[P] Open Images Dataset v7",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqf4io/d_vaes_for_classification/",
          "author": null,
          "description": "Straight to the point: \n is it possible for the latent space of a VAE to learn a classification not fed to the model? As an example let’s say you had a bunch of images of cats and dogs together, no labels, could a VAE model learn implicitly whether it’s a cat or a dog via the latent space?\n As another question, could you use the loss of a VAE in tandem with a classification model and use the loss of the VAE to gauge the likelihood that a sample is one of the labels in the first place?\n    submitted by    /u/Radiant_Walrus3007  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqf4io/d_vaes_for_classification/",
          "publishedOn": "2024-02-14T05:16:11.000Z",
          "wordCount": 1890,
          "title": "[D] VAEs for classification",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqewye/d_a_problem_that_seems_like_a_ml_problem/",
          "author": null,
          "description": "Hello. \n I work on a system that is highly parameterized i.e. has a high number of parameters (binary values or a range of integers). These parameters are not independent. Although a lot of the possible combinations are not valid, they still result in a very high number of possible configurations. \n Now, the task at hand is to find the best configuration of this parameterized system that will maximize a metric that is directly measurable, subject to the input setting. Again the input space is non-trivially large. \n It seems like a classical machine learning problem but seems more of a simulation-type problem where given an ideal world where I have infinite resources, I would run all system configurations against the input setting in question and find the setting that maximizes my metric in question. In “test time”, I will use this information in hand to run the system in the most optimal setting. \n Does this problem setting sound close to any existing well-researched area? Thanks. \n PS - I am being cryptic as I am not in a position to disclose the exact system in question.\n https://preview.redd.it/frgvz33wghic1.png?width=1292&format=png&auto=webp&s=5ea8d9720e185d0cb7de3fc3c29a5593f342c5d4\n    submitted by    /u/Traditional_Two7396  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqewye/d_a_problem_that_seems_like_a_ml_problem/",
          "publishedOn": "2024-02-14T05:04:31.000Z",
          "wordCount": 1970,
          "title": "[D] A problem that seems like a ML problem.",
          "imageUrl": "https://external-preview.redd.it/frgvz33wghic1.png?overlay-align=bottom,left&crop=1200:628.272251309,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1200&height=628.272251309&auto=webp&s=41703eeca898c1492a79f86d3bbd9772b7430f63"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqe5v4/need_to_finetune_llms_on_chattranscript_data_d/",
          "author": null,
          "description": "Hello, I've hundreds of transcripts having conversations between agent and customer. I've been working on finetuning mistral using QLoRa. The objective is to make a chatbot or virtual agent. However, after finetuning, instead of a single response, the model generates an entire conversation. My prompt looks something like - \n Conversation :\n {A Small Snippet from a Transcript}\n Agent :\n { }\n Should I change the prompt? Does anyone have any experience on finetuning on chat/transcript data? Any help would be highly appreciated.\n    submitted by    /u/Evermore2307  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqe5v4/need_to_finetune_llms_on_chattranscript_data_d/",
          "publishedOn": "2024-02-14T04:23:28.000Z",
          "wordCount": 1735,
          "title": "Need to Finetune LLMs on chat/Transcript Data [D]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqdde4/d_seeking_advice_cloud_certification_for_aiml/",
          "author": null,
          "description": "Hi everyone,\n I'm currently pursuing a Master's in AI and have a year of experience as a software developer. As I'm aspiring to break into machine learning/AI or data science roles, I've been contemplating adding a cloud certification to my portfolio. Given the significance of cloud infrastructure in deploying AI models and handling big data, I'm considering the Google Associate Cloud Engineer certification. However, with the rising popularity of Microsoft's cloud solutions, I'm at a crossroads.\n My primary goal is to enhance my employability and secure a well-paying job in the AI/ML or data science domain. While I have a few projects under my belt, I believe a cloud certification could potentially make my resume stand out.\n I'm seeking advice on a few fronts:\n  \nRelevance of Cloud Certifications: How beneficial is a cloud certification, specifically for someone aiming for AI/ML or data science roles? Does it significantly impact job prospects?\n Google Cloud vs. Microsoft Azure: Given the industry trends and job market demands, would you recommend Google Cloud or Microsoft Azure? Or is there another platform I should consider?\n Investment Worth: Considering the time and financial investment, do you think pursuing a cloud certification is a strategic move for someone with my background and goals?\n  \nAny insights, experiences, or recommendations would be greatly appreciated. I'm here to learn from those who've navigated similar paths or have insights into the industry trends. Thank you in advance for your guidance!\n    submitted by    /u/No_Masterpiece_1430  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqdde4/d_seeking_advice_cloud_certification_for_aiml/",
          "publishedOn": "2024-02-14T03:42:50.000Z",
          "wordCount": 1821,
          "title": "[D] Seeking Advice: Cloud Certification for AI/ML Career Path",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqd24k/d_benchmarking_retrieval_across_context_lengths/",
          "author": null,
          "description": "The following “needle in a haystack” test for GPT 4 went viral earlier this year, showing 100% retrieval for the first 64k tokens:\n https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n Is this considered a valid test among machine learning experts? If it is valid, then has it been replicated anywhere else? It seems unlikely that this would be the only public implementation of the test if it was valid.\n If it is not valid, what method might be?\n Finally, overall how many tokens of context do you personally think GPT 4 can remember well?\n    submitted by    /u/Ok_Elephant_1806  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqd24k/d_benchmarking_retrieval_across_context_lengths/",
          "publishedOn": "2024-02-14T03:26:19.000Z",
          "wordCount": 1798,
          "title": "[D] Benchmarking retrieval across context lengths",
          "imageUrl": "https://external-preview.redd.it/0SWqwbR6fSpKYpV2sYO_UJny76Bv2iNDtVJoraeVSgQ.jpg?width=1200&height=600&auto=webp&crop=1200:600,smart&s=ee59604bc1018f9a716b50bb9b796e8125dd0eba"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqcdq8/d_pointer_on_using_knowledge_graphs_in_rag/",
          "author": null,
          "description": "I'm curious to learn more about using knowledge graphs in retrieve-and-generate (RAG) systems. RAG involves retrieving external knowledge to help generate responses, so it seems like knowledge graphs could be very useful.\n Some specific questions I have:\n  \nWhat types of knowledge graphs work best for RAG applications? Do domain-specific graphs tend to be more useful compared to large, general graphs?\n What are some effective techniques for querying and retrieving relevant knowledge from graphs to generate responses? Are there any best practices?\n How feasible is it to keep knowledge graphs updated as new information emerges? Does the graph need to be static or can RAG systems handle frequent graph updates?\n Can knowledge graphs help with tasks like disambiguation of entities and concepts when generating responses?\n Are there any good open source knowledge graphs out there that can be pre-trained with RAG models? Or examples of systems that showcase using graphs well?\n  \nI'd appreciate any insight you can offer around integrating knowledge graphs into RAG workflows. Feel free to point me to any papers, examples, or other materials too. Looking forward to learning more about this area.\n    submitted by    /u/Electrical_Study_617  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqcdq8/d_pointer_on_using_knowledge_graphs_in_rag/",
          "publishedOn": "2024-02-14T02:52:25.000Z",
          "wordCount": 1890,
          "title": "[d] Pointer on Using Knowledge graphs in RAG",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqc9t7/d_practical_tips_or_learned_experiences_fine/",
          "author": null,
          "description": "Hi,\n I'm currently working with a pre-trained ViT (ViT_B_16) for downstream tasks. I'm in the process of fine-tuning it with approximately 13,000 images distributed across 100 classes. My approach involves following PyTorch official tutorials and conducting training on a Cloud Tesla T4 (g4dn.8xlarge). Each epoch takes around 14 minutes, and for the initial run, I've arbitrarily set 20 epochs.\n For additional context, my model specifications include a batch size of 32, and I'm utilizing an Adam optimizer with a learning rate set to 1e-3. I have a few questions:\n  \nIs a 14-minute duration per epoch reasonable?\n Are there ways to decrease the training duration? Would you suggest a linear learning rate scheduler, increasing the batch size, or adjusting the DataLoader number of workers?\n Did I overlook anything by not incorporating weight decay, epsilon, or betas?\n  \nI'm feeling a bit anxious about the setup and would greatly appreciate any advice.\n Many thanks for your time.\n    submitted by    /u/Numerous_Speed_9107  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqc9t7/d_practical_tips_or_learned_experiences_fine/",
          "publishedOn": "2024-02-14T02:47:00.000Z",
          "wordCount": 1747,
          "title": "[D] Practical tips or learned experiences fine tuning a ViT for a classification exercise",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aqbeix/d_camera_options_for_detecting_and_tracking_small/",
          "author": null,
          "description": "Looking for some help with a hobby project. I would like to detect small objects (2-10mm size) on the ground while on the move. So camera would be attached to the vehicle going up to 10km/h and looking for objects on the ground. Distance between the camera and ground would be about 500mm. I have would like to use Jetson Nano/Xavier which I already have. My biggest worry is the camera - any idea what fps/sensor size I would need to get clear images?\n    submitted by    /u/mrbronec  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aqbeix/d_camera_options_for_detecting_and_tracking_small/",
          "publishedOn": "2024-02-14T02:04:19.000Z",
          "wordCount": 1745,
          "title": "[D] Camera options for detecting and tracking small objects on the groung",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq8p38/d_if_i_have_a_sampling_strategy_a_b_and_c_and_b/",
          "author": null,
          "description": "So I've been testing different sampling strategies for NLP data with binary labels (Even, stratified, and the middle between the two on the label). I've been testing them on 20% of the data and have found that the middle sampling strategy has done the best so far. If I scale to 100% of the data what reasons might the middle one no longer be the best?\n    submitted by    /u/DolantheMFWizard  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq8p38/d_if_i_have_a_sampling_strategy_a_b_and_c_and_b/",
          "publishedOn": "2024-02-13T23:58:36.000Z",
          "wordCount": 1809,
          "title": "[D] If I have a sampling strategy A, B, and C and B perform the best. Would that still be true if data scaled?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq8dvw/d_information_retrievalsearch/",
          "author": null,
          "description": "I am looking for documentation on building a search engine. Specifically around handling queries and building embeddings for them.\n Some of the use cases can be long queries, maintaining long context, spelling mistakes, handling multiple conditions, rewriting, expansion, query intent , NLU.\n I will probably build it using RAG+LLM but I think the basic principles will still apply. Any suggestions on where/what to read up?\n    submitted by    /u/Worldly-Pen-8101  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq8dvw/d_information_retrievalsearch/",
          "publishedOn": "2024-02-13T23:45:16.000Z",
          "wordCount": 1611,
          "title": "[D] Information retrieval/search",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq8cm5/p_what_is_a_robust_pretrained_word2vec_model/",
          "author": null,
          "description": "I'm trying to build an RNN to predict sentences, but I need to start with a good Word2Vec model that is robust to things like numbers (in non-word form so: 1,2,3), human names, and so on. I have data, but new data can come in with words not previously seen, thus the need for a robust Word2Vec model. Any suggestions?\n Note: I can't use Transformers for this problem due to certain problem constraints since I know the most common response will be to use a pre-trained Transformer.\n    submitted by    /u/DolantheMFWizard  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq8cm5/p_what_is_a_robust_pretrained_word2vec_model/",
          "publishedOn": "2024-02-13T23:43:46.000Z",
          "wordCount": 1643,
          "title": "[P] What is a robust pre-trained Word2Vec model?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq8aw9/python_code_for_chatgpt_api_r/",
          "author": null,
          "description": "My Python code interacts successfully with the ChatGPT API; however, the results it yields differ from what I expect. Outputs from ChatGPT are typically more elaborate and extended, but the responses I receive from my API calls are brief and lack detail. Despite tweaking the temperature and token values, I haven't seen an improvement. I would appreciate any assistance with this issue.\n def get_completion(prompt, model=\"gpt-4\", temperature=0.7, max_tokens=5000):\n messages = [{\"role\": \"user\", \"content\": prompt}]\n response = openai.ChatCompletion.create(\n model=model,\n messages=messages,\n temperature=temperature,\n max_tokens=max_tokens,\n )\n return response.choices[0].message[\"content\"]\n ​\n ChatGPT API response : Plano, Texas is known for its affluent population and highly prioritized educatio…",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq8aw9/python_code_for_chatgpt_api_r/",
          "publishedOn": "2024-02-13T23:41:42.000Z",
          "wordCount": 2059,
          "title": "Python code for chatgpt API [R]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq7rd5/p_speech_synthesis_with_mamba_beginner_friendly/",
          "author": null,
          "description": "Hi all, I came across this post last month and found it super interesting. I'm a developer advocate at Determined AI and am always looking to learn new things, so I wanted to work through it myself. Super well written blog post by u/ExaminationNo8522 helped too.\n Anyways, I wanted to go through it and reproduce for myself on a different dataset, and port to Determined. The result is a beginner friendly notebook + blog post. Check these out if you're interested.\n And of course, let me know if you have thoughts/feedback/comments/issues!\n    submitted by    /u/ishabytes  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq7rd5/p_speech_synthesis_with_mamba_beginner_friendly/",
          "publishedOn": "2024-02-13T23:18:44.000Z",
          "wordCount": 1728,
          "title": "[P] Speech Synthesis with Mamba: Beginner friendly notebook + code",
          "imageUrl": "https://external-preview.redd.it/TkgxTNDh7DxverobqDJQELlZpjix8Uy3b72eStqY3hU.jpg?width=728&height=381.151832461&auto=webp&crop=728:381.151832461,smart&s=dbddfdbe4595c5f6b012425614d95f4ea4231cf7"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq7n16/d_yolov5_memory_usage/",
          "author": null,
          "description": "Hi r/MachineLearning,\n ​\n I have recently been training a custom YOLOv5 dataset for a project I am working on. I notice that when I run train.py, initially the system loads up my RAM with what I assume are the model weights before running the training? \n ​\n I say this because my GPU has near-zero usage for the first minute or so while my RAM usage goes from baseline to almost 16 GB used by Vscode. Is this normal behavior? I am running the YOLOv5m pretrained weights which is a 21.2 M parameter model. GPU memory usage peaks at around 8.3 GB/12 GB. \n ​\n I am training on 1280x1280 px images and a batch size of 4. The larger images are required for my application as I need to identify small features. The batch size is limited by my GPU memory as far as I can tell. \n ​\n Just wanted to see if anyone else has seen this or if the high RAM usage points to some inefficiency or memory leak.\n ​\n Thanks!\n ​\n ​\n ​\n    submitted by    /u/LuckyBucky77  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq7n16/d_yolov5_memory_usage/",
          "publishedOn": "2024-02-13T23:13:42.000Z",
          "wordCount": 1725,
          "title": "[D] YOLOv5 - Memory Usage",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq78ao/d_whats_the_standard_practice_for_setting/",
          "author": null,
          "description": "Hi I am trying to build a modularized LLM application using Langchain where within any individual conversation the app can seamless switch between multiple LLMs to respond to the query, for example:\n  \nUser: What's 1+ 1?\n App (GPT-3.5): 1+1 is 2\n User: Concatenate the last name of the current president of the US with the answer from your last response\n App (Gemini Ultra): Biden2\n  \nI have two technical questions that I hope this subreddit can help answer:\n  \nWhat's the standard practice for setting the initialization prompts or background prompts? For example I want to tell this App that \"your name is Bob\", and I want this App to continuously remember it's Bob regardless how long the conversation has gotten or any switching between LLMs. Do I set this at the beginning of the conversation or before every single response?\n What's the standard practice for conversation memory management when there's switching of LLM involved within one conversation? Do I store all the conversation history within a vector database and do a index search prior to any individual response?\n  \n   submitted by    /u/Try_StockAnalystGPT  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq78ao/d_whats_the_standard_practice_for_setting/",
          "publishedOn": "2024-02-13T22:56:36.000Z",
          "wordCount": 1774,
          "title": "[D] What's the standard practice for setting initialization prompts and maintaining context when switching LLMs within the same conversation?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq572l/r_diffusion_of_thoughts_chainofthought_reasoning/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.07754\n Code: https://github.com/HKUNLP/diffusion-of-thoughts\n Abstract:\n  \nDiffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.\n  \n   submitted by    /u/FastestGPU  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq572l/r_diffusion_of_thoughts_chainofthought_reasoning/",
          "publishedOn": "2024-02-13T21:32:22.000Z",
          "wordCount": 1692,
          "title": "[R] Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq4sp9/r_scaling_laws_for_finegrained_mixture_of_experts/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.07871\n Code: https://github.com/llm-random/llm-random\n Abstract:\n  \nMixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.\n  \n   submitted by    /u/FastestGPU  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq4sp9/r_scaling_laws_for_finegrained_mixture_of_experts/",
          "publishedOn": "2024-02-13T21:16:20.000Z",
          "wordCount": 1713,
          "title": "[R] Scaling Laws for Fine-Grained Mixture of Experts",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq40xo/mutual_information_regularized_offline/",
          "author": null,
          "description": "submitted by    /u/LushousLightfoot  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq40xo/mutual_information_regularized_offline/",
          "publishedOn": "2024-02-13T20:45:03.000Z",
          "wordCount": 1605,
          "title": "Mutual Information Regularized Offline Reinforcement Learning",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq2czi/predicted_output_after_decoding_is_always_empty/",
          "author": null,
          "description": "Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine.\n I created a new project to debug with, my real one is much more complicated, but this code is just to show my issue:\n ```import tensorflow as tf\n import numpy as np\n from keras.callbacks import EarlyStopping\n from keras.layers import Bidirectional, Dropout, BatchNormalization, Embedding, LSTM, Dense\n from keras.optimizers import Adam\n from keras.regularizers import l2\n from keras.models import Sequential\n from keras.preprocessing.sequence import pad_sequences\n from sklearn.model_selection import train_test_split\n user_prompts = np.array([\n \"What's your favorite animal?\",\n \"What's your favorite movie?\",\n \"What's your favorite book?\",\n \"What's your favorite season?\",\n \"What's your favorit…",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq2czi/predicted_output_after_decoding_is_always_empty/",
          "publishedOn": "2024-02-13T19:37:37.000Z",
          "wordCount": 2855,
          "title": "Predicted output after decoding is always empty strings in a list of tokens, but prediction looks fine. [R]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq2cx9/p_gaussian_processes_with_gpytorch_more_output/",
          "author": null,
          "description": "I am following the basic tutorial https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html to train a Gaussian Process for the following data\n train_x is a tensor of [4058, 12] train_y is a tensor of [4058, 140] \n I get an error calculating the loss\n loss = -mll(output, train_y) \n saying that output(model(train_x)) and train_ydon't have the same dimension. Given this, I tried MultitaskGPModelhttps://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html getting a very similar error\n RuntimeError: The size of tensor a (568120) must match the size of tensor b (8116) at non-singleton dimension 0 \n Apparently MultitaskGPModelrequires the same total number of entries to be equal. Is there a way to train a multiple-entry GP?\n    submitted by    /u/WarpDrive2  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq2cx9/p_gaussian_processes_with_gpytorch_more_output/",
          "publishedOn": "2024-02-13T19:37:33.000Z",
          "wordCount": 1669,
          "title": "[P] Gaussian Processes with GPytorch - More output than input data",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq1hux/d_leverage_stable_diffusion_prior_to_detect/",
          "author": null,
          "description": "Hello people. I've been thinking about how a model like Stable Diffusion trained on immense amounts of data could be used to detect anomalies in images in a zero-shot manner. For example, detecting folds and tears in old photos:\n https://preview.redd.it/wqlhl516deic1.png?width=663&format=png&auto=webp&s=e90deb650443f4d0f8c32e3d01d6c9b1941a21d9\n I know there are many works which use SD to inpaint the damaged areas given a mask; but what if we don't have the mask? There are also approaches such as the very cool DiffEdit, where we can use language to localise areas affected the most by a text prompt; but what about the cases where language just isn't precise enough? \n If any images can be inverted to SD's latent space, could the inverted latents tell us something useful about how to detect the damage? Or are there any other properties of either the model or the defects which can be leveraged? Are there any works which try to leverage the distribution learned by SD* to detect the areas which should be inpainted? \n *My intuition is that while SD has definitely been trained on images like this, the presence of deterioration is often not described precisely in language, i.e. this image's caption may be something along the lines of \"old photo\", and the folds are only one of the attributes which \"old photo\" will cover, another one being the sepia tone for example. \n Any pointers and discussion are appreciated!\n ​\n    submitted by    /u/35mmpy  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq1hux/d_leverage_stable_diffusion_prior_to_detect/",
          "publishedOn": "2024-02-13T19:02:38.000Z",
          "wordCount": 1853,
          "title": "[D] Leverage Stable Diffusion Prior to detect contextual anomalies in real images",
          "imageUrl": "https://external-preview.redd.it/wqlhl516deic1.png?overlay-align=bottom,left&crop=663:347.120418848,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=663&height=347.120418848&auto=webp&s=92dc4e95e1aed8083d734527782fd86388469351"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq0ama/d_iclr_openreview_visible/",
          "author": null,
          "description": "I recently submit a paper to ICML, which is rejected from ICLR. I found that my paper in ICLR's openreview console is visible to everyone. Is it OK? As the title of the paper in ICLR and ICML are the same, ICML may not be perfectly anonymous then.\n Do I have to change the visibility manually?\n    submitted by    /u/Shot-Button-9010  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq0ama/d_iclr_openreview_visible/",
          "publishedOn": "2024-02-13T18:15:33.000Z",
          "wordCount": 1703,
          "title": "[D] ICLR openreview visible?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aq06v5/d_r_topic_for_msc_in_aiml/",
          "author": null,
          "description": "Hi everyone,\n Currently I am searching for a topic for my dissertation and I was thinking going for machine vision,\n my idea here is to apply ML to identify drugs or ilicit food in x-ray scan images.\n but I am kind of lost on where to start looking for or even if there is data available for training.\n any help is appreciated\n    submitted by    /u/XicoLeite  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aq06v5/d_r_topic_for_msc_in_aiml/",
          "publishedOn": "2024-02-13T18:11:26.000Z",
          "wordCount": 1624,
          "title": "[D] [R] Topic for MSc in AI/ML",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apzau5/d_is_there_a_way_to_improve_clustering_by/",
          "author": null,
          "description": "Suppose you generate embeddings using two different pre-trained encoders. Is there a way I can leverage these two types of embeddings to get an improved clustering of the data? Is there a term for this, or some prior literature I can read?\n    submitted by    /u/fullgoopy_alchemist  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apzau5/d_is_there_a_way_to_improve_clustering_by/",
          "publishedOn": "2024-02-13T17:36:25.000Z",
          "wordCount": 1647,
          "title": "[D] Is there a way to improve clustering by considering two different types of embeddings?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apyymt/d_is_there_a_need_for_compiled_language_in/",
          "author": null,
          "description": "We all know that machine learning scene is dominated by Python, simple, yet slow language. From my experience languages like Rust, C++, C and Zig are on average 10x faster (it may vary a LOT of course, and the use of C-written modules doesn't help that much for multiple reasons), but they suffer from a need to compile, which takes some time out of development. Still for complex algorithms used in ML they have a clear advantage, once the development process slows down. Why do they have so small market share than? So far I have found literally ONE framework dedicated to run trained NN models called ggml. \n Edit: I have forgot that TensorFlow is actually, mostly written in c++. So that counters my point about compiled languages not having much market share.\n    submitted by    /u/LetsNya  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apyymt/d_is_there_a_need_for_compiled_language_in/",
          "publishedOn": "2024-02-13T17:22:57.000Z",
          "wordCount": 3419,
          "title": "[D] Is there a need for compiled language in complex machine learning projects?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apyymg/d_3k_ml_build_discussion/",
          "author": null,
          "description": "Hello all, \n I am making a custom build pc and want to make sure I can use it for ML since I am taking ml courses currently and would also like to train and test my own models. Disclaimer the build was mostly meant for gaming in mind since that is another use case I will be using it for although I am open to changing parts to increase ML effectiveness.\n I am posting here to check if it’s compatible and effective for machine learning. I am open to part swap suggestions as well. \n GPU: Aero 4080 super\n CPU: Intel Core i9-14900K\n RAM: Corsair Vengeance RGB 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory\n PSU: Corsair RMe (2023) 1200 W 80+ Gold Certified Fully Modular ATX Power Supply\n SSD: Crucial T500 W/Heatsink 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive\n Motherboard: Gigabyte Z790 AORUS ELITE AX ICE\n Cooler: Kraken Elite 360mm\n Case: Lian li vs nzxt (i am still deciding between nzxt mid-towers vs Lian Li O11 Dynamic EVO ATX Mid Tower Case, so for sizing you can keep both in mind however currently favoring lian li o11 slightly)\n Link to build: https://pcpartpicker.com/list/JQckmD\n    submitted by    /u/justdoitjustice  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apyymg/d_3k_ml_build_discussion/",
          "publishedOn": "2024-02-13T17:22:56.000Z",
          "wordCount": 1754,
          "title": "[D] 3K ML Build Discussion",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apyx4t/d_how_are_positional_embedding_solved_when/",
          "author": null,
          "description": "As per the title, how is the problem treated and what is the literature?\n Further, I have sequences that should be positioned on a 3D lattice but whatever, the point is the training is at way shorter length.\n    submitted by    /u/reverendCappuccino  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apyx4t/d_how_are_positional_embedding_solved_when/",
          "publishedOn": "2024-02-13T17:21:21.000Z",
          "wordCount": 1713,
          "title": "[D] How are positional embedding solved when training and test length is different?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apyuj5/p_training_tesseract_on_images_of_text_lines/",
          "author": null,
          "description": "Hello,\n I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.\n Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.\n Any help is highly appreciated!\n    submitted by    /u/SirVampyr  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apyuj5/p_training_tesseract_on_images_of_text_lines/",
          "publishedOn": "2024-02-13T17:18:29.000Z",
          "wordCount": 1653,
          "title": "[P] Training Tesseract on images of text lines + transcriptions?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apxr2n/r_fiddler_cpugpu_orchestration_for_fast_inference/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.07033 \n Github: https://github.com/efeslab/fiddler \n Abstract:\n  \nLarge Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over 3 tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods.\n  \nhttps://preview.redd.it/q9l3fciyqdic1.jpg?width=1338&format=pjpg&auto=webp&s=2e39726c970c655d6ee39f2b68c323204c6b2289\n https://preview.redd.it/epjd0fiyqdic1.jpg?width=1661&format=pjpg&auto=webp&s=701a2d61f8ab50d054db0301a30e40119898dab6\n    submitted by    /u/Singularian2501  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apxr2n/r_fiddler_cpugpu_orchestration_for_fast_inference/",
          "publishedOn": "2024-02-13T16:34:59.000Z",
          "wordCount": 1740,
          "title": "[R] Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models - University of Washington 2024 - Over 10x faster in inference than existing systems!",
          "imageUrl": "https://external-preview.redd.it/q9l3fciyqdic1.jpg?overlay-align=bottom,left&crop=1200:628.272251309,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1200&height=628.272251309&auto=webp&s=40005e58792306f49d3a9261a7a6df8158b91ecc"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apxg42/d_deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/",
          "author": null,
          "description": "Hi everyone,\n In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.\n vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).\n See the detailed how-to here: https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html\n In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.\n Please don't hesitate to share feedbacks about this article, it will be very much appreciated!\n Julien\n    submitted by    /u/juliensalinas  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apxg42/d_deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/",
          "publishedOn": "2024-02-13T16:22:44.000Z",
          "wordCount": 1735,
          "title": "[D] Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM",
          "imageUrl": "https://external-preview.redd.it/rHlwhkKNx_zkZKi7mOfhKyLZkgcurxxrp36uMpgQTFM.jpg?width=800&height=229&auto=webp&crop=800:229,smart&s=d044c4188a9a2c8bcd40fceb7b26252a0938b461"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apwwme/research_a_framework_to_share_analytics_data_in/",
          "author": null,
          "description": "GStreamer has long been the best framework to build pipelines to handle video streams, and in particular, live ones. Engineers have widely adopted it to build video analytics pipelines, and while many companies have indeed built their machine learning analysis framework around GStreamer, no one had made the effort to contribute upstream, until now. \n https://www.collabora.com/news-and-blog/news-and-events/a-framework-to-share-analytics-data-in-gstreamer.html\n    submitted by    /u/mfilion  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apwwme/research_a_framework_to_share_analytics_data_in/",
          "publishedOn": "2024-02-13T16:01:09.000Z",
          "wordCount": 1621,
          "title": "[Research] A framework to share analytics data in GStreamer",
          "imageUrl": "https://external-preview.redd.it/XxaKbnSML1xjCK08CX4gqZjY71Uo9GH_ZJKnFWsnD28.jpg?width=1024&height=536&auto=webp&crop=1024:536,smart&s=2e5615a500435cff84d24cb6406d178c612d37c1"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apwlkm/r_oscopilot_towards_generalist_computer_agents/",
          "author": null,
          "description": "Paper: https://arxiv.org/abs/2402.07456 \n Github: https://github.com/OS-Copilot/FRIDAY \n Abstract:\n  \nAutonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents. \n  \nhttps://preview.redd.it/uzec8udohdic1.jpg?width=1655&format=pjpg&auto=webp&s=893b5561ca47c26c789b69925efdc26e5b783007\n https://preview.redd.it/vfwfwudohdic1.jpg?width=1653&format=pjpg&auto=webp&s=9eafc2a5ea0ad188a156d3de446508d82d9cc913\n https://preview.redd.it/lmi8rwdohdic1.jpg?width=1123&format=pjpg&auto=webp&s=dbc67b27585b980d0c592f9bd9f87f3ec6531f66\n https://preview.redd.it/20yo21eohdic1.jpg?width=1037&format=pjpg&auto=webp&s=72fab36d585b862eed4ff6c7deed2be0cd62f637\n    submitted by    /u/Singularian2501  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apwlkm/r_oscopilot_towards_generalist_computer_agents/",
          "publishedOn": "2024-02-13T15:48:33.000Z",
          "wordCount": 1783,
          "title": "[R] OS-Copilot: Towards Generalist Computer Agents with Self-Improvement - Shanghai AI Laboratory 2024",
          "imageUrl": "https://external-preview.redd.it/uzec8udohdic1.jpg?overlay-align=bottom,left&crop=1200:550,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=1200&height=550&auto=webp&s=4f39e3ea6921a7deb85a89bbb8884aa6fffed7a3"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apwl8v/r_applications_of_gnns_a_survey_video/",
          "author": null,
          "description": "Hi all, I'm sharing my overview explainer video of Graph Neural Networks (GNN) applications:\n 🎥 https://youtu.be/9QH6jnwqrAk?si=nEARUXquZ0aetjCD\n I've compiled a batch of info in one video, highlighting recent breakthroughs and concrete applications of GNNs in 7 diverse areas.\n GNNs have been making rapid crazy strides recently. Despite much less hype than other AI buzzwords, they have powered numerous achievements in the last year alone.\n I plan to create more content on GNNs, like a short series that will dive into (some) technical details of how GNNs work and more. It would be very helpful to hear your thoughts on this one!\n    submitted by    /u/mrx-ai  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apwl8v/r_applications_of_gnns_a_survey_video/",
          "publishedOn": "2024-02-13T15:48:08.000Z",
          "wordCount": 1664,
          "title": "[R] Applications of GNNs - A survey (video)",
          "imageUrl": "https://external-preview.redd.it/Sw_CxNHWf9ODgQx1IGZ3PTc8oAGFlfic80W8n513wfc.jpg?width=480&height=251.308900524&auto=webp&crop=480:251.308900524,smart&s=927bee1e18dd64ef6810dfed81f4c8977f127775"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/",
          "author": null,
          "description": "Recently I've been working on making LLM evaluations fast by using bayesian optimization to select a sensible subset.\n Bayesian optimization is used because it’s good for exploration / exploitation of expensive black box (paraphrase, LLM).\n Project link\n I would love to hear your thoughts and suggestions on this!\n    submitted by    /u/b06901038g  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apv97t/r_p_10_times_faster_llm_evaluation_with_bayesian/",
          "publishedOn": "2024-02-13T14:51:30.000Z",
          "wordCount": 3850,
          "title": "[R] [P] 10 times faster LLM evaluation with bayesian optimization",
          "imageUrl": "https://external-preview.redd.it/qrI7gALZEdRmROpt4OMl-vclk6EgZKScMo5hg6rILVE.jpg?width=1200&height=600&auto=webp&crop=1200:600,smart&s=1efabdc225268895feb136de2b6ab55ea22b8be3"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/",
          "author": null,
          "description": "Hello people. I was just messing around to see the behavior for adversarial attacks on image classifiers, and decided to try it with an object detector as well. I noticed that an untargeted adversarial attack on these models yielded some interested masks. The image classifier generated the usually expected noise mask that is popular, but the object detector under the same conditions generated a mask that closely resembles the objects in question. What is the reasoning behind this? Thank you for your help!\n The first picture is the adversarial mask for the object detector, the second one for an image classifier, and the last picture is the original picture.\n https://preview.redd.it/abo3yj7xddic1.png?width=425&format=png&auto=webp&s=0e73a11997b2c27a6f73832204862d97e5847b4a\n https://preview.redd.it/mbsi6k7xddic1.png?width=425&format=png&auto=webp&s=41de2eca4348afbddfb36154da514046b1be78be\n https://preview.redd.it/zusv0k7xddic1.jpg?width=400&format=pjpg&auto=webp&s=cf08f3911c24e10c632b12e42a976cd35ff3f490\n    submitted by    /u/tatteredsky  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apv6ws/p_why_do_object_detection_model_adversaries_look/",
          "publishedOn": "2024-02-13T14:48:37.000Z",
          "wordCount": 1774,
          "title": "[P] Why do object detection model adversaries look different from image classifiers",
          "imageUrl": "https://external-preview.redd.it/abo3yj7xddic1.png?overlay-align=bottom,left&crop=425:222.513089005,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=425&height=222.513089005&auto=webp&s=ec26538fdde860eacb1b20c6fc081b961148a5c6"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/",
          "author": null,
          "description": "submitted by    /u/Elven77AI  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aprsv9/240207901_fast_factorizable_attention_for/",
          "publishedOn": "2024-02-13T12:00:51.000Z",
          "wordCount": 1682,
          "title": "[2402.07901] FAST: Factorizable Attention for Speeding up Transformers",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/",
          "author": null,
          "description": "https://huggingface.co/blog/TuringsSolutions/pfafresearch\n So this blog went over most peoples heads but basically it captures a lot more information of words compared to vectors it actually seems like it could potentially be better then vectors when scaled up. It has more dimensionality compared to vectors. Other geometric representations are being looked into as well like triangle space, square space and more to find the optimal geometric encoding for word relationships that might be better at modeling information compared to vectors. It has slight score improvements on some benchmarks so as a proof of concept it really does seem to work. Essentially how it works is it converts words to a fractal style numerical embedding to capture richer information.\n https://arxiv.org/abs/2402.06184\n Ironically training was visualized as a fractal recently too so it shouldn't be surprising a word2fractal style embedding process was theorized as well.\n    submitted by    /u/TheCrazyAcademic  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apln6p/rpfas_might_be_a_better_embedding_space_for_words/",
          "publishedOn": "2024-02-13T05:13:48.000Z",
          "wordCount": 2556,
          "title": "[R]PFAS might be a better embedding space for words compared to vectors and possibly even more compute efficient",
          "imageUrl": "https://external-preview.redd.it/z1zhORpF9zwy9ax6Jnx4IMSLSAVid0BZQRf2FYHcJXs.jpg?width=1200&height=628.272251309&auto=webp&crop=1200:628.272251309,smart&s=cee4f0cfdc6a044bc5439108abfdd3a4d5e4b8d3"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/",
          "author": null,
          "description": "I was having trouble tracking deadlines for healthcare ML conferences, so I updated the deadlines website, Added 6 conferences/workshops for healthcare domain.\n https://openlifescience-ai.github.io/ai-deadlines/?sub=ML,CV,NLP,SP \n Now track easily the paper submission deadline & even add it to your Google calendar.\n See a conference missing? Please contribute here: https://github.com/openlifescience-ai/ai-deadlines\n https://preview.redd.it/dfxxlep679ic1.png?width=829&format=png&auto=webp&s=686aa600ad863b26d28211021e802708a79957a8\n    submitted by    /u/aadityaura  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apgsz0/p_tracking_healthcare_domain_conference_deadlines/",
          "publishedOn": "2024-02-13T01:14:39.000Z",
          "wordCount": 1607,
          "title": "[P] Tracking Healthcare Domain Conference Deadlines",
          "imageUrl": "https://external-preview.redd.it/dfxxlep679ic1.png?overlay-align=bottom,left&crop=829:434.031413613,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=829&height=434.031413613&auto=webp&s=8ef0cf9ae1dbb36089a10061d559ce130e064aa4"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/",
          "author": null,
          "description": "What frameworks and libraries are you using in your RAG? \n I'm most curious if LangChain is as popular as it was?\n Here's mine at a high-level: \n  \n langchain to use OpenAI for creating embeddings\n Pinecone for storing embedding\n langchain to load document splitters and characters splitters for chunking\n Mongo for conversations memory\n  \n​\n    submitted by    /u/EnvironmentalDepth62  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/",
          "publishedOn": "2024-02-12T22:14:07.000Z",
          "wordCount": 4154,
          "title": "Whats in your RAG setup? [D]",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/",
          "author": null,
          "description": "A large fraction of recently released LLMs are using RMSNorm instead of LayerNorm.\n The original RMSNorm paper (https://arxiv.org/pdf/1910.07467.pdf) and most references I've seen argue that RMSNorm is better than LayerNorm because it is much more computationally efficient.\n However, LayerNorm is a tiny fraction of overall compute, so it's not clear to me why that speedup would help very much. Asymptotically, LayerNorm is O(d_model), while there are components like the MLP that are O(d_model2 ), or attention that is O(d_model*seq_len + d_model2 ).\n Is it just that the mean centering part of LayerNorm is not all that useful, and so RMSNorm gives you a minor efficiency boost without any important loss in expressivity? Or does RMSNorm have other benefits I'm not seeing?\n    submitted by    /u/kei147  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1apb3th/d_why_does_it_matter_that_rmsnorm_is_faster_than/",
          "publishedOn": "2024-02-12T21:09:53.000Z",
          "wordCount": 2122,
          "title": "[D] Why does it matter that RMSNorm is faster than LayerNorm in transformers?",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/",
          "author": null,
          "description": "New episode of Imbue's Generally Intelligent podcast with Tri Dao, author of FlashAttention and Chief Scientist at Together AI.\n Some topics covered:\n  \nTaking a contrarian bet on recurrent connections over attention\n Using data augmentation to encode knowledge into models\n Designing algorithms that take advantage of hardware\n  \nListen to the conversation:\n  \nSpotify\n Apple Podcasts\n Pocket Casts\n Highlights and referenced papers\n  \n   submitted by    /u/thejashGI  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1ap9v6v/d_interview_with_tri_dao_stanford_on/",
          "publishedOn": "2024-02-12T20:20:59.000Z",
          "wordCount": 1750,
          "title": "[D] Interview with Tri Dao, Stanford: On FlashAttention and sparsity, quantization, and efficient inference",
          "imageUrl": "https://external-preview.redd.it/L8HcEujHVDGsoQ23CE26G6rlO1lcowzs2cegoAXr7Ks.jpg?width=1200&height=628.272251309&auto=webp&crop=1200:628.272251309,smart&s=2dbbc11125712b97accb145725b2664bb7ac897f"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/",
          "author": null,
          "description": "Worth reading just for the neat visualizations.\n https://sohl-dickstein.github.io/2024/02/12/fractal.html\n https://arxiv.org/abs/2402.06184\n    submitted by    /u/currentscurrents  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1ap92d8/r_the_boundary_of_neural_network_trainability_is/",
          "publishedOn": "2024-02-12T19:50:07.000Z",
          "wordCount": 2442,
          "title": "[R] The boundary of neural network trainability is fractal - Jascha Sohl-Dickstein",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/",
          "author": null,
          "description": "It is well known that batch inference is a common practice for efficient LLM serving (which is one primary reason why services like ChatGPT have an initial delay). This batching practice is motivated by the fact that inference latency is mostly limited by the I/O cost of model loading but not the actual compute, where serving multiple requests in a batched manner adds tolerable latency increase while bringing in massive savings on cost per token. However, one issue of batched inference (or long context tasks, or both) is the massive KV cache required. As illustrated in this previous paper by Jeff Dean: a 500B+ model with bs=512 and seqlen=2048 has a total KV cache about 3TB — this is 3 times the model weight and brings another I/O challenge as the GPU will need to load the entire KV cache …",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1ap3b65/rp_kv_cache_is_huge_and_bottlenecks_llm_inference/",
          "publishedOn": "2024-02-12T16:00:37.000Z",
          "wordCount": 3490,
          "title": "[R][P] KV Cache is huge and bottlenecks LLM inference. We quantize them to 2bit in a finetuning-free + plug-and-play fashion.",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        },
        {
          "id": "https://old.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/",
          "author": null,
          "description": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n Thread will stay alive until next one so keep posting after the date in the title.\n Thanks to everyone for answering questions in the previous thread!\n    submitted by    /u/AutoModerator  \n [link]   [comments]",
          "link": "https://old.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/",
          "publishedOn": "2024-02-11T16:00:18.000Z",
          "wordCount": 1922,
          "title": "[D] Simple Questions Thread",
          "imageUrl": "https://www.redditstatic.com/new-icon.png"
        }
      ]
    },
    {
      "title": "Machine Learning Questions",
      "feedUrl": "https://www.reddit.com/r/MLQuestions.rss",
      "siteUrl": "https://www.reddit.com/r/MLQuestions",
      "articles": [
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aqj2xx/what_to_choose_between_job_with_little_ml_vs/",
          "author": null,
          "description": "Hi! I have this dilemma right now. I'm currently working as a Computer Vision engineer in my company. Unfortunately, the majority of my tasks do not include any Computer vision, and it's often some code tasks or creating algorithms of how to automatically control robot with PID controllers and stuff. However, there are sometimes Computer Vision tasks such as using opencv trackers. That said, its important to notice, that I can have my time learning different stuff while working and nobody really pushes me there.\n I've also joined an ML internship in another company. The project I'm working on there is EDA analysis for tabular data. By the end of this internship, I will probably get an offer from the company, but I dont know whether Im gonna be working with Computer Vision.\n That said, Im currently in a situation, where, due to my university, I must choose between them. It's important to note that I want to grow as a Computer Vision developer. What should I choose between my job and internship? Can I become a Strong Junior dev just by searching and learning stuff from books and pet-projects?\n    submitted by    /u/tepes_creature_8888  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aqj2xx/what_to_choose_between_job_with_little_ml_vs/",
          "publishedOn": "2024-02-14T09:38:18.000Z",
          "wordCount": null,
          "title": "What to choose between job with little ML vs internship in ML company?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aqfi8a/how_to_think_about_tensorsvector_dimensions/",
          "author": null,
          "description": "submitted by    /u/jdjddhdjdjdj  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aqfi8a/how_to_think_about_tensorsvector_dimensions/",
          "publishedOn": "2024-02-14T05:37:52.000Z",
          "wordCount": null,
          "title": "How to think about Tensors/Vector dimensions intuitively?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aqda31/faq_chatbot/",
          "author": null,
          "description": "I have faqs and answers .when ever user asks question related to the faq the bot should give response.the faq size is very huge.Iam new to ML.iam thinking of implementing classification model with feed forward neural network.I will create some sample questions for a faq and create a class for each question.is this model feasible when the data is huge ?. Please suggest if there is any appropriate model for this.\n    submitted by    /u/Intelligent_Usual392  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aqda31/faq_chatbot/",
          "publishedOn": "2024-02-14T03:37:59.000Z",
          "wordCount": null,
          "title": "Faq chatbot",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aq7894/how_do_you_reassemble_full_images_from_models/",
          "author": null,
          "description": "In order to do image processing on large or irregularly shaped images one strategy is to divide the input image into patches, say 64x64 pixels. How do you reassemble those patches into a large image again after inference? Obviously you can just concatenate them, but that would risk edges where the patches meet. Sliding windows with overlap and averaging can result in blurring. This is less of a problem when doing segmentation, but what about other image processing?\n If you're doing super resolution or style transfer or anything that results in a new natural image, how would you merge patches back into a full size image? Are there any standard ways of doing it, or even comparisons between multiple methods?\n I tried searching, but all I can find is how to split images into patches, not the other way around.\n    submitted by    /u/bearnaisepudding  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aq7894/how_do_you_reassemble_full_images_from_models/",
          "publishedOn": "2024-02-13T22:56:32.000Z",
          "wordCount": null,
          "title": "How do you reassemble full images from models that use patches?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aq3jke/train_a_model_with_our_voices_and_music/",
          "author": null,
          "description": "Hello,\n In my family, we have this special kind of music and singing that is from a really small part in Europe, and I really want to keep it safe for the future and experiment with on an AI/LLM model (I don’t know that exact name).\n Is there any kind of model where I can teach it to use our voices and music, so I can make songs with it using my and family’s own voice and our music?\n Many thanks in advance!\n    submitted by    /u/magicmetagic  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aq3jke/train_a_model_with_our_voices_and_music/",
          "publishedOn": "2024-02-13T20:25:14.000Z",
          "wordCount": null,
          "title": "Train a model with our voices and music?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aq2y9q/can_someone_explain_bert_to_me_i_am_having/",
          "author": null,
          "description": "I understand what BERT is, and I am very new to my ML journey. I was wondering how it is built upon to create personalized things like chatbots, generating text summaries, etc. If anyone could provide me with some insight, I would greatly appreciate it!\n    submitted by    /u/No-Buffalo-2565  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aq2y9q/can_someone_explain_bert_to_me_i_am_having/",
          "publishedOn": "2024-02-13T20:01:24.000Z",
          "wordCount": null,
          "title": "Can someone explain BERT to me? I am having trouble grasping how it is used.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aq2kuj/sound_identification_on_edge_microcontrollers/",
          "author": null,
          "description": "Hi,\n I would like to deploy a simple app on an ESP32-S3 microcontroller that can identify when a cat meowing is heard. How feasible is that? How would you suggest I approach it?\n TensorFlow has TinyML, but it seems to be a lot of work to get it running on edge devices like the ESP32. Is that really the best option?\n TIA, -T\n    submitted by    /u/gamename  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aq2kuj/sound_identification_on_edge_microcontrollers/",
          "publishedOn": "2024-02-13T19:46:28.000Z",
          "wordCount": null,
          "title": "Sound Identification On Edge Microcontrollers",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apz1ua/cant_access_private_gradio_spaces_api_endpoint/",
          "author": null,
          "description": "I am trying to access my Huggingface space through a Postman request. When I copy the exported curl command, it looks like this:\n  curl --location 'https://MY_SPACE_NAME_HERE.hf.space/--replicas/RANDOMDIGITS/' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer hf_REST_OF_MY_TOKEN_HERE' \\ --data '{\"data\": [\"data:image/jpeg;base64,PIC_IN_BASE64_HERE\"]}'  \n But when I run this request, I get:\n  { \"detail\": \"Method Not Allowed\" }  \n How do get this request to succeed? \n Also my API endpoint works perfectly when I access it via the boilerplate python code that Gradio gives me. Do you know what the problem is?\n    submitted by    /u/warpanomaly  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apz1ua/cant_access_private_gradio_spaces_api_endpoint/",
          "publishedOn": "2024-02-13T17:26:34.000Z",
          "wordCount": null,
          "title": "Can’t Access Private Gradio Spaces API Endpoint Through Postman",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apyv59/training_tesseract_on_images_of_text_lines/",
          "author": null,
          "description": "Hello,\n I am immensely confused about how to train Tesseract for OCR with just images + txts. It suppossedly works, but I can't get it to work.\n Does anyone here have experience with it and could provide me with some help / an example on how to use it properly? I know there is some stuff on the GitHub repo, but (as I said) it's really confusing to me on how to actually use it.\n Any help is highly appreciated!\n    submitted by    /u/SirVampyr  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apyv59/training_tesseract_on_images_of_text_lines/",
          "publishedOn": "2024-02-13T17:19:07.000Z",
          "wordCount": null,
          "title": "Training Tesseract on images of text lines + transcriptions?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apwzte/hardware_requirements/",
          "author": null,
          "description": "Hi,\n I am on a Mac M1 and want to do model training on images, obviously no GPU. Last time I tried it took me forever just for a few iterations, when for good results I'd need minimum 400k and ideally more than a 1 million.\n I was interested into cloud computing solutions, but when I looked at the pricing (e.g. google cloud tensorflow) my wallet started to shake unexplainably.\n Do any of you have any recommendation in terms of what are the best options out there for my purpose?\n    submitted by    /u/friedbat  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apwzte/hardware_requirements/",
          "publishedOn": "2024-02-13T16:04:43.000Z",
          "wordCount": null,
          "title": "Hardware requirements",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apurzj/am_i_misunderstanding_what_gpt3_training_data_is/",
          "author": null,
          "description": "Hi there I hope you're all well,\n I am writing a paper with my PhD supervisor and it has been noted that these values add up to 101%.\n Am I misunderstanding what this represents or should it be 100%?\n Thank you for any information you may be able to provide.\n ​\n ​\n https://preview.redd.it/xi33pf705dic1.png?width=948&format=png&auto=webp&s=01e29567fe9ae50fc4b8f2bd1219891bd83ae695\n ​\n https://preview.redd.it/5uz3zqlx4dic1.png?width=357&format=png&auto=webp&s=a857e8425a4b8ef23faff5e0d08ce53fde831c9c\n    submitted by    /u/richard93UK  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apurzj/am_i_misunderstanding_what_gpt3_training_data_is/",
          "publishedOn": "2024-02-13T14:29:32.000Z",
          "wordCount": null,
          "title": "Am I misunderstanding what GPT-3 training data is?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apq8jb/i_need_you_help/",
          "author": null,
          "description": "Hello everyone,\n I'm a machine learning student trying to help friends who are organizing a festival.\n There will be around 300 volunteers and I thought I could automate the planning wich take a lot of time.\n There are different stands where volunteers are placed and this spread over two days : Friday and Saturday.\n Each volunteer chose their three favorite stands in an order of preference from a list.\n Here the file Pole.xlsx with the name of each stand in color\n They also indicate the people they would like to be with with the format :\n BARACK Obama, BARACK Obama, BARACK Obama, ...\n So quite a few conditions... I get an Excel extraction of the first answers.\n Here are the headers of the file I keep : \n Inscriptions.xlsx\n I've already tried to do it with a greedy algorithm and Gale Shapley but I can't achieve much.\n Do you have any ideas for accomplishing this task ? \n Thanks in advance.\n    submitted by    /u/Hmouimaisnon  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apq8jb/i_need_you_help/",
          "publishedOn": "2024-02-13T10:19:31.000Z",
          "wordCount": null,
          "title": "I need you help",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apkro9/p_need_help_creating_a_tflite_modelmaker/",
          "author": null,
          "description": "submitted by    /u/Geo_The_Legend  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apkro9/p_need_help_creating_a_tflite_modelmaker/",
          "publishedOn": "2024-02-13T04:27:40.000Z",
          "wordCount": null,
          "title": "[P] Need Help Creating a TFLite ModelMaker EfficientDet-Lite0 Detector-like Wrapper for YOLOv8",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apjf83/any_good_ml_workshops_in_the_us/",
          "author": null,
          "description": "I’m an engineer in tech industry working with mostly ML research scientists. My dept is encouraging me to take a course or tutorial/workshop and I want to use the opportunity to learn more in the ML space. Wondering if there were any good workshops in person you all know of? I’ve got a baseline understanding and have taken ML coursework, but would love to find a workshop / week long course that deep dives into an area or multiple areas of ML.\n    submitted by    /u/drdrrr  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apjf83/any_good_ml_workshops_in_the_us/",
          "publishedOn": "2024-02-13T03:19:51.000Z",
          "wordCount": null,
          "title": "Any good ML workshops in the US?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aphhrr/data_preprocessing_help_beginner_here/",
          "author": null,
          "description": "​\n https://preview.redd.it/s13qle1kc9ic1.png?width=1546&format=png&auto=webp&s=d67b8be7f6036eec39988b5a741f16edb6d0ac6e\n Beginner Here - i uploaded a snippet of the dataset \n Am trying to create the infamous price prediction model for houses.\n I uploaded my dataset, reduced some data columns, splitted the independent variables and the target variable, then split them into train and test datasets, detected(using histograms and boxplots) and deleted some outliers, scaled the features, and label encoded the non-numerical data [ oneHotEncoding is impossible because of TOO MANY unique values] and finally trained my model - linear regression using Sklearn.\n I scaled the test independent variables and I got an R2 score of about 0.2 and a high MAE score.\n Problem: I can't understand why the score are bad. The steps of the model training are correct. So probably the data preprocessing is wrong. I tried other model training such as knn regression and random forest regression but ended up with bad results. \n More details:\n - Deleted \"City\" and \"Province\" columns since I have longitude and latitude.\n - Label encoded \"Address\". Removing does not do anything and I think it should not be removed as the prices changes drastically between different values of this feature.\n    submitted by    /u/gtfryh352  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aphhrr/data_preprocessing_help_beginner_here/",
          "publishedOn": "2024-02-13T01:47:24.000Z",
          "wordCount": null,
          "title": "Data Preprocessing Help - Beginner Here",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aphaic/training_transformer_based_language_model_from/",
          "author": null,
          "description": "submitted by    /u/gubberex  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aphaic/training_transformer_based_language_model_from/",
          "publishedOn": "2024-02-13T01:37:45.000Z",
          "wordCount": null,
          "title": "Training Transformer based language model from scratch",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apcefp/need_help_in_advancing_in_ai_field_as_a_new_grad/",
          "author": null,
          "description": "I studied computer engineering (2023 pass out) in India and am currently employed as a machine learning engineer at a growing startup\n ​\n A little background about me \n - Average Student in College\n - Can do average coding in python, Data Analysis, SQL, ML/DL, CV/NLP, GenAI, etc. \n - Tried academia in my last year but felt like it was not my cup of tea\n - Hunt for internships and got one; handled the whole ML part alone for 3-4 months since the previous team left\n - Placed at the same startup as MLE (~Rs.12 LPA)\n - Spent first time out of my hometown alone in new city.\n - Felt good in initial 3-4 months where I had parties, worked day-night and had fun\n - Slowly I realized how fast things are moving in the outside world and I was still stuck at GPT2 \n ​\n My problem:\n - As the field progres…",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apcefp/need_help_in_advancing_in_ai_field_as_a_new_grad/",
          "publishedOn": "2024-02-12T22:01:56.000Z",
          "wordCount": null,
          "title": "Need help in advancing in AI field as a new grad",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1apbbwv/question_regarding_the_training_process_of_a/",
          "author": null,
          "description": "Hello everyone,\n I am currently training a neural network for a regression task (to predict prices in housing dataset). I have been testing out different parameter combinations on my network but noticed something a bit weird. When training my model, the first epoch always as an abnormal large value that suddenly decreases by a lot:\n Epoch 1/20 loss: 0.4525 - mse: 0.4508 - val_loss: 0.1827 - val_mse: 0.0653\n Epoch 2/20 - loss: 0.1556 - mse: 0.0618 - val_loss: 0.1367 - val_mse: 0.0552 \n Epoch 3/20 loss: 0.1451 - mse: 0.0577 - val_loss: 0.1396 - val_mse: 0.0592 \n Epoch 4/20 loss: 0.1455 - mse: 0.0581 - val_loss: 0.1369 - val_mse: 0.0564 \n Epoch 5/20 loss: 0.1289 - mse: 0.0497 - val_loss: 0.0932 - val_mse: 0.0422 \n Epoch 6/20 - loss: 0.1300 - mse: 0.0502 - val_loss: 0.1053 - val_mse: 0.0482 \n Epoch 7/20 - loss: 0.1255 - mse: 0.0476 - val_loss: 0.1056 - val_mse: 0.0454 \n Epoch 8/20 - loss: 0.1271 - mse: 0.0482 - val_loss: 0.1416 - val_mse: 0.0537 \n Epoch 9/20 - loss: 0.1279 - mse: 0.0489 - val_loss: 0.1217 - val_mse: 0.0507 \n Epoch 10/20 - loss: 0.1232 - mse: 0.0489 - val_loss: 0.1232 - val_mse: 0.0464 \n I have tried batch normalization, dropout, learning_rate and do not see any improvements.\n My issue here is when looking at the plots of the train and validation loss i am unable to clearly see the difference between both metrics. I believe my model is appropriate for the data but i am unable to reduce that single first epoch any ideas? I have bellow the metrics that i have for both train and test. In this scenario should I not emphasize this plot as much? Thank you \n Measure Train Test \n MSE 0.03 0.04 \n MAE 0.09 0.09 \n R-squared 0.81 0.76 \n    submitted by    /u/Minute-Fix-1493  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1apbbwv/question_regarding_the_training_process_of_a/",
          "publishedOn": "2024-02-12T21:18:56.000Z",
          "wordCount": null,
          "title": "Question regarding the training process of a neural network",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ap9fvz/what_is_the_best_model_for_multiclass/",
          "author": null,
          "description": "I have a csv file with 70000 rows and 20 columns. I want to make a model where I can input column values and the model will predict the column header for me. What is the best model to predict column headers using column values? How do I approach this task in a step by step manner?\n    submitted by    /u/Leading_Particular60  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ap9fvz/what_is_the_best_model_for_multiclass/",
          "publishedOn": "2024-02-12T20:04:37.000Z",
          "wordCount": null,
          "title": "What is the best model for multiclass classification",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aothfs/recommended_model_for_numerous_features_and_a/",
          "author": null,
          "description": "I'm looking for a suitable algorithm to predict whether a customer will purchase an item or not based on a set of features. The output is binary - either 0 (no purchase) or 1 (purchase). The goal is to get the highest possible percentage of correct predictions\n Some additional info\n  \nThere's an significant amount of available features, up to 50 different ones. \n \nThere's an abundance of labeled data - close to a million entries that can be utilized\n \nOnly 1% of the entries have are purchased (minority class), the remaining 99% aren't (majority class), meaning there is an imbalance.\n \n I'm providing a sample below with only part of the features.\n  \n item name customer GEO customer gender device type OS customer age Employed Annual Income Married Has Children Purchased Item? \n  \n Vacuum Cleaner XYZ USA Female PC Windows 45 Yes 200k Yes Yes No \n \n Please advise as to what type of alghorithm to utilize for this scenario, that are likely to have the highest % of accurate predictions.\n    submitted by    /u/miabananaz  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aothfs/recommended_model_for_numerous_features_and_a/",
          "publishedOn": "2024-02-12T06:23:45.000Z",
          "wordCount": null,
          "title": "Recommended model for numerous features and a binary output?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aot79r/style_transfer_with_target_images_provided/",
          "author": null,
          "description": "In most style transfer models I have seen, there is no target image but rather an image classifier network that uses internal features learned which is used to identify the style of each image. The training error is calculated by checking if the new image learns the same features of the 2 input kmages. What if I was to create a synthetic data that contained the target image that both styles mixed would output to. What should I use as loss function here? MSE? I read somewhere that it doesn't work well. What other loss functions can be best used in the case where the target images are provided and output of the model should match the target image?\n    submitted by    /u/silently--here  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aot79r/style_transfer_with_target_images_provided/",
          "publishedOn": "2024-02-12T06:06:24.000Z",
          "wordCount": null,
          "title": "Style Transfer with target images provided.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aosl1o/will_the_people_that_develop_and_create_ai_be/",
          "author": null,
          "description": "Will machine learning engineers and data scientists be replaced by AI?\n    submitted by    /u/Daniu_13  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aosl1o/will_the_people_that_develop_and_create_ai_be/",
          "publishedOn": "2024-02-12T05:30:47.000Z",
          "wordCount": null,
          "title": "Will the people that develop and create AI be replaced by AI?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aoshzm/sklearnquantile_has_quantilerandomforestregrssor/",
          "author": null,
          "description": "submitted by    /u/Sea-Coconut-3833  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aoshzm/sklearnquantile_has_quantilerandomforestregrssor/",
          "publishedOn": "2024-02-12T05:25:59.000Z",
          "wordCount": null,
          "title": "Sklearn.quantile has quantilerandomforestregrssor, i want help to use same model to predict mean prediction too, as i am predicting for quantiles",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aok4n1/are_treesrandom_forests_still_used_given_the/",
          "author": null,
          "description": "I found a thread from a few years back that was saying yes because they are easier/cheaper to implement and even may perform better when there are fewer features with little interaction between them. \n I was wondering if that's still true in 2024.\n    submitted by    /u/Apprehensive_Act4898  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aok4n1/are_treesrandom_forests_still_used_given_the/",
          "publishedOn": "2024-02-11T22:14:49.000Z",
          "wordCount": null,
          "title": "are trees/random forests still used given the advances in neural networks?",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1aoiiwm/text_classification_to_15/",
          "author": null,
          "description": "Hi, I have a question.\n So I have a CSV file with transcripts which is text and the each transcript has a rating from 1-5 (this can be used as training data). I have another CSV with transcripts but no ratings. I want to be able to predict the ratings. What model do you suggest for something like this or how do you suggest I go about this? Thank you so much for you help!\n    submitted by    /u/Eem2323  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1aoiiwm/text_classification_to_15/",
          "publishedOn": "2024-02-11T21:05:49.000Z",
          "wordCount": null,
          "title": "Text classification to 1-5",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ao5r6s/developing_a_chrome_extension_to_filter_hateful/",
          "author": null,
          "description": "Hey Reddit community,\n ​\n My college project team consisting of two members is working on developing a Chrome extension aimed at filtering out hateful speech and negative comments encountered by users while surfing the web. Our goal is to create a tool that promotes a more positive online experience for users.\n Details:\n Functionality: Our extension will monitor the websites users visit in real-time. When it detects any content containing hateful speech or negative comments, it will automatically hide or filter out such content from the user's view.\n Feasibility: While we're excited about this project, we're also mindful of its feasibility. As two members, we want to ensure that the scope of the project is manageable within the given timeframe and resources. We're open to any suggestions or advice on how to approach the development process efficiently.\n Technical Considerations: We're particularly interested in insights on the technical aspects of building such an extension. Should we focus on specific programming languages or frameworks? Are there any existing libraries or APIs that could assist us in detecting and filtering out negative content effectively?\n Ethical Concerns: Additionally, we're aware of the ethical implications of content moderation. How can we strike a balance between filtering harmful content and respecting freedom of expression? Are there any best practices or guidelines we should adhere to in this regard?\n Any advice, tips, or resources you can provide would be greatly appreciated. We're eager to learn and make meaningful contributions to this project. Thanks in advance for your help!\n    submitted by    /u/jerry_10_  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ao5r6s/developing_a_chrome_extension_to_filter_hateful/",
          "publishedOn": "2024-02-11T11:17:22.000Z",
          "wordCount": null,
          "title": "Developing a Chrome Extension to Filter Hateful Speech and Negative Comments - Advice Needed for College Project.",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ao467y/get_alerted_when_any_new_code_is_released_for_a/",
          "author": null,
          "description": "Just built out something for this community I thought i'd personally share 🙂 Would love your feedback :)\n You can now get alerted when any new code is released for a given paper or topic! You can select any paper or topic as you're browsing the internet (Google, Scholar, Arxiv, IEEE, etc.)\n Just install the code finder extension (Chrome: https://chromewebstore.google.com/detail/ai-code-finder-for-papers/aikkeehnlfpamidigaffhfmgbkdeheil | Firefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/ | Edge: https://microsoftedge.microsoft.com/addons/detail/get-papers-with-code-ever/mflbgfojghoglejmalekheopgadjmlkm), click on any bell/alert icon you come across while browsing and follow the next steps on the screen 🙂\n Also, with alerts \n  \nget the latest developments in your area of interest delivered straight to your inbox.\n Author's newest work: be the first to know when an author releases new papers.\n  \n​\n https://preview.redd.it/wsssu3pcdxhc1.png?width=3074&format=png&auto=webp&s=c695d93161e47f4233850c8781cda99f76d8c97e\n https://preview.redd.it/jshzj3pcdxhc1.png?width=1848&format=png&auto=webp&s=cd64d93e9535dc920a5ec633e8781db42326f902\n https://preview.redd.it/t0s7g3pcdxhc1.png?width=1890&format=png&auto=webp&s=e077f6ed446d4211cb89d4aee4674eb8f8ad0293\n    submitted by    /u/MLtinkerer  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ao467y/get_alerted_when_any_new_code_is_released_for_a/",
          "publishedOn": "2024-02-11T09:27:47.000Z",
          "wordCount": null,
          "title": "Get alerted when any new code is released for a given paper or topic! Would love your feedback :)",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ao3vjs/why_gemini_ultra_outshines_chatgpt_4_performance/",
          "author": null,
          "description": "submitted by    /u/UseCreative4765  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ao3vjs/why_gemini_ultra_outshines_chatgpt_4_performance/",
          "publishedOn": "2024-02-11T09:07:05.000Z",
          "wordCount": null,
          "title": "Why Gemini Ultra Outshines ChatGPT 4: Performance Comparison",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ao1u2l/industrial_engineer_thinking_to_get_into_machine/",
          "author": null,
          "description": "I'm industrial engineer and planning to work in machine learning do you advice me to do that ? and if yes how should i start? \n    submitted by    /u/Lopsided-Adeptness42  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ao1u2l/industrial_engineer_thinking_to_get_into_machine/",
          "publishedOn": "2024-02-11T06:50:40.000Z",
          "wordCount": null,
          "title": "Industrial engineer thinking to get into machine learning",
          "imageUrl": null
        },
        {
          "id": "https://www.reddit.com/r/MLQuestions/comments/1ao0fxs/sentiment_analysis_on_arabic_reviews/",
          "author": null,
          "description": "I have a dataset with review text in arabic and also a rating column(out of 100) It is a part of a data science assessment for a jon opportunity. The objective is to perform sentiment analysis and produce insights on positive,neutral,negative reviews. I am unable to decide whether I should: 1. Simply use the rating column to get sentiment(pos/neu/neg) and then start getting insights? May or may not train a supervised training model. 2. Go via the route of an unsupervised setting using these reviews and then analyse which were identified as pos/neu/neg\n    submitted by    /u/Vishesh1597  \n [link]   [comments]",
          "link": "https://www.reddit.com/r/MLQuestions/comments/1ao0fxs/sentiment_analysis_on_arabic_reviews/",
          "publishedOn": "2024-02-11T05:25:40.000Z",
          "wordCount": null,
          "title": "Sentiment Analysis on Arabic reviews",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "ML in Production",
      "feedUrl": "https://mlinproduction.com/feed/",
      "siteUrl": "https://mlinproduction.com/",
      "articles": []
    },
    {
      "title": "Akshaj Wields Pytorch on Medium",
      "feedUrl": "https://medium.com/feed/tag/akshaj-wields-pytorch",
      "siteUrl": "https://medium.com/tag/akshaj-wields-pytorch/latest?source=rss------akshaj_wields_pytorch-5",
      "articles": []
    },
    {
      "title": "Deep Learning on Medium",
      "feedUrl": "https://medium.com/feed/tag/deep-learning",
      "siteUrl": "https://medium.com/tag/deep-learning/latest?source=rss------deep_learning-5",
      "articles": [
        {
          "id": "https://medium.com/p/615c2a100592",
          "author": "Sik-Ho Tsang",
          "description": "Foundation Model or Large Multimodal Model (LMM) Accepting Textual, Visual, and Audio as Inputs\nContinue reading on Medium »",
          "link": "https://sh-tsang.medium.com/review-gemini-a-family-of-highly-capable-multimodal-models-615c2a100592?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:49:08.000Z",
          "wordCount": 2746,
          "title": "Review — Gemini: A Family of Highly Capable Multimodal Models",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*3VPK1Q83xLHkDbMgfX7A9A.jpeg"
        },
        {
          "id": "https://medium.com/p/06541ecc624a",
          "author": "Cryptodesign AI",
          "description": "In the realm of interior design, the fusion of creativity and technology has sparked a revolution, and at the forefront of this innovation…\nContinue reading on Medium »",
          "link": "https://medium.com/@cryptodesignai/decogenius-unveiling-the-complexity-of-a-neural-visionary-06541ecc624a?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:42:12.000Z",
          "wordCount": 1705,
          "title": "DecoGenius: Unveiling the Complexity of a Neural Visionary",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*FaidsUQ21Z8_28S17I3DyQ.png"
        },
        {
          "id": "https://medium.com/p/44cc9e870a32",
          "author": "Danish Elahi",
          "description": "This post will discuss the importance of norms and different kinds of metrics and vectors.\nContinue reading on Medium »",
          "link": "https://medium.com/@danishelahi016/norms-and-special-kinds-of-metrices-and-vectors-44cc9e870a32?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:24:56.000Z",
          "wordCount": 1865,
          "title": "Norms and Special Kinds of Metrices and Vectors.",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*BIfwc2I2o3J2etlL"
        },
        {
          "id": "https://medium.com/p/d456fd10d727",
          "author": "Careervira",
          "description": "In this blog, we will delve into the detailed comparison between deep learning vs machine learning from basic parameters to career points.\nContinue reading on Medium »",
          "link": "https://medium.com/@careervira.community/deep-learning-vs-machine-learning-the-ultimate-comparison-d456fd10d727?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:19:42.000Z",
          "wordCount": 3630,
          "title": "Deep Learning VS Machine Learning: The Ultimate Comparison",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1000/0*QOgMfQzXXxsc4aVS"
        },
        {
          "id": "https://medium.com/p/015e48113a4b",
          "author": "Iqra Hamid",
          "description": "Continue reading on Medium »",
          "link": "https://medium.com/@iqrah191999/humanity-doesnt-mean-selfishness-015e48113a4b?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:12:02.000Z",
          "wordCount": 625,
          "title": "“Humanity doesn’t mean selfishness”",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/ac9178184232",
          "author": "JaffaBranding",
          "description": "Image: Bing Image Generator\nContinue reading on Write A Catalyst »",
          "link": "https://medium.com/write-a-catalyst/how-to-do-more-in-2-hours-than-others-do-in-2-weeks-ac9178184232?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T12:11:41.000Z",
          "wordCount": 1367,
          "title": "How To Do More In 2 Hours Than Others Do In 2 Weeks",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1024/1*JjXQZzdvmxLucpqLguyFqA.jpeg"
        },
        {
          "id": "https://medium.com/p/1d44822d4b3c",
          "author": "Ashish Malhotra",
          "description": "Word2Vec is a popular technique in natural language processing for word embedding, where words are represented as dense vectors in a…\nContinue reading on Medium »",
          "link": "https://medium.com/@ypredofficial/word2vec-decoded-1d44822d4b3c?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T11:39:41.000Z",
          "wordCount": 2585,
          "title": "Word2Vec Decoded",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/666b84788099",
          "author": "Anju Reddy K",
          "description": "Last week we learned about how multi-layer perceptron, cost function, hyper-parameters, weights and bias work. \nNow, in this week we will…\nContinue reading on Medium »",
          "link": "https://medium.com/@anju75061/deep-learning-tutorial-week-03-666b84788099?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T11:31:44.000Z",
          "wordCount": 4312,
          "title": "Deep Learning Tutorial — Week 03",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*x9_tf-PbgRnatynsTX6TQg.jpeg"
        },
        {
          "id": "https://medium.com/p/58c6b8af26ee",
          "author": "Makomingu",
          "description": "Jam 2 pagi itu mending tidur, bukan mikir \nContinue reading on Medium »",
          "link": "https://medium.com/@Makotoma/kontemplasi-soal-diri-dan-pribadi-58c6b8af26ee?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T11:18:37.000Z",
          "wordCount": 1982,
          "title": "Kontemplasi Soal Diri dan Pribadi",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*dXzNf8e-JR2k9-ViHZY_OA.jpeg"
        },
        {
          "id": "https://medium.com/p/e5792a6355d7",
          "author": "Ashish Malhotra",
          "description": "Word embedding is a technique used in natural language processing (NLP) to represent words as dense vectors of real numbers in a…\nContinue reading on Medium »",
          "link": "https://medium.com/@ypredofficial/word-embedding-and-why-its-used-over-tfidf-and-bow-e5792a6355d7?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T11:03:11.000Z",
          "wordCount": 2171,
          "title": "Word Embedding and why it’s used over TFIDF and BoW",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/2cc7ad3aa0eb",
          "author": "Amritesh Kumar",
          "description": "Continue reading on Medium »",
          "link": "https://medium.com/@kelixirr/a-solid-test-of-consciousness-for-ai-2cc7ad3aa0eb?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T09:17:34.000Z",
          "wordCount": 748,
          "title": "A Solid Test Of Consciousness For AI",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*iNRZh6MsiZEZAh__JbSF-A.jpeg"
        },
        {
          "id": "https://medium.com/p/a268cba1dd3b",
          "author": "Deep Dwivedi",
          "description": "This paper was accepted in AIED conference under short papers track , this paper summarises some of my work done at Extramarks Education…\nContinue reading on Medium »",
          "link": "https://medium.com/@deepdwivedi/k-12bert-bert-for-k-12-education-a268cba1dd3b?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T08:52:24.000Z",
          "wordCount": null,
          "title": "K-12BERT: BERT for K-12 education",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/104cada453de",
          "author": "Mirko Peters",
          "description": "PyTorch is a popular and PyTorch 2 are popular libraries for deep learning. efficient framework for deep learning implementations. The…\nContinue reading on Mirko Peters — Data & Analytics Blog »",
          "link": "https://blog.mirkopeters.com/introduction-to-pytorch-a-powerful-framework-for-deep-learning-104cada453de?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T08:38:18.000Z",
          "wordCount": null,
          "title": "Introduction to PyTorch: A Powerful Framework for Deep Learning",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/436dd6770904",
          "author": "Kindlykumar",
          "description": "Continue reading on Medium »",
          "link": "https://medium.com/@kindlykumar07/money-fine-loan-customer-care-helpline-number-%E2%9D%BE%E2%9D%BD%E2%9D%BD%E2%9D%B8%E2%9D%B9%E2%9D%B6%E2%9D%BD%E2%9D%B9%E2%9D%BC%E2%9D%BD-436dd6770904?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T08:23:58.000Z",
          "wordCount": null,
          "title": "Money Fine loan CUSTOMER care helpline Number ❾❽❽❸❹❶❽❹❼❽ !!✅",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/e6c7e16457a4",
          "author": "Nina Chan",
          "description": "Your cheat sheet on the main types of AI that are relevant and why they are important. A guide for everyone in tech or otherwise.\nContinue reading on Medium »",
          "link": "https://medium.com/@ninaandhercomputer/these-are-the-types-of-ai-you-need-to-know-in-2024-e6c7e16457a4?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T06:54:28.000Z",
          "wordCount": null,
          "title": "These are the types of AI you need to know in 2024",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/36994d7370a3",
          "author": "Simon Hugo",
          "description": "Image from MIT Technology Review \nContinue reading on Medium »",
          "link": "https://medium.com/@simon.hugo59/improve-your-brute-force-attacks-with-chatgpt-36994d7370a3?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T06:44:35.000Z",
          "wordCount": null,
          "title": "Improve your brute force attacks with ChatGPT",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/ae8053c428b4",
          "author": "Ayşe Kübra Kuyucu",
          "description": "Learn how to use transfer learning for adapting large language models to new tasks or domains.\nContinue reading on Artificial Intelligence in Plain English »",
          "link": "https://ai.plainenglish.io/llm-tutorial-16-transfer-learning-in-large-language-models-ae8053c428b4?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T06:16:35.000Z",
          "wordCount": null,
          "title": "LLM Tutorial 16 — Transfer Learning in Large Language Models",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/3c5375b879cf",
          "author": "Aqsazafar",
          "description": "If you’re interested in how computers can make art, music, and stories, you’re in for a treat. In 2024, computers are getting really good…\nContinue reading on Medium »",
          "link": "https://aqsazafar81.medium.com/7-best-generative-ai-projects-of-2024-3c5375b879cf?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T06:14:16.000Z",
          "wordCount": 2304,
          "title": "7 Best Generative AI Projects of 2024",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*31FTVrNoFc758OvKa5jttQ.jpeg"
        },
        {
          "id": "https://medium.com/p/9f1f40d8958a",
          "author": "Albertomoccardi",
          "description": "A case study on N-CMAPSS Datasets\nContinue reading on Medium »",
          "link": "https://medium.com/@albertomoccardi/deep-learning-strategies-for-predictive-maintenance-9f1f40d8958a?source=rss------deep_learning-5",
          "publishedOn": "2024-02-14T05:29:00.000Z",
          "wordCount": null,
          "title": "Deep Learning Strategies for Predictive Maintenance",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/2f2a0c44dd46",
          "author": "Mfavianardrarazan",
          "description": "Definisi Machine Learning\nContinue reading on Medium »",
          "link": "https://medium.com/@mfavianardrarazan/penerapan-algoritma-machine-learning-pada-visualisasi-data-2f2a0c44dd46?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T23:46:19.000Z",
          "wordCount": 4495,
          "title": "Penerapan Algoritma Machine Learning pada Visualisasi Data",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*LVsUo2GlC76S3JC7iKNSbg.jpeg"
        },
        {
          "id": "https://medium.com/p/e15237239f47",
          "author": "Everton Gomede, PhD",
          "description": "Introduction\nContinue reading on  . »",
          "link": "https://medium.com/aimonks/shufflenet-revolutionizing-mobile-deep-learning-e15237239f47?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T22:18:18.000Z",
          "wordCount": 3898,
          "title": "ShuffleNet: Revolutionizing Mobile Deep Learning",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/0*iWjcxBENO7Ids6WA.jpg"
        },
        {
          "id": "https://medium.com/p/185ae0d6fc7b",
          "author": "Najma David Tryggvason",
          "description": "Loving through healing  \nContinue reading on Medium »",
          "link": "https://medium.com/@tryggvason.david.najma/healing-through-love-%EF%B8%8F-185ae0d6fc7b?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T22:16:01.000Z",
          "wordCount": 2070,
          "title": "Healing through love ❤️",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1103/1*Hi7B1m97HjxjveLGjFiRwg@2x.jpeg"
        },
        {
          "id": "https://medium.com/p/9978f54bf441",
          "author": "Harsh Vardhan",
          "description": "Introduction\nContinue reading on Medium »",
          "link": "https://medium.com/@harsh.vardhan7695/machine-learning-and-ai-for-healthcare-9978f54bf441?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T20:57:44.000Z",
          "wordCount": 5603,
          "title": "Machine learning and AI for healthcare",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/0*B4rRakUotxB7z2Le.jpg"
        },
        {
          "id": "https://medium.com/p/72174b78e560",
          "author": "Jhoan Sebastián Fuentes Hernández",
          "description": "1- Deep Learning for protein design\nContinue reading on Medium »",
          "link": "https://medium.com/@jhoansfuentes1999/seven-technologies-to-watch-in-2024-according-to-nature-72174b78e560?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T20:51:33.000Z",
          "wordCount": 1641,
          "title": "Seven Technologies to Watch in 2024 According to Nature",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:619/1*x98ebeVCaSpCQ7JdUgbj9Q.png"
        },
        {
          "id": "https://medium.com/p/5272c8c53d0d",
          "author": "Debaprasann Bhoi",
          "description": "A turning point for AI?\nContinue reading on GoPenAI »",
          "link": "https://blog.gopenai.com/chatgpt-vs-gemini-5272c8c53d0d?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T20:35:00.000Z",
          "wordCount": 2142,
          "title": "ChatGPT Vs Gemini",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*ivto5WmOjrodIzth"
        },
        {
          "id": "https://medium.com/p/8968f77bcd15",
          "author": "Gobi Shangar",
          "description": "Large Language Models (LLMs):\nContinue reading on Medium »",
          "link": "https://medium.com/@gobishangar11/llama-2-a-detailed-guide-to-fine-tuning-the-large-language-model-8968f77bcd15?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T20:14:05.000Z",
          "wordCount": null,
          "title": "LLaMA 2: A Detailed Guide to Fine-Tuning the Large Language Model",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/ba2cf24c95d0",
          "author": "Zayiszzz",
          "description": "♡♡♡\nContinue reading on Medium »",
          "link": "https://medium.com/@zayisreading/40-rules-of-love-ba2cf24c95d0?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T19:55:58.000Z",
          "wordCount": null,
          "title": "40 Rules of love ♡",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/e951b5b8c806",
          "author": "Tepo Wendy Makgatlhe",
          "description": "Continue reading on Medium »",
          "link": "https://medium.com/@tepowendymakgatlhe/whats-the-best-way-to-manipulate-a-human-e951b5b8c806?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T19:38:45.000Z",
          "wordCount": null,
          "title": "What’s the best way to manipulate a human?",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/dd1beac356d5",
          "author": "ENFINITE BALANCE",
          "description": "Good morning, good afternoon, and good evening I’ve been getting a lot of feedback on my different type a Star Seeds post that I thought I…\nContinue reading on New Earth Consciousness »",
          "link": "https://medium.com/new-earth-consciousness/pleiadian-starseeds-cosmic-kinfolk-dd1beac356d5?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T19:35:53.000Z",
          "wordCount": null,
          "title": "“Pleiadian Starseeds: Cosmic Kinfolk”",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/ae2d216d7f17",
          "author": "John Clayton Blanc",
          "description": "How do Transformers work?\nContinue reading on AI Mind »",
          "link": "https://pub.aimind.so/attention-is-all-you-need-ae2d216d7f17?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T18:42:51.000Z",
          "wordCount": null,
          "title": "Attention Is All You Need",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/5714f21efab4",
          "author": "Sam Salmasi",
          "description": "In the intricate dance of the natural world, animals often resort to fascinating survival strategies, including self-medication through…\nContinue reading on Medium »",
          "link": "https://medium.com/@mr.bioprospecting/visiting-natures-pharmacy-using-computer-vision-to-explore-wild-animals-self-medication-patterns-5714f21efab4?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T18:41:13.000Z",
          "wordCount": null,
          "title": "Visiting Nature’s Pharmacy: Using Computer Vision to Explore Wild Animals’ Self-Medication Patterns",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/1fec01432d21",
          "author": "Eugene Shevchenko",
          "description": "The vanishing gradient problem is a significant challenge in training deep neural networks, particularly Recurrent Neural Networks (RNNs)…\nContinue reading on Medium »",
          "link": "https://medium.com/@eugenesh4work/overcoming-rnn-limitations-dot-product-attention-for-mitigating-vanishing-gradients-1fec01432d21?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T17:51:31.000Z",
          "wordCount": null,
          "title": "Overcoming RNN Limitations: Dot Product Attention for Mitigating Vanishing Gradients",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/3d0e3bc39e78",
          "author": "Atharv Pal",
          "description": "Ever embarked on a project with the enthusiasm of a toddler diving into a ball pit, only to realize it’s more like navigating a labyrinth…\nContinue reading on Medium »",
          "link": "https://medium.com/@atharvpal17/deep-learning-for-beginners-the-concept-you-know-the-struggles-you-dont-3d0e3bc39e78?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T17:46:07.000Z",
          "wordCount": null,
          "title": "Deep Learning for Beginners; the concept you know, the struggles you don’t.",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/012eff24ce1b",
          "author": "Joe Saviano",
          "description": "Healing With Self Awareness\nContinue reading on ILLUMINATION »",
          "link": "https://medium.com/illumination/understanding-your-toxic-relationships-012eff24ce1b?source=rss------deep_learning-5",
          "publishedOn": "2024-02-13T17:07:16.000Z",
          "wordCount": null,
          "title": "Understanding Your Toxic Relationships",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "PyTorch - Medium",
      "feedUrl": "https://medium.com/feed/pytorch",
      "siteUrl": "https://medium.com/pytorch?source=rss----512b8efdf2e7---4",
      "articles": [
        {
          "id": "https://medium.com/p/26d2e4b9fd92",
          "author": "Yang You",
          "description": "The most prominent distinction between LLaMA-1 and LLaMA-2 lies in the incorporation of higher-quality corpora, a pivotal factor…",
          "link": "https://medium.com/pytorch/colossal-llama-2-low-cost-and-high-quality-domain-specific-llm-solution-using-llama-and-26d2e4b9fd92?source=rss----512b8efdf2e7---4",
          "publishedOn": "2024-01-29T16:40:21.000Z",
          "wordCount": 5204,
          "title": "Colossal-LLaMA-2: Low Cost and High-quality Domain-specific LLM Solution Using LLaMA and…",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*3QwwCCRuFuox72b5"
        },
        {
          "id": "https://medium.com/p/356a495a20c4",
          "author": "Romain Brégier",
          "description": "Struggling with quaternions, rotation vectors, right-hand rules and all these stuffs? Try RoMa: an easy-to-to-use, stable and efficient…",
          "link": "https://medium.com/pytorch/3d-rotations-and-spatial-transformations-made-easy-with-roma-356a495a20c4?source=rss----512b8efdf2e7---4",
          "publishedOn": "2024-01-25T00:02:54.000Z",
          "wordCount": 1868,
          "title": "3D rotations and spatial transformations made easy with RoMa",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*5wOBI5bLik76q_kucGza9g.png"
        }
      ]
    },
    {
      "title": "NYU Center for Data Science",
      "feedUrl": "https://cds.nyu.edu/feed/",
      "siteUrl": "https://cds.nyu.edu/",
      "articles": []
    },
    {
      "title": "PyImageSearch",
      "feedUrl": "https://www.pyimagesearch.com/feed/",
      "siteUrl": "https://pyimagesearch.com/",
      "articles": [
        {
          "id": "https://pyimagesearch.com/?p=43125",
          "author": "Aditya Sharma",
          "description": "Table of Contents Image Processing with Gemini Pro Getting Started with Gemini Pro: An Overview Gemini Pro Setup Integrating Google AI Python SDK with Gemini Pro Image Processing with Gemini Pro: Python Code Generation Comprehensive List of GenAI Models Compatible…\nThe post Image Processing with Gemini Pro appeared first on PyImageSearch.",
          "link": "https://pyimagesearch.com/2024/02/12/image-processing-with-gemini-pro/",
          "publishedOn": "2024-02-12T14:00:00.000Z",
          "wordCount": 8377,
          "title": "Image Processing with Gemini Pro",
          "imageUrl": "https://pyimagesearch.com/wp-content/uploads/2024/02/image-processing-with-gemini-pro-featured.png"
        },
        {
          "id": "https://pyimagesearch.com/?p=37872",
          "author": "Shivam Chandhok",
          "description": "Table of Contents Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow Building the Face Recognition Application with Siamese Networks Introduction to Model Evaluation in Face Recognition Introduction to Siamese Networks in Facial Recognition Systems Utilizing Siamese…\nThe post Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow appeared first on PyImageSearch.",
          "link": "https://pyimagesearch.com/2024/02/05/evaluating-siamese-network-accuracy-f1-score-precision-and-recall-with-keras-and-tensorflow/",
          "publishedOn": "2024-02-05T14:00:00.000Z",
          "wordCount": 8769,
          "title": "Evaluating Siamese Network Accuracy (F1-Score, Precision, and Recall) with Keras and TensorFlow",
          "imageUrl": "https://pyimagesearch.com/wp-content/uploads/2023/07/sn-201-5-featured-revised.png"
        },
        {
          "id": "https://pyimagesearch.com/?p=40831",
          "author": "Shivam Chandhok",
          "description": "Table of Contents Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL) Introduction to Advanced Adversarial Techniques in Machine Learning Harnessing NSL for Robust Model Training: Insights from Part 2 Deep Dive into…\nThe post Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL) appeared first on PyImageSearch.",
          "link": "https://pyimagesearch.com/2024/01/29/adversarial-learning-with-keras-and-tensorflow-part-3-exploring-adversarial-attacks-using-neural-structured-learning-nsl/",
          "publishedOn": "2024-01-29T14:00:00.000Z",
          "wordCount": 8567,
          "title": "Adversarial Learning with Keras and TensorFlow (Part 3): Exploring Adversarial Attacks Using Neural Structured Learning (NSL)",
          "imageUrl": "https://pyimagesearch.com/wp-content/uploads/2024/01/adversarial-learning-with-keras-and-tensorflow-part-3-featured.png.png"
        },
        {
          "id": "https://pyimagesearch.com/?p=42750",
          "author": "Aritra Roy Gosthipaty and Ritwik Raha",
          "description": "Table of Contents Getting Started with Diffusers for Text-to-Image Introduction A Brief Primer on Diffusion Configuring Your Development Environment Need Help Configuring Your Development Environment? Setup and Imports Diffusers But What Is AutoPipeline? What Are Some Other Pipelines and Models?…\nThe post Getting Started with Diffusers for Text-to-Image appeared first on PyImageSearch.",
          "link": "https://pyimagesearch.com/2024/01/22/getting-started-with-diffusers-for-text-to-image/",
          "publishedOn": "2024-01-22T14:00:00.000Z",
          "wordCount": 7300,
          "title": "Getting Started with Diffusers for Text-to-Image",
          "imageUrl": "https://pyimagesearch.com/wp-content/uploads/2024/01/getting-started-with-diffusers-for-text-to-image-featured.png"
        }
      ]
    },
    {
      "title": "Another Datum",
      "feedUrl": "https://anotherdatum.com/feeds/all.atom.xml",
      "siteUrl": "https://anotherdatum.com/",
      "articles": []
    },
    {
      "title": "Rob’s Homepage",
      "feedUrl": "https://roberttlange.github.io/feed.xml",
      "siteUrl": "http://localhost:4000/",
      "articles": []
    },
    {
      "title": "betanalpha.github.io",
      "feedUrl": "https://betanalpha.github.io/feed.xml",
      "siteUrl": "https://betanalpha.github.io/",
      "articles": []
    },
    {
      "title": "The Roycoding Blog",
      "feedUrl": "https://roycoding.com/blog/rss.xml",
      "siteUrl": "https://roycoding.com/blog",
      "articles": []
    },
    {
      "title": "Gallamine's Scientific Computing Blog",
      "feedUrl": "http://www.gallamine.com/feeds/posts/default",
      "siteUrl": "http://www.gallamine.com/",
      "articles": []
    },
    {
      "title": "tmarthal.com",
      "feedUrl": "https://www.tmarthal.com/feeds/posts/default",
      "siteUrl": "https://www.tmarthal.com/",
      "articles": []
    },
    {
      "title": "randyzwitch - Articles",
      "feedUrl": "https://randyzwitch.com/feed.xml",
      "siteUrl": "http://randyzwitch.com",
      "articles": []
    },
    {
      "title": "J Ben Deaton",
      "feedUrl": "https://jbendeaton.com/feed.xml",
      "siteUrl": "https://jbendeaton.com/",
      "articles": []
    },
    {
      "title": "the ml engineer",
      "feedUrl": "https://www.kill-the-newsletter.com/feeds/clgdkag7vq1e4pl4shei.xml",
      "siteUrl": "https://kill-the-newsletter.com/",
      "articles": [
        {
          "id": "https://kill-the-newsletter.com/alternates/e5alayne5z5znfhu.html",
          "author": null,
          "description": "The Machine Learning Engineer 🤖 #269#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…",
          "link": "https://kill-the-newsletter.com/alternates/e5alayne5z5znfhu.html",
          "publishedOn": "2024-02-11T14:48:40.000Z",
          "wordCount": 1863,
          "title": "The Machine Learning Engineer 🤖 #269 -     Writing a Search Engine in 80 Lines of Python,\n    Trillion Row Challenge,\n    Comparing LLMs to Lawyers,\n    AI Calls Now Illegal in US\n    + more 🚀",
          "imageUrl": null
        },
        {
          "id": "https://kill-the-newsletter.com/alternates/9qh1v0h0qrzxv4w8.html",
          "author": null,
          "description": "The Machine Learning Engineer 🤖 #268#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…",
          "link": "https://kill-the-newsletter.com/alternates/9qh1v0h0qrzxv4w8.html",
          "publishedOn": "2024-02-04T15:08:56.000Z",
          "wordCount": 1823,
          "title": "The Machine Learning Engineer 🤖 #268 -     Democratising LLM Inference, Google Forecast Foundation Model,\n    Cyber-resilience Act, Stanford's Algorithms,\n    LLaVA 1.16, ML Frameworks\n    + more 🚀",
          "imageUrl": null
        },
        {
          "id": "https://kill-the-newsletter.com/alternates/1krzbyojxlwbn7bo.html",
          "author": null,
          "description": "The Machine Learning Engineer 🤖 #267#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…",
          "link": "https://kill-the-newsletter.com/alternates/1krzbyojxlwbn7bo.html",
          "publishedOn": "2024-01-28T15:39:34.000Z",
          "wordCount": 1677,
          "title": "The Machine Learning Engineer 🤖 #267 -     Sampling in Large Language Models\n    UK Govt's GenAI Framework,    Why ML is Hard, AI Fundamentals, MLOps Events, ML Frameworks\n    + more 🚀",
          "imageUrl": null
        },
        {
          "id": "https://kill-the-newsletter.com/alternates/t22ditc6m1ksloqq.html",
          "author": null,
          "description": "The Machine Learning Engineer 🤖 #266#outlook a{padding:0;}.ExternalClass{width:100%;}.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div{line-height:100%;}table td{border-collapse:collapse;mso-line-height-rule:exactly;}.editable.image{font-size:0 !important;line-height:0 !important;}.nl2go_preheader{display:none !important;mso-hide:all !important;mso-line-height-rule:exactly;visibility:hidden !important;line-height:0px !important;font-size:0px !important;}body{width:100% !important;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%;margin:0;padding:0;}img{outline:none;text-decoration:none;-ms-interpolation-mode:bicubic;}a img{border:none;}table{border-collapse:collapse;mso-table-lspace:0pt;mso-table-rspace:0pt;}th{font-wei…",
          "link": "https://kill-the-newsletter.com/alternates/t22ditc6m1ksloqq.html",
          "publishedOn": "2024-01-21T17:14:20.000Z",
          "wordCount": 1601,
          "title": "The Machine Learning Engineer 🤖 #266 -     Forecasting & Causal Inference, OSS Value,\n    TextToSpeech Whisper, ISO Global Standards. Meta Infra, MLOps Events, ML Frameworks + more 🚀",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "projects to know",
      "feedUrl": "https://www.kill-the-newsletter.com/feeds/nr3re6vid9thykg5f0z7.xml",
      "siteUrl": "https://kill-the-newsletter.com/",
      "articles": []
    },
    {
      "title": "Data Science Community newsletter sign-up",
      "feedUrl": "https://www.kill-the-newsletter.com/feeds/qb47kyhrjqkkbll8gbiq.xml",
      "siteUrl": "https://kill-the-newsletter.com/",
      "articles": [
        {
          "id": "https://kill-the-newsletter.com/alternates/f0uuxsjowsfqzzwx.html",
          "author": null,
          "description": "96\n            \n        \n        \n        \n        \n        \n        DSCN #287: AI Act vs US Fair Use Trial - Which matters more now?\n\n    \n\t\tp{\n\t\t\tmargin:10px 0;\n\t\t\tpadding:0;\n\t\t}\n\t\ttable{\n\t\t\tborder-collapse:collapse;\n\t\t}\n\t\th1,h2,h3,h4,h5,h6{\n\t\t\tdisplay:block;\n\t\t\tmargin:0;\n\t\t\tpadding:0;\n\t\t}\n\t\timg,a img{\n\t\t\tborder:0;\n\t\t\theight:auto;\n\t\t\toutline:none;\n\t\t\ttext-decoration:none;\n\t\t}\n\t\tbody,#bodyTable,#bodyCell{\n\t\t\theight:100%;\n\t\t\tmargin:0;\n\t\t\tpadding:0;\n\t\t\twidth:100%;\n\t\t}\n\t\t.mcnPreviewText{\n\t\t\tdisplay:none !important;\n\t\t}\n\t\t#outlook a{\n\t\t\tpadding:0;\n\t\t}\n\t\timg{\n\t\t\t-ms-interpolation-mode:bicubic;\n\t\t}\n\t\ttable{\n\t\t\tmso-table-lspace:0pt;\n\t\t\tmso-table-rspace:0pt;\n\t\t}\n\t\t.ReadMsgBody{\n\t\t\twidth:100%;\n\t\t}\n\t\t.ExternalClass{\n\t\t\twidth:100%;\n\t\t}\n\t\tp,a,li,td,blockquote{\n\t\t\tmso-line-height-rule:exactly;\n\t\t}\n\t\ta…",
          "link": "https://kill-the-newsletter.com/alternates/f0uuxsjowsfqzzwx.html",
          "publishedOn": "2024-02-09T14:21:07.000Z",
          "wordCount": 3244,
          "title": "DSCN #287: AI Act vs US Fair Use Trial - Which matters more now?",
          "imageUrl": null
        },
        {
          "id": "https://kill-the-newsletter.com/alternates/7e3m2axeo0jykbov.html",
          "author": null,
          "description": "96\n            \n        \n        \n        \n        \n        \n        DSCN #286: Will AI Match Human Performance In 1-5 Years? What are the risks?\n\n    \n\t\tp{\n\t\t\tmargin:10px 0;\n\t\t\tpadding:0;\n\t\t}\n\t\ttable{\n\t\t\tborder-collapse:collapse;\n\t\t}\n\t\th1,h2,h3,h4,h5,h6{\n\t\t\tdisplay:block;\n\t\t\tmargin:0;\n\t\t\tpadding:0;\n\t\t}\n\t\timg,a img{\n\t\t\tborder:0;\n\t\t\theight:auto;\n\t\t\toutline:none;\n\t\t\ttext-decoration:none;\n\t\t}\n\t\tbody,#bodyTable,#bodyCell{\n\t\t\theight:100%;\n\t\t\tmargin:0;\n\t\t\tpadding:0;\n\t\t\twidth:100%;\n\t\t}\n\t\t.mcnPreviewText{\n\t\t\tdisplay:none !important;\n\t\t}\n\t\t#outlook a{\n\t\t\tpadding:0;\n\t\t}\n\t\timg{\n\t\t\t-ms-interpolation-mode:bicubic;\n\t\t}\n\t\ttable{\n\t\t\tmso-table-lspace:0pt;\n\t\t\tmso-table-rspace:0pt;\n\t\t}\n\t\t.ReadMsgBody{\n\t\t\twidth:100%;\n\t\t}\n\t\t.ExternalClass{\n\t\t\twidth:100%;\n\t\t}\n\t\tp,a,li,td,blockquote{\n\t\t\tmso-line-height-rule:exac…",
          "link": "https://kill-the-newsletter.com/alternates/7e3m2axeo0jykbov.html",
          "publishedOn": "2024-01-25T16:41:58.000Z",
          "wordCount": 3601,
          "title": "DSCN #286: Will AI Match Human Performance In 1-5 Years? What are the risks?",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Challenge in PursuitOfData on Medium",
      "feedUrl": "https://medium.com/feed/pursuitnotes/tagged/challenge",
      "siteUrl": "https://medium.com/pursuitnotes/tagged/challenge?source=rss----f127ff4e076c--challenge",
      "articles": []
    },
    {
      "title": "gradient science",
      "feedUrl": "https://gradientscience.org/feed.xml",
      "siteUrl": "https://gradientscience.org/",
      "articles": [
        {
          "id": "https://gradientscience.org/dsdm/",
          "author": null,
          "description": "Code\n\n\n\n   Paper\n\n\ntl;dr: When training large-scale models, standard practice is to select training data that is intuitively useful. However, it turns out that such data can actually hurt model performance. We instead design a framework that selects by modeling how models learn from data—and thereby greatly improve performance.\nSuppose we want to train a large-scale ML model, like a language model or a diffusion model. How do we choose which data to train on? Standard methods tend to select data using human notions of data quality. For example, the GPT-3 training procedure selects training data that matches intuitively “high quality” data sources like Wikipedia. Filtering like this yields (qualitatively) clean data that feels like it should improve model performance. But does it actually i…",
          "link": "https://gradientscience.org/dsdm/",
          "publishedOn": "2024-01-24T00:00:00.000Z",
          "wordCount": 1613,
          "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
          "imageUrl": null
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}