{
  "sources": [
    {
      "title": "Release notes from osmosfeed",
      "feedUrl": "https://github.com/osmoscraft/osmosfeed/releases.atom",
      "siteUrl": "https://github.com/osmoscraft/osmosfeed/releases",
      "articles": []
    },
    {
      "title": "Towards Data Science - Medium",
      "feedUrl": "https://towardsdatascience.com/feed",
      "siteUrl": "https://towardsdatascience.com?source=rss----7f60cf5620c9---4",
      "articles": [
        {
          "id": "https://medium.com/p/3001f54c48e9",
          "author": "Yoann Mocquin",
          "description": "Always keep a dummy by your side.\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/the-dummy-models-of-scikit-learn-3001f54c48e9?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T09:06:17.000Z",
          "wordCount": null,
          "title": "The Dummy models of Scikit-learn",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/a54701d1b621",
          "author": "Andrew Skabar, PhD",
          "description": "",
          "link": "https://towardsdatascience.com/evaluating-synthetic-data-the-million-dollar-question-a54701d1b621?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T06:12:20.000Z",
          "wordCount": null,
          "title": "Evaluating Synthetic Data‚Ää‚Äî‚ÄäThe Million Dollar Question",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/360dce356df5",
          "author": "Mikhail Sarafanov",
          "description": "",
          "link": "https://towardsdatascience.com/stream-ordering-how-and-why-a-geo-scientist-sometimes-needed-to-rank-rivers-on-a-map-360dce356df5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-14T06:09:20.000Z",
          "wordCount": null,
          "title": "Stream Ordering: How And Why a Geo-scientist Sometimes Needed to Rank Rivers on a Map",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/41425a40d7d0",
          "author": "Vincent Koc",
          "description": "Architecture patterns and mental models for working with Large Language Models",
          "link": "https://towardsdatascience.com/generative-ai-design-patterns-a-comprehensive-guide-41425a40d7d0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T23:19:59.000Z",
          "wordCount": 3759,
          "title": "Generative AI Design Patterns: A Comprehensive Guide",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*2tBQkVvC-_sHACgSs0ouug.png"
        },
        {
          "id": "https://medium.com/p/ee11ab5f9f20",
          "author": "Sachin Date",
          "description": "The life and times of Abraham De Moivre, his famous theorem, and how it set the stage for the discovery of the Central Limit Theorem",
          "link": "https://towardsdatascience.com/abraham-de-moivre-his-famous-theorem-and-the-birth-of-the-normal-curve-ee11ab5f9f20?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T21:37:50.000Z",
          "wordCount": 8015,
          "title": "Abraham De Moivre, His Famous Theorem, and the Birth of the Normal Curve",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*SoJHemrSFz0wWPxZl3q4wg.jpeg"
        },
        {
          "id": "https://medium.com/p/cf3682f8563b",
          "author": "Robert A. Gonsalves",
          "description": "Transforming your ideas into tangible artifacts using Midjourney and open-source projects: Shap-E, MVDream, and threestudio\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/molding-the-imagination-using-ai-to-create-new-3d-printable-objects-cf3682f8563b?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T21:31:18.000Z",
          "wordCount": 1497,
          "title": "Molding the Imagination: Using AI to Create New 3D-Printable Objects",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*sao9nlpIadkA3dzQig2qHQ.jpeg"
        },
        {
          "id": "https://medium.com/p/e823535e0eb1",
          "author": "LucianoSphere (Luciano Abriata, PhD)",
          "description": "A careful selection looking for performance, flexibility, and richness of features.\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/the-most-advanced-libraries-for-data-visualization-and-analysis-on-the-web-e823535e0eb1?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T20:39:41.000Z",
          "wordCount": 1520,
          "title": "The Most Advanced Libraries for Data Visualization and Analysis on the Web",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*Cpgpk7Sl0hGofvcE7OEWMA.png"
        },
        {
          "id": "https://medium.com/p/cb1bd2f3f4bb",
          "author": "Yennie Jun",
          "description": "How well can AI models solve (and create) rebus puzzles?\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/measuring-ais-creativity-with-visual-word-puzzles-cb1bd2f3f4bb?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T20:10:28.000Z",
          "wordCount": null,
          "title": "Measuring AI‚Äôs Creativity with Visual Word Puzzles",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/0af4ea38f7b5",
          "author": "John Leung",
          "description": "Explore the potentials and constraints of LangChain for customer analytics, accompanied by practical implementation codes\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T19:34:16.000Z",
          "wordCount": null,
          "title": "Performing Customer Analytics with LangChain and LLMs",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/5c1db85984a1",
          "author": "Okan Bulut",
          "description": "",
          "link": "https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T17:50:27.000Z",
          "wordCount": null,
          "title": "Lexicon-Based Sentiment Analysis Using R",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/795582423666",
          "author": "Cristian Leo",
          "description": "",
          "link": "https://towardsdatascience.com/the-math-and-code-behind-k-means-clustering-795582423666?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T15:21:24.000Z",
          "wordCount": null,
          "title": "The Math and Code Behind K-Means Clustering",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/9d50b36bcbd3",
          "author": "Sofya Lipnitskaya",
          "description": "",
          "link": "https://towardsdatascience.com/bird-by-bird-using-finite-automata-9d50b36bcbd3?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T15:15:51.000Z",
          "wordCount": null,
          "title": "Finite Automata Simulation for Leveraging AI-Assisted Systems",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/9afdfaf2bd7c",
          "author": "Marco Peixeiro",
          "description": "Explore the architecture of Lag-Llama and learn to apply it in a forecasting project using Python\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/lag-llama-open-source-foundation-model-for-time-series-forecasting-9afdfaf2bd7c?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T14:52:49.000Z",
          "wordCount": null,
          "title": "Lag-Llama: Open-Source Foundation Model for Time Series Forecasting",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/e62531bca7a0",
          "author": "Rami Krispin",
          "description": "",
          "link": "https://towardsdatascience.com/setting-a-dockerized-python-environment-the-hard-way-e62531bca7a0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T14:45:42.000Z",
          "wordCount": null,
          "title": "Setting A Dockerized Python Environment‚Ää‚Äî‚ÄäThe Hard Way",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/afd97fce8fb5",
          "author": "Mariya Mansurova",
          "description": "",
          "link": "https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T08:03:13.000Z",
          "wordCount": null,
          "title": "Text Embeddings: Comprehensive Guide",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/71f714440923",
          "author": "Jarom Hulet",
          "description": "Understanding EMD through theory and from-scratch calculation\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/comparison-of-distributions-with-earth-movers-distance-71f714440923?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T07:55:44.000Z",
          "wordCount": null,
          "title": "Comparison of Distributions with Earth Mover‚Äôs Distance",
          "imageUrl": null
        },
        {
          "id": "https://medium.com/p/465970a969e0",
          "author": "Ugur Yildirim",
          "description": "How to know the unknowable in observational studies",
          "link": "https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-13T00:27:50.000Z",
          "wordCount": 5372,
          "title": "Sensitivity Analysis for Unobserved Confounding",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*pw5JJ7_hGcYcTWNoyZsfzQ.png"
        },
        {
          "id": "https://medium.com/p/2902224aabf3",
          "author": "Stefano Bosisio",
          "description": "In this first article, we‚Äôre exploring Apache Beam, from a simple pipeline to a more complicated one, using GCP Dataflow. Let‚Äôs learn what‚Ä¶\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/apache-beam-data-processing-data-pipelines-dataflow-and-flex-templates-2902224aabf3?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:40:21.000Z",
          "wordCount": 1464,
          "title": "Apache Beam: Data Processing, Data Pipelines, Dataflow and Flex Templates",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*qDe_sNC7PPh3fU0KWs04Ug.jpeg"
        },
        {
          "id": "https://medium.com/p/a7dc47723788",
          "author": "Claudia Ng",
          "description": "Learn to build a Graph Convolutional Network that can handle heterogeneous graph data for link prediction\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/how-to-build-a-graph-based-neural-network-for-anomaly-detection-in-6-steps-a7dc47723788?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:22:38.000Z",
          "wordCount": 1414,
          "title": "How to Build a Graph-based Neural Network for Anomaly Detection in 6 Steps",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:640/1*Lb_4cCQckr6i7aDoJ2SN5w.png"
        },
        {
          "id": "https://medium.com/p/17f84378430b",
          "author": "Toon Beerten",
          "description": "A hands-on marketing use case\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/powerful-collaboration-of-ai-agents-with-crewai-17f84378430b?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:12:44.000Z",
          "wordCount": 1355,
          "title": "Powerful Collaboration of AI Agents with CrewAI",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*40kwxJ7oZ1kTAJsRt_2bow.png"
        },
        {
          "id": "https://medium.com/p/a4e876f9a532",
          "author": "Mohammed Mohammed",
          "description": "A constructive approach to measuring distribution differences.",
          "link": "https://towardsdatascience.com/understanding-kl-divergence-intuitively-a4e876f9a532?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T19:05:57.000Z",
          "wordCount": 3032,
          "title": "Understanding KL Divergence Intuitively",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*RlzqwDFtqqNYQjD2"
        },
        {
          "id": "https://medium.com/p/a57512cabd69",
          "author": "Mahyar Aboutalebi, Ph.D.",
          "description": "Learn how to work with Lexcube, a Python package for data visualization in the space-time domain!\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/3d-visualization-of-geospatial-big-data-by-lexcube-python-a57512cabd69?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T16:09:16.000Z",
          "wordCount": 1471,
          "title": "3D Visualization of Geospatial Big Data by Lexcube! (Python)",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*ThD3jbF6-ifOSyZVtzhdXg.png"
        },
        {
          "id": "https://medium.com/p/a5d7b15f3d1d",
          "author": "Jacky Kaub",
          "description": "Why distribution drifts can really hurt your models\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/the-biggest-weakness-of-boosting-trees-a5d7b15f3d1d?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T15:24:32.000Z",
          "wordCount": 1389,
          "title": "The Biggest Weakness Of Boosting Trees",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*bcxyWsq_OvXuZZkT"
        },
        {
          "id": "https://medium.com/p/ccb3a116f6eb",
          "author": "Conor O'Sullivan",
          "description": "Automation, machine learning and LLMs in the chip industry\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/7-lessons-from-an-ml-internship-at-intel-ccb3a116f6eb?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T14:37:22.000Z",
          "wordCount": 1403,
          "title": "7 Lessons from an ML Internship at Intel",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*D7KYKmCGk2NkPMdb6PFw0Q.png"
        },
        {
          "id": "https://medium.com/p/ea0f9ce61719",
          "author": "Christopher Tao",
          "description": "Get Holidays from Any Country, Any Year, Any date\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/python-could-know-your-holidays-no-matter-which-country-you-live-ea0f9ce61719?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T14:37:12.000Z",
          "wordCount": 1388,
          "title": "Python Could Know Your Holidays No Matter Which Country You Live",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*pk-kVwA2UxPMtRC_k1ED1Q.jpeg"
        },
        {
          "id": "https://medium.com/p/d3b3b28ba121",
          "author": "Marcin Kozak",
          "description": "The article shows simple examples of flat and nested recursion patterns in Python.\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/recursion-in-python-demystified-d3b3b28ba121?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-12T13:06:35.000Z",
          "wordCount": 1447,
          "title": "Recursion in Python Demystified",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*8EKWgHhKoScGLrZW"
        },
        {
          "id": "https://medium.com/p/769f43b46779",
          "author": "M√°rton Kardos",
          "description": "Understand Semantic Structures with Transformers and Topic Modeling",
          "link": "https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:55:45.000Z",
          "wordCount": 5294,
          "title": "Semantic Signal Separation",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1200/1*3_2zDivWp758eLvJPhp8Aw.png"
        },
        {
          "id": "https://medium.com/p/ee9b0b0ef082",
          "author": "Naomi Kriger",
          "description": "How to Create a Speech-to-Text-to-Speech Program",
          "link": "https://towardsdatascience.com/speech-to-text-to-speech-with-ai-using-python-a-how-to-guide-ee9b0b0ef082?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:47:58.000Z",
          "wordCount": 4225,
          "title": "Speech to Text to Speech with AI Using Python‚Ää‚Äî‚Ääa How-To Guide",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*Ai0UNZUQI8jW0nau"
        },
        {
          "id": "https://medium.com/p/6abe3e8b045c",
          "author": "Nate Cibik",
          "description": "Multilingual Learning with an Ollama-Python Walkie-Talkie",
          "link": "https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:17:09.000Z",
          "wordCount": 5607,
          "title": "LingoNaut Language Assistant",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1024/1*uxOckzPs6zbkuQZM80C7NA.jpeg"
        },
        {
          "id": "https://medium.com/p/e66b65ece369",
          "author": "Andy McDonald",
          "description": "Bringing Order to a Python Streamlit App Through an Organised Project Folder Structure\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/how-to-structure-and-organise-a-streamlit-app-e66b65ece369?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-11T15:04:44.000Z",
          "wordCount": 1504,
          "title": "How to Structure and Organise a Streamlit App",
          "imageUrl": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*PWOSQYW83vqbv4c-"
        },
        {
          "id": "https://medium.com/p/a191965ac538",
          "author": "Mike Shakhomirov",
          "description": "Advanced techniques to process and load data efficiently\nContinue reading on Towards Data Science ¬ª",
          "link": "https://towardsdatascience.com/pandas-for-data-engineers-a191965ac538?source=rss----7f60cf5620c9---4",
          "publishedOn": "2024-02-10T16:50:07.000Z",
          "wordCount": 1515,
          "title": "Pandas for Data Engineers",
          "imageUrl": "https://miro.medium.com/v2/resize:fit:1024/1*v64M9uKn1Yqs_ly80ucq5Q.png"
        }
      ]
    },
    {
      "title": "Machine Learning Blog | ML@CMU | Carnegie Mellon University",
      "feedUrl": "https://blog.ml.cmu.edu/feed/",
      "siteUrl": "https://blog.ml.cmu.edu",
      "articles": []
    },
    {
      "title": "Apple Machine Learning Research",
      "feedUrl": "https://machinelearning.apple.com/rss.xml",
      "siteUrl": "https://machinelearning.apple.com",
      "articles": [
        {
          "id": "resource-constrained",
          "author": null,
          "description": "We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning‚Ä¶",
          "link": "https://machinelearning.apple.com/research/resource-constrained",
          "publishedOn": "2024-02-13T00:00:00.000Z",
          "wordCount": 2406,
          "title": "Resource-constrained Stereo Singing Voice Cancellation",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "autoregressive-image-models",
          "author": null,
          "description": "This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2‚Ä¶",
          "link": "https://machinelearning.apple.com/research/autoregressive-image-models",
          "publishedOn": "2024-02-01T00:00:00.000Z",
          "wordCount": 1259,
          "title": "Scalable Pre-training of Large Autoregressive Image Models",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "acoustic-model-fusion",
          "author": null,
          "description": "Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted its accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain‚Ä¶",
          "link": "https://machinelearning.apple.com/research/acoustic-model-fusion",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 3302,
          "title": "Acoustic Model Fusion for End-to-end Speech Recognition",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "large-scale-training",
          "author": null,
          "description": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite the widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for‚Ä¶",
          "link": "https://machinelearning.apple.com/research/large-scale-training",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1158,
          "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "flexible-keyword",
          "author": null,
          "description": "Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (for example, large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder, which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a‚Ä¶",
          "link": "https://machinelearning.apple.com/research/flexible-keyword",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1015,
          "title": "Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "investigating-salient-representation",
          "author": null,
          "description": "Representations from models such as Bidirectional Encoder Representations from Transformers (BERT) and Hidden units BERT (HuBERT) have helped to achieve state-of-the-art performance in dimensional speech emotion recognition. Both HuBERT, and BERT models generate fairly large dimensional representations, and such models were not trained with emotion recognition task in mind. Such large dimensional representations result in speech emotion models with large parameter size, resulting in both memory and computational cost complexities. In this work, we investigate the selection of representations‚Ä¶",
          "link": "https://machinelearning.apple.com/research/investigating-salient-representation",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1038,
          "title": "Investigating Salient Representations and Label Variance Modeling in Dimensional Speech Emotion Analysis",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "coml",
          "author": null,
          "description": "Machine learning (ML) models are fundamentally shaped by data, and building inclusive ML systems requires significant considerations around how to design representative datasets. Yet, few novice-oriented ML modeling tools are designed to foster hands-on learning of dataset design practices, including how to design for data diversity and inspect for data quality.\nTo this end, we outline a set of four data design practices (DDPs) for designing inclusive ML models and share how we designed a tablet-based application called Co-ML to foster the learning of DDPs through a collbaborative ML model‚Ä¶",
          "link": "https://machinelearning.apple.com/research/coml",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 1957,
          "title": "Co-ML: Collaborative Machine Learning Model Building for Developing Dataset Design Practices",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "user-level-differentially",
          "author": null,
          "description": "We study differentially private stochastic convex optimization (DP-SCO) under user-level privacy where each user may hold multiple data items. Existing work for  user-level DP-SCO either requires super-polynomial runtime or requires number of users that grows polynomially with the dimensionality of the problem. We develop new algorithms for user-level DP-SCO that obtain optimal rates, run in polynomial time, and require a number of users that grow logarithmically in the dimension. Moreover, our algorithms are the first  to obtain optimal rates for non-smooth functions in polynomial time. These‚Ä¶",
          "link": "https://machinelearning.apple.com/research/user-level-differentially",
          "publishedOn": "2024-01-29T00:00:00.000Z",
          "wordCount": 3172,
          "title": "User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "transport-solvers",
          "author": null,
          "description": "Two salient limitations have long hindered the relevance of optimal transport methods to machine learning. First, the  computational cost of standard sample-based solvers (when used on batches of  samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \\textit{all} points from both measures, their output can be heavily influenced by outliers. A flurry of recent works has addressed these computational and modeling limitations. Still it has resulted in two separate strains of methods: While the computational outlook was‚Ä¶",
          "link": "https://machinelearning.apple.com/research/transport-solvers",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 2210,
          "title": "Unbalanced Low-Rank Optimal Transport Solvers",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "hybrid-model-learning",
          "author": null,
          "description": "This paper was accepted at the workshop Deep Generative Models for Health at NeurIPS 2023.\nCardiovascular diseases (CVDs) are a major global health concern, making the longitudinal monitoring of cardiovascular biomarkers vital for early diagnosis and intervention. A core challenge is the inference of cardiac pulse parameters from pulse waves, especially when acquired from wearable sensors at peripheral body locations. Traditional machine learning (ML) approaches face hurdles in this context due to the scarcity of labeled data, primarily sourced from clinical settings. Simultaneously, physical‚Ä¶",
          "link": "https://machinelearning.apple.com/research/hybrid-model-learning",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 4466,
          "title": "Hybrid Model Learning for Cardiovascular Biomarkers Inference",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "one-wide-ffn",
          "author": null,
          "description": "This paper was accepted at WMT conference at EMNLP.\nThe Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work, we explore the role of FFN and find that despite, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by‚Ä¶",
          "link": "https://machinelearning.apple.com/research/one-wide-ffn",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 5315,
          "title": "One Wide Feedforward is All You Need",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        },
        {
          "id": "faster-nerf",
          "author": null,
          "description": "Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, and/or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple NeRF+SR pipeline that directly combines existing modules, and we propose a lightweight augmentation‚Ä¶",
          "link": "https://machinelearning.apple.com/research/faster-nerf",
          "publishedOn": "2024-01-22T00:00:00.000Z",
          "wordCount": 1050,
          "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline",
          "imageUrl": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png"
        }
      ]
    },
    {
      "title": "Google AI Blog",
      "feedUrl": "https://blog.research.google/atom.xml",
      "siteUrl": "http://blog.research.google/",
      "articles": [
        {
          "id": "http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html",
          "author": null,
          "description": "Posted by M√≥nica Ribero D√≠az, Research Scientist, Google Research\n\n\n\n\n\nDifferential privacy (DP) is a property of randomized mechanisms that limit the influence of any individual user‚Äôs information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies across industries and government applications (e.g., the US census) without compromising individual user identities.  As its adoption increases, it‚Äôs important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, researchers compared six sparse vector technique (SVT) variations and found that only two‚Ä¶",
          "link": "http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html",
          "publishedOn": "2024-02-13T22:11:00.000Z",
          "wordCount": 27935,
          "title": "DP-Auditorium: A flexible library for auditing differential privacy",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/w1200-h630-p-k-no-nu/hero.jpg"
        },
        {
          "id": "http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html",
          "author": null,
          "description": "Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML\n\n\n\n\nObjects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation ‚Äî take for example transportation networks, production networks, knowledge graphs, or social networks. Discrete mathematics and computer science have a long history of formalizing such networks as graphs, consisting of nodes connected by edges in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. \n\n\n\n\nGraph neural networks, or GNNs for short, have emerged‚Ä¶",
          "link": "http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html",
          "publishedOn": "2024-02-06T19:17:00.000Z",
          "wordCount": 27781,
          "title": "Graph neural networks in TensorFlow",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/w1200-h630-p-k-no-nu/TFGNN%20hero.gif"
        },
        {
          "id": "http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html",
          "author": null,
          "description": "Posted by Rajat Sen and Yichen Zhou, Google Research\n\n\n\n\n\nTime-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models performed well in the M5 competition).\n\n\n\nAt the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as translation, retrieval-augmented generation, and code completion. Thes‚Ä¶",
          "link": "http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html",
          "publishedOn": "2024-02-02T19:07:00.000Z",
          "wordCount": 27728,
          "title": "A decoder-only foundation model for time-series forecasting",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/w1200-h630-p-k-no-nu/hero.jpg"
        },
        {
          "id": "http://blog.research.google/2024/02/intervening-on-early-readouts-for.html",
          "author": null,
          "description": "Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research\n\n\n\n\nMachine learning models in the real world are often trained on limited data that may contain unintended statistical biases. For example, in the CELEBA celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting ‚Äúblond‚Äù as the hair color for most female faces ‚Äî here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as medical diagnosis. \n\n\n\n\n\nSurprisingly, recent work has also discovered an inherent tendency of deep networks to amplify such statistical biases, through the so-called simplicity bias of deep learning. T‚Ä¶",
          "link": "http://blog.research.google/2024/02/intervening-on-early-readouts-for.html",
          "publishedOn": "2024-02-02T17:49:00.000Z",
          "wordCount": 27761,
          "title": "Intervening on early readouts for mitigating spurious features and simplicity bias",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/w1200-h630-p-k-no-nu/SiFer%20Hero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html",
          "author": null,
          "description": "Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML\n\n\n\n\nText-to-image diffusion models have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., Stable Diffusion, DALL¬∑E, and Imagen). While recent advancements in inference solutions on Android via MediaPipe and iOS via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.\n \n\nTo that end, in ‚ÄúMobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices‚Äù, we introduce a novel approach with the potential for rapid text-to-image generation on‚Ä¶",
          "link": "http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html",
          "publishedOn": "2024-01-31T21:59:00.000Z",
          "wordCount": 27727,
          "title": "MobileDiffusion: Rapid text-to-image generation on-device",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/w1200-h630-p-k-no-nu/InstantTIGO%20hero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html",
          "author": null,
          "description": "Posted by Manish Gupta, Staff Software Engineer, Google Research\n\n\n\n\nAI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs).  LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver tens of exaflops of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively.\n\n\n\nThe bulk of an LLM‚Äôs memory and compute are consumed by weights in matrix multiplication operations. Using narrower data types reduces memory consumption. For example, storing weights in the 8-bit integer (i.e., U8 or S8)‚Ä¶",
          "link": "http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html",
          "publishedOn": "2024-01-26T19:56:00.000Z",
          "wordCount": 27947,
          "title": "Mixed-input matrix multiplication performance optimizations",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/w1200-h630-p-k-no-nu/matrixhero.png"
        },
        {
          "id": "http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html",
          "author": null,
          "description": "Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google\n\n\n\n\nGraphs, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. \n\n\n\n\nA common approach to learning on graphs are graph neural networks (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a message-passing framework,‚Ä¶",
          "link": "http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html",
          "publishedOn": "2024-01-23T22:27:00.000Z",
          "wordCount": 27895,
          "title": "Exphormer: Scaling transformers for graph-structured data",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/w1200-h630-p-k-no-nu/EXPHORMER%2005large.gif"
        },
        {
          "id": "http://blog.research.google/2024/01/introducing-aspire-for-selective.html",
          "author": null,
          "description": "Posted by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team\n\n\n\n\n\nIn the fast-evolving landscape of artificial intelligence, large language models (LLMs) have revolutionized the way we interact with machines, pushing the boundaries of natural language understanding and generation to unprecedented heights. Yet, the leap into high-stakes decision-making applications remains a chasm too wide, primarily due to the inherent uncertainty of model predictions. Traditional LLMs generate responses recursively, yet they lack an intrinsic mechanism to assign a confidence score to these responses. Although one can derive a confidence score by summing up the probabilities of individual tokens in the sequence, traditional approaches typically fall short in reliably dist‚Ä¶",
          "link": "http://blog.research.google/2024/01/introducing-aspire-for-selective.html",
          "publishedOn": "2024-01-18T18:03:00.000Z",
          "wordCount": 27602,
          "title": "Introducing ASPIRE for selective prediction in LLMs",
          "imageUrl": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/w1200-h630-p-k-no-nu/ASPIRE%20hero.jpg"
        }
      ]
    },
    {
      "title": "FastML",
      "feedUrl": "https://fastml.com/atom.xml",
      "siteUrl": "https://fastml.com/atom.xml",
      "articles": []
    },
    {
      "title": "Adit Deshpande",
      "feedUrl": "https://adeshpande3.github.io/feed.xml",
      "siteUrl": "https://adeshpande3.github.io",
      "articles": []
    },
    {
      "title": "Kaggle Blog - Medium",
      "feedUrl": "https://medium.com/feed/kaggle-blog",
      "siteUrl": "https://medium.com/kaggle-blog?source=rss----4b0982ce16a3---4",
      "articles": []
    },
    {
      "title": "Hugging Face - Blog",
      "feedUrl": "https://huggingface.co/blog/feed.xml",
      "siteUrl": "https://huggingface.co/blog",
      "articles": []
    },
    {
      "title": "Lil'Log",
      "feedUrl": "https://lilianweng.github.io/index.xml",
      "siteUrl": "https://lilianweng.github.io/",
      "articles": [
        {
          "id": "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/",
          "author": null,
          "description": "[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper ‚ÄúVox populi‚Äù) and nice feedback. üôè ]\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.",
          "link": "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/",
          "publishedOn": "2024-02-05T00:00:00.000Z",
          "wordCount": 9024,
          "title": "Thinking about High-Quality Human Data",
          "imageUrl": null
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}